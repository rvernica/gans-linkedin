{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rvernica/gans-linkedin/blob/main/GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "ypreYlxEtTel",
        "outputId": "ed79ecf8-3803-4889-e540-bf89b00c11cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.5.1+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Intall torchvision\n",
        "%pip install torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mWCjL_IStTen"
      },
      "outputs": [],
      "source": [
        "# Import the required libraries\n",
        "# For this example we will use pytorch to manage the construction of the neural networks and the training\n",
        "# torchvision is a module that is part of pytorch that supports vision datasets and it will be where we will source the mnist - handwritten digits - data\n",
        "\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OMHWtqITtTeo",
        "outputId": "3f277564-a86e-44af-a211-e1ec99ba306b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Seed:  8063\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c22fef026f0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Setting a seed will determine which data elements are selected. To replicate results keep the same seed.\n",
        "manualSeed = random.randint(1, 10000)\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IWXaGc01tTep",
        "outputId": "6488ea68-14a5-49b7-85bf-9713db9915c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True 1\n"
          ]
        }
      ],
      "source": [
        "# This is a check if there is a gpu available for training. At the moment we are assuming that it is not available.\n",
        "print(torch.cuda.is_available(), torch.cuda.device_count())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "uhsI-ia4x_CT",
        "outputId": "fbc44e35-5c1d-4b70-e0a6-e92c74d4b31d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Mar 14 04:50:52 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   64C    P8             11W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qxjj_4jGtTeq"
      },
      "outputs": [],
      "source": [
        "# Assuming the GPU is not available means we will set the device to cpu and set up some parameters\n",
        "cudnn.benchmark = True\n",
        "device = torch.device(\"cuda\")\n",
        "ngpu = 1\n",
        "# This is the width of the latent space matrix\n",
        "nz = 100\n",
        "# This is the generator matrix shape\n",
        "ngf = 64\n",
        "# This is the descrimator matrix shape\n",
        "ndf = 64\n",
        "# This is the number of color channels - other datasets may have 3 if they are color\n",
        "nc = 1\n",
        "# The number of sample to process per pass\n",
        "batch_size = 1024 # 64\n",
        "# The number of CPU workers to work on the dataset\n",
        "workers = 2\n",
        "# The number of epocs\n",
        "nepocs = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "m6BuVYwotTeq"
      },
      "outputs": [],
      "source": [
        "dataset = dset.MNIST(root='data', download=True,\n",
        "                      transform=transforms.Compose([\n",
        "                          transforms.Resize(64),\n",
        "                          transforms.ToTensor(),\n",
        "                          transforms.Normalize((0.5,), (0.5,)),\n",
        "                      ]))\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                         shuffle=True, num_workers=int(workers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BZlCaK03tTer"
      },
      "outputs": [],
      "source": [
        "# custom weights initialization called on netG and netD\n",
        "# The weights will need to be initialised based on the layer type to some value before training. These could be imported from past training steps.\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        torch.nn.init.normal_(m.weight, 1.0, 0.02)\n",
        "        torch.nn.init.zeros_(m.bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "B3t-Q58QtTer",
        "outputId": "336ee7d9-9c25-4933-d8c6-381a6cf43133",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator(\n",
            "  (main): Sequential(\n",
            "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (13): Tanh()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# This is the bulk of the neural network definition for the Generator.\n",
        "# The init sets up the layers and connecting activation functions.\n",
        "# The forward function processes the data through the layers\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            nn.ConvTranspose2d(ngf,      nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. (nc) x 64 x 64\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        if input.is_cuda and self.ngpu > 1:\n",
        "            output = nn.parallel.data_parallel(\n",
        "                self.main, input, range(self.ngpu))\n",
        "        else:\n",
        "            output = self.main(input)\n",
        "        return output\n",
        "\n",
        "\n",
        "netG = Generator(ngpu).to(device)\n",
        "netG.apply(weights_init)\n",
        "print(netG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fAnqZAF3tTes",
        "outputId": "a91e6972-3a47-4ebf-f9c5-9527c8b0b16b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discriminator(\n",
            "  (main): Sequential(\n",
            "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (12): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# This is the bulk of the neural network definition for the Discrimator.\n",
        "# The init sets up the layers and connecting activation functions.\n",
        "# The forward function processes the data through the layers\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is (nc) x 64 x 64\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf) x 32 x 32\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*4) x 8 x 8\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        if input.is_cuda and self.ngpu > 1:\n",
        "            output = nn.parallel.data_parallel(\n",
        "                self.main, input, range(self.ngpu))\n",
        "        else:\n",
        "            output = self.main(input)\n",
        "\n",
        "        return output.view(-1, 1).squeeze(1)\n",
        "\n",
        "netD = Discriminator(ngpu).to(device)\n",
        "netD.apply(weights_init)\n",
        "print(netD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gWig0bFItTet"
      },
      "outputs": [],
      "source": [
        "# Set the loss function from pytorches established modules\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Set up the initial noise of the latent space to sample from.\n",
        "# Set the label of a real and fake sample to 0,1\n",
        "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
        "real_label = 1\n",
        "fake_label = 0\n",
        "\n",
        "# Create the optimiser which will dynamically change the parameters of the learning function over time to imporve the training process\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=0.0005, betas=(0.5, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=0.0005, betas=(0.5, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": true,
        "id": "scJ6hebVtTet",
        "outputId": "a5383a61-f7a5-4562-ffac-fea1740b57f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0/1][0/59] Loss_D: 1.7192 Loss_G: 4.4632 D(x): 0.3428 D(G(z)): 0.3451 / 0.0202\n",
            "[0/1][1/59] Loss_D: 11.0815 Loss_G: 8.6431 D(x): 1.0000 D(G(z)): 1.0000 / 0.0004\n",
            "[0/1][2/59] Loss_D: 4.4420 Loss_G: 16.9538 D(x): 0.9996 D(G(z)): 0.9789 / 0.0000\n",
            "[0/1][3/59] Loss_D: 1.3979 Loss_G: 7.6499 D(x): 0.5818 D(G(z)): 0.0946 / 0.0010\n",
            "[0/1][4/59] Loss_D: 8.1126 Loss_G: 16.3102 D(x): 0.9681 D(G(z)): 0.9994 / 0.0000\n",
            "[0/1][5/59] Loss_D: 0.1979 Loss_G: 14.0214 D(x): 0.9237 D(G(z)): 0.0112 / 0.0000\n",
            "[0/1][6/59] Loss_D: 0.5071 Loss_G: 17.5090 D(x): 0.9463 D(G(z)): 0.3010 / 0.0000\n",
            "[0/1][7/59] Loss_D: 0.1267 Loss_G: 13.1280 D(x): 0.9362 D(G(z)): 0.0024 / 0.0000\n",
            "[0/1][8/59] Loss_D: 1.0785 Loss_G: 28.2294 D(x): 0.9555 D(G(z)): 0.5796 / 0.0000\n",
            "[0/1][9/59] Loss_D: 1.1749 Loss_G: 26.8159 D(x): 0.5368 D(G(z)): 0.0000 / 0.0000\n",
            "[0/1][10/59] Loss_D: 0.0069 Loss_G: 8.8480 D(x): 0.9951 D(G(z)): 0.0003 / 0.0003\n",
            "[0/1][11/59] Loss_D: 9.6363 Loss_G: 29.6971 D(x): 0.9995 D(G(z)): 0.9999 / 0.0000\n",
            "[0/1][12/59] Loss_D: 0.5267 Loss_G: 34.7275 D(x): 0.7840 D(G(z)): 0.0000 / 0.0000\n",
            "[0/1][13/59] Loss_D: 0.4068 Loss_G: 35.4026 D(x): 0.8364 D(G(z)): 0.0000 / 0.0000\n",
            "[0/1][14/59] Loss_D: 0.0504 Loss_G: 35.4307 D(x): 0.9754 D(G(z)): 0.0000 / 0.0000\n",
            "[0/1][15/59] Loss_D: 0.0220 Loss_G: 35.3843 D(x): 0.9890 D(G(z)): 0.0000 / 0.0000\n",
            "[0/1][16/59] Loss_D: 0.0041 Loss_G: 35.3153 D(x): 0.9977 D(G(z)): 0.0000 / 0.0000\n",
            "[0/1][17/59] Loss_D: 0.0020 Loss_G: 35.2752 D(x): 0.9982 D(G(z)): 0.0000 / 0.0000\n",
            "[0/1][18/59] Loss_D: 0.0007 Loss_G: 35.1700 D(x): 0.9993 D(G(z)): 0.0000 / 0.0000\n",
            "[0/1][19/59] Loss_D: 0.0046 Loss_G: 35.0837 D(x): 0.9973 D(G(z)): 0.0000 / 0.0000\n",
            "[0/1][20/59] Loss_D: 0.0027 Loss_G: 34.9952 D(x): 0.9979 D(G(z)): 0.0000 / 0.0000\n",
            "[0/1][21/59] Loss_D: 0.0023 Loss_G: 34.8876 D(x): 0.9982 D(G(z)): 0.0000 / 0.0000\n",
            "[0/1][22/59] Loss_D: 0.0017 Loss_G: 34.7308 D(x): 0.9987 D(G(z)): 0.0000 / 0.0000\n",
            "[0/1][23/59] Loss_D: 0.0014 Loss_G: 34.6076 D(x): 0.9987 D(G(z)): 0.0000 / 0.0000\n",
            "[0/1][24/59] Loss_D: 0.0005 Loss_G: 34.4517 D(x): 0.9995 D(G(z)): 0.0000 / 0.0000\n",
            "[0/1][25/59] Loss_D: 0.0018 Loss_G: 34.2149 D(x): 0.9989 D(G(z)): 0.0000 / 0.0000\n",
            "[0/1][26/59] Loss_D: 0.0049 Loss_G: 33.9115 D(x): 0.9985 D(G(z)): 0.0000 / 0.0000\n",
            "[0/1][27/59] Loss_D: 0.0014 Loss_G: 33.5262 D(x): 0.9987 D(G(z)): 0.0000 / 0.0000\n",
            "[0/1][28/59] Loss_D: 0.0008 Loss_G: 32.9265 D(x): 0.9993 D(G(z)): 0.0000 / 0.0000\n",
            "[0/1][29/59] Loss_D: 0.0011 Loss_G: 31.8387 D(x): 0.9992 D(G(z)): 0.0000 / 0.0000\n",
            "[0/1][30/59] Loss_D: 0.0006 Loss_G: 29.0852 D(x): 0.9994 D(G(z)): 0.0000 / 0.0000\n",
            "[0/1][31/59] Loss_D: 0.0003 Loss_G: 17.0188 D(x): 0.9997 D(G(z)): 0.0000 / 0.0000\n",
            "[0/1][32/59] Loss_D: 4.4367 Loss_G: 36.8684 D(x): 0.9990 D(G(z)): 0.9836 / 0.0000\n",
            "[0/1][33/59] Loss_D: 6.0823 Loss_G: 35.8321 D(x): 0.0146 D(G(z)): 0.0000 / 0.0000\n",
            "[0/1][34/59] Loss_D: 0.0433 Loss_G: 34.9553 D(x): 0.9797 D(G(z)): 0.0000 / 0.0000\n",
            "[0/1][35/59] Loss_D: 0.0311 Loss_G: 34.3427 D(x): 0.9846 D(G(z)): 0.0000 / 0.0000\n",
            "[0/1][36/59] Loss_D: 0.0238 Loss_G: 33.8542 D(x): 0.9898 D(G(z)): 0.0000 / 0.0000\n",
            "[0/1][37/59] Loss_D: 0.0120 Loss_G: 33.3646 D(x): 0.9940 D(G(z)): 0.0000 / 0.0000\n",
            "[0/1][38/59] Loss_D: 0.0184 Loss_G: 32.6834 D(x): 0.9921 D(G(z)): 0.0000 / 0.0000\n",
            "[0/1][39/59] Loss_D: 0.0036 Loss_G: 31.4739 D(x): 0.9973 D(G(z)): 0.0000 / 0.0000\n",
            "[0/1][40/59] Loss_D: 0.0080 Loss_G: 28.0681 D(x): 0.9957 D(G(z)): 0.0000 / 0.0000\n",
            "[0/1][41/59] Loss_D: 0.0031 Loss_G: 12.0910 D(x): 0.9984 D(G(z)): 0.0010 / 0.0010\n",
            "[0/1][42/59] Loss_D: 93.3413 Loss_G: 0.0002 D(x): 0.9978 D(G(z)): 1.0000 / 0.9998\n",
            "[0/1][43/59] Loss_D: 63.1541 Loss_G: 8.4351 D(x): 0.9971 D(G(z)): 0.9999 / 0.0611\n",
            "[0/1][44/59] Loss_D: 1.9814 Loss_G: 21.1772 D(x): 0.6528 D(G(z)): 0.3101 / 0.0000\n",
            "[0/1][45/59] Loss_D: 4.9677 Loss_G: 1.3418 D(x): 0.1172 D(G(z)): 0.0000 / 0.3870\n",
            "[0/1][46/59] Loss_D: 8.5959 Loss_G: 12.9577 D(x): 0.9855 D(G(z)): 0.9759 / 0.0000\n",
            "[0/1][47/59] Loss_D: 0.7226 Loss_G: 11.1022 D(x): 0.7772 D(G(z)): 0.0021 / 0.0001\n",
            "[0/1][48/59] Loss_D: 1.4146 Loss_G: 3.6754 D(x): 0.6536 D(G(z)): 0.1749 / 0.0461\n",
            "[0/1][49/59] Loss_D: 6.1708 Loss_G: 13.1053 D(x): 0.7599 D(G(z)): 0.8570 / 0.0000\n",
            "[0/1][50/59] Loss_D: 5.5698 Loss_G: 5.2638 D(x): 0.0297 D(G(z)): 0.0003 / 0.0070\n",
            "[0/1][51/59] Loss_D: 1.4639 Loss_G: 4.8739 D(x): 0.7774 D(G(z)): 0.4851 / 0.0207\n",
            "[0/1][52/59] Loss_D: 0.8769 Loss_G: 5.0472 D(x): 0.7476 D(G(z)): 0.3010 / 0.0186\n",
            "[0/1][53/59] Loss_D: 0.8301 Loss_G: 3.9997 D(x): 0.7116 D(G(z)): 0.2498 / 0.0265\n",
            "[0/1][54/59] Loss_D: 0.9471 Loss_G: 5.8884 D(x): 0.7571 D(G(z)): 0.3879 / 0.0067\n",
            "[0/1][55/59] Loss_D: 0.9403 Loss_G: 1.4756 D(x): 0.5445 D(G(z)): 0.0391 / 0.2500\n",
            "[0/1][56/59] Loss_D: 1.5214 Loss_G: 9.2270 D(x): 0.9504 D(G(z)): 0.7168 / 0.0015\n",
            "[0/1][57/59] Loss_D: 2.0075 Loss_G: 4.2352 D(x): 0.2497 D(G(z)): 0.0038 / 0.0380\n",
            "[0/1][58/59] Loss_D: 0.2632 Loss_G: 1.7876 D(x): 0.9401 D(G(z)): 0.1658 / 0.1983\n",
            "[1/1][0/59] Loss_D: 1.2862 Loss_G: 7.6465 D(x): 0.9835 D(G(z)): 0.6660 / 0.0035\n",
            "[1/1][1/59] Loss_D: 1.0100 Loss_G: 5.2423 D(x): 0.5063 D(G(z)): 0.0124 / 0.0170\n",
            "[1/1][2/59] Loss_D: 0.4144 Loss_G: 1.7711 D(x): 0.7846 D(G(z)): 0.0881 / 0.2019\n",
            "[1/1][3/59] Loss_D: 1.2224 Loss_G: 6.3313 D(x): 0.9577 D(G(z)): 0.6382 / 0.0057\n",
            "[1/1][4/59] Loss_D: 1.0920 Loss_G: 3.3535 D(x): 0.4477 D(G(z)): 0.0216 / 0.0655\n",
            "[1/1][5/59] Loss_D: 0.5501 Loss_G: 3.0892 D(x): 0.8760 D(G(z)): 0.2787 / 0.0643\n",
            "[1/1][6/59] Loss_D: 0.5741 Loss_G: 5.1275 D(x): 0.8769 D(G(z)): 0.3184 / 0.0116\n",
            "[1/1][7/59] Loss_D: 0.5719 Loss_G: 3.0339 D(x): 0.6555 D(G(z)): 0.0564 / 0.0728\n",
            "[1/1][8/59] Loss_D: 0.6812 Loss_G: 5.0360 D(x): 0.8718 D(G(z)): 0.3720 / 0.0114\n",
            "[1/1][9/59] Loss_D: 0.4899 Loss_G: 3.4623 D(x): 0.7238 D(G(z)): 0.0847 / 0.0503\n",
            "[1/1][10/59] Loss_D: 0.8510 Loss_G: 6.1620 D(x): 0.8797 D(G(z)): 0.4534 / 0.0043\n",
            "[1/1][11/59] Loss_D: 0.9540 Loss_G: 1.9815 D(x): 0.5238 D(G(z)): 0.0865 / 0.1816\n",
            "[1/1][12/59] Loss_D: 1.5865 Loss_G: 7.0275 D(x): 0.9185 D(G(z)): 0.7231 / 0.0041\n",
            "[1/1][13/59] Loss_D: 1.6252 Loss_G: 2.8850 D(x): 0.3249 D(G(z)): 0.0317 / 0.1581\n",
            "[1/1][14/59] Loss_D: 1.1408 Loss_G: 3.8394 D(x): 0.9293 D(G(z)): 0.4960 / 0.0449\n",
            "[1/1][15/59] Loss_D: 0.6966 Loss_G: 4.5063 D(x): 0.8148 D(G(z)): 0.3114 / 0.0197\n",
            "[1/1][16/59] Loss_D: 0.9523 Loss_G: 1.9837 D(x): 0.5578 D(G(z)): 0.1650 / 0.1805\n",
            "[1/1][17/59] Loss_D: 1.0013 Loss_G: 5.5230 D(x): 0.8760 D(G(z)): 0.5352 / 0.0065\n",
            "[1/1][18/59] Loss_D: 0.7833 Loss_G: 3.2920 D(x): 0.5609 D(G(z)): 0.0272 / 0.0497\n",
            "[1/1][19/59] Loss_D: 0.3520 Loss_G: 3.0282 D(x): 0.9054 D(G(z)): 0.2008 / 0.0631\n",
            "[1/1][20/59] Loss_D: 0.5218 Loss_G: 5.3750 D(x): 0.9275 D(G(z)): 0.3352 / 0.0071\n",
            "[1/1][21/59] Loss_D: 0.5395 Loss_G: 3.3974 D(x): 0.7202 D(G(z)): 0.1012 / 0.0430\n",
            "[1/1][22/59] Loss_D: 1.5072 Loss_G: 7.3779 D(x): 0.8175 D(G(z)): 0.6742 / 0.0015\n",
            "[1/1][23/59] Loss_D: 2.3479 Loss_G: 1.7797 D(x): 0.1774 D(G(z)): 0.0155 / 0.2369\n",
            "[1/1][24/59] Loss_D: 1.2199 Loss_G: 4.8857 D(x): 0.9251 D(G(z)): 0.6028 / 0.0143\n",
            "[1/1][25/59] Loss_D: 0.4305 Loss_G: 4.3764 D(x): 0.7923 D(G(z)): 0.1076 / 0.0208\n",
            "[1/1][26/59] Loss_D: 0.5077 Loss_G: 2.7901 D(x): 0.7896 D(G(z)): 0.1678 / 0.0799\n",
            "[1/1][27/59] Loss_D: 0.7523 Loss_G: 3.8579 D(x): 0.8104 D(G(z)): 0.3695 / 0.0256\n",
            "[1/1][28/59] Loss_D: 0.7410 Loss_G: 2.2506 D(x): 0.6390 D(G(z)): 0.1363 / 0.1319\n",
            "[1/1][29/59] Loss_D: 0.8439 Loss_G: 4.5548 D(x): 0.8393 D(G(z)): 0.4372 / 0.0138\n",
            "[1/1][30/59] Loss_D: 0.8831 Loss_G: 1.7987 D(x): 0.5359 D(G(z)): 0.0888 / 0.1918\n",
            "[1/1][31/59] Loss_D: 1.0036 Loss_G: 5.3757 D(x): 0.8967 D(G(z)): 0.5445 / 0.0073\n",
            "[1/1][32/59] Loss_D: 0.8210 Loss_G: 2.6686 D(x): 0.5604 D(G(z)): 0.0676 / 0.1039\n",
            "[1/1][33/59] Loss_D: 1.0814 Loss_G: 4.9364 D(x): 0.8732 D(G(z)): 0.5433 / 0.0134\n",
            "[1/1][34/59] Loss_D: 1.0140 Loss_G: 2.0217 D(x): 0.5023 D(G(z)): 0.0749 / 0.1793\n",
            "[1/1][35/59] Loss_D: 0.7865 Loss_G: 4.5074 D(x): 0.8961 D(G(z)): 0.4325 / 0.0200\n",
            "[1/1][36/59] Loss_D: 0.4924 Loss_G: 3.7956 D(x): 0.7489 D(G(z)): 0.0950 / 0.0381\n",
            "[1/1][37/59] Loss_D: 0.4480 Loss_G: 3.1491 D(x): 0.8447 D(G(z)): 0.1813 / 0.0622\n",
            "[1/1][38/59] Loss_D: 0.4947 Loss_G: 3.6059 D(x): 0.8454 D(G(z)): 0.2237 / 0.0378\n",
            "[1/1][39/59] Loss_D: 0.5117 Loss_G: 2.9454 D(x): 0.7849 D(G(z)): 0.1685 / 0.0756\n",
            "[1/1][40/59] Loss_D: 0.7761 Loss_G: 4.0213 D(x): 0.7903 D(G(z)): 0.3544 / 0.0279\n",
            "[1/1][41/59] Loss_D: 0.9862 Loss_G: 1.4929 D(x): 0.5611 D(G(z)): 0.2142 / 0.2956\n",
            "[1/1][42/59] Loss_D: 1.4585 Loss_G: 7.9784 D(x): 0.8762 D(G(z)): 0.6634 / 0.0008\n",
            "[1/1][43/59] Loss_D: 2.5157 Loss_G: 1.4756 D(x): 0.1357 D(G(z)): 0.0046 / 0.3335\n",
            "[1/1][44/59] Loss_D: 1.4851 Loss_G: 4.7494 D(x): 0.9624 D(G(z)): 0.6657 / 0.0214\n",
            "[1/1][45/59] Loss_D: 0.5250 Loss_G: 4.0841 D(x): 0.7354 D(G(z)): 0.1241 / 0.0297\n",
            "[1/1][46/59] Loss_D: 0.7259 Loss_G: 2.2507 D(x): 0.7006 D(G(z)): 0.2267 / 0.1494\n",
            "[1/1][47/59] Loss_D: 1.1425 Loss_G: 3.8229 D(x): 0.7444 D(G(z)): 0.4829 / 0.0323\n",
            "[1/1][48/59] Loss_D: 1.3474 Loss_G: 0.7634 D(x): 0.3884 D(G(z)): 0.1370 / 0.5130\n",
            "[1/1][49/59] Loss_D: 1.4735 Loss_G: 5.8309 D(x): 0.9292 D(G(z)): 0.7078 / 0.0055\n",
            "[1/1][50/59] Loss_D: 1.2962 Loss_G: 2.9511 D(x): 0.3783 D(G(z)): 0.0190 / 0.0867\n",
            "[1/1][51/59] Loss_D: 0.5395 Loss_G: 2.8575 D(x): 0.9151 D(G(z)): 0.3131 / 0.0843\n",
            "[1/1][52/59] Loss_D: 0.6110 Loss_G: 4.3385 D(x): 0.8837 D(G(z)): 0.3372 / 0.0209\n",
            "[1/1][53/59] Loss_D: 0.7549 Loss_G: 1.9705 D(x): 0.6083 D(G(z)): 0.1098 / 0.1872\n",
            "[1/1][54/59] Loss_D: 0.9735 Loss_G: 4.7472 D(x): 0.8688 D(G(z)): 0.4969 / 0.0162\n",
            "[1/1][55/59] Loss_D: 0.9941 Loss_G: 1.3801 D(x): 0.4857 D(G(z)): 0.0740 / 0.3153\n",
            "[1/1][56/59] Loss_D: 1.3668 Loss_G: 6.1905 D(x): 0.9250 D(G(z)): 0.6612 / 0.0050\n",
            "[1/1][57/59] Loss_D: 1.6326 Loss_G: 1.3877 D(x): 0.2923 D(G(z)): 0.0246 / 0.3374\n",
            "[1/1][58/59] Loss_D: 1.2856 Loss_G: 6.3721 D(x): 0.9363 D(G(z)): 0.6076 / 0.0039\n",
            "[2/1][0/59] Loss_D: 1.4119 Loss_G: 1.0540 D(x): 0.3252 D(G(z)): 0.0218 / 0.4308\n",
            "[2/1][1/59] Loss_D: 1.2712 Loss_G: 4.7821 D(x): 0.9673 D(G(z)): 0.6278 / 0.0132\n",
            "[2/1][2/59] Loss_D: 1.3151 Loss_G: 1.2529 D(x): 0.3769 D(G(z)): 0.0424 / 0.3432\n",
            "[2/1][3/59] Loss_D: 1.3407 Loss_G: 3.7368 D(x): 0.9368 D(G(z)): 0.6429 / 0.0506\n",
            "[2/1][4/59] Loss_D: 1.0069 Loss_G: 2.2398 D(x): 0.5448 D(G(z)): 0.1786 / 0.1640\n",
            "[2/1][5/59] Loss_D: 0.9242 Loss_G: 2.6555 D(x): 0.7189 D(G(z)): 0.3647 / 0.0950\n",
            "[2/1][6/59] Loss_D: 0.7238 Loss_G: 2.7213 D(x): 0.6925 D(G(z)): 0.2288 / 0.0836\n",
            "[2/1][7/59] Loss_D: 0.5504 Loss_G: 3.0665 D(x): 0.7917 D(G(z)): 0.2323 / 0.0642\n",
            "[2/1][8/59] Loss_D: 0.5780 Loss_G: 3.2695 D(x): 0.8010 D(G(z)): 0.2565 / 0.0563\n",
            "[2/1][9/59] Loss_D: 0.6535 Loss_G: 2.3899 D(x): 0.7179 D(G(z)): 0.2171 / 0.1225\n",
            "[2/1][10/59] Loss_D: 0.7171 Loss_G: 3.9963 D(x): 0.8083 D(G(z)): 0.3487 / 0.0274\n",
            "[2/1][11/59] Loss_D: 0.6412 Loss_G: 1.9674 D(x): 0.6365 D(G(z)): 0.0803 / 0.1820\n",
            "[2/1][12/59] Loss_D: 0.7213 Loss_G: 5.1874 D(x): 0.9236 D(G(z)): 0.4322 / 0.0094\n",
            "[2/1][13/59] Loss_D: 0.6023 Loss_G: 2.6616 D(x): 0.6468 D(G(z)): 0.0468 / 0.0986\n",
            "[2/1][14/59] Loss_D: 0.8836 Loss_G: 5.4002 D(x): 0.8887 D(G(z)): 0.4814 / 0.0072\n",
            "[2/1][15/59] Loss_D: 1.1685 Loss_G: 1.0965 D(x): 0.4169 D(G(z)): 0.0341 / 0.4056\n",
            "[2/1][16/59] Loss_D: 1.2812 Loss_G: 7.7449 D(x): 0.9547 D(G(z)): 0.6407 / 0.0008\n",
            "[2/1][17/59] Loss_D: 1.4294 Loss_G: 2.7300 D(x): 0.3629 D(G(z)): 0.0051 / 0.0899\n",
            "[2/1][18/59] Loss_D: 0.9306 Loss_G: 4.4003 D(x): 0.9225 D(G(z)): 0.5138 / 0.0236\n",
            "[2/1][19/59] Loss_D: 1.3381 Loss_G: 0.4419 D(x): 0.3901 D(G(z)): 0.1318 / 0.6790\n",
            "[2/1][20/59] Loss_D: 2.2609 Loss_G: 7.1762 D(x): 0.9596 D(G(z)): 0.8438 / 0.0020\n",
            "[2/1][21/59] Loss_D: 2.3102 Loss_G: 3.7106 D(x): 0.1969 D(G(z)): 0.0053 / 0.0451\n",
            "[2/1][22/59] Loss_D: 0.4472 Loss_G: 1.4959 D(x): 0.8105 D(G(z)): 0.1567 / 0.2766\n",
            "[2/1][23/59] Loss_D: 1.0856 Loss_G: 4.7540 D(x): 0.9604 D(G(z)): 0.5855 / 0.0149\n",
            "[2/1][24/59] Loss_D: 0.6217 Loss_G: 3.2581 D(x): 0.6355 D(G(z)): 0.0650 / 0.0612\n",
            "[2/1][25/59] Loss_D: 0.5686 Loss_G: 2.0367 D(x): 0.7743 D(G(z)): 0.2215 / 0.1752\n",
            "[2/1][26/59] Loss_D: 0.6276 Loss_G: 3.3326 D(x): 0.8512 D(G(z)): 0.3383 / 0.0540\n",
            "[2/1][27/59] Loss_D: 0.5676 Loss_G: 2.0113 D(x): 0.6841 D(G(z)): 0.1130 / 0.1673\n",
            "[2/1][28/59] Loss_D: 0.4884 Loss_G: 3.2556 D(x): 0.8993 D(G(z)): 0.2909 / 0.0541\n",
            "[2/1][29/59] Loss_D: 0.4115 Loss_G: 2.6990 D(x): 0.7880 D(G(z)): 0.1159 / 0.0886\n",
            "[2/1][30/59] Loss_D: 0.4291 Loss_G: 3.0934 D(x): 0.8643 D(G(z)): 0.2169 / 0.0620\n",
            "[2/1][31/59] Loss_D: 0.3915 Loss_G: 2.8029 D(x): 0.8281 D(G(z)): 0.1574 / 0.0816\n",
            "[2/1][32/59] Loss_D: 0.4036 Loss_G: 2.6515 D(x): 0.8334 D(G(z)): 0.1715 / 0.0930\n",
            "[2/1][33/59] Loss_D: 0.4294 Loss_G: 2.5167 D(x): 0.8198 D(G(z)): 0.1742 / 0.1055\n",
            "[2/1][34/59] Loss_D: 0.3931 Loss_G: 3.2423 D(x): 0.8702 D(G(z)): 0.1992 / 0.0596\n",
            "[2/1][35/59] Loss_D: 0.4512 Loss_G: 1.4407 D(x): 0.7625 D(G(z)): 0.1285 / 0.2849\n",
            "[2/1][36/59] Loss_D: 0.7663 Loss_G: 6.4399 D(x): 0.9327 D(G(z)): 0.4560 / 0.0051\n",
            "[2/1][37/59] Loss_D: 2.3312 Loss_G: 0.2869 D(x): 0.1729 D(G(z)): 0.0128 / 0.7724\n",
            "[2/1][38/59] Loss_D: 2.0459 Loss_G: 5.9537 D(x): 0.9875 D(G(z)): 0.8304 / 0.0174\n",
            "[2/1][39/59] Loss_D: 4.3632 Loss_G: 1.4894 D(x): 0.0428 D(G(z)): 0.0283 / 0.2880\n",
            "[2/1][40/59] Loss_D: 1.1125 Loss_G: 0.4126 D(x): 0.6647 D(G(z)): 0.4333 / 0.6854\n",
            "[2/1][41/59] Loss_D: 1.6749 Loss_G: 2.8071 D(x): 0.9184 D(G(z)): 0.7527 / 0.0821\n",
            "[2/1][42/59] Loss_D: 1.1364 Loss_G: 1.6976 D(x): 0.4263 D(G(z)): 0.1260 / 0.2284\n",
            "[2/1][43/59] Loss_D: 0.9004 Loss_G: 1.5232 D(x): 0.7522 D(G(z)): 0.4020 / 0.2599\n",
            "[2/1][44/59] Loss_D: 0.9507 Loss_G: 2.1479 D(x): 0.7280 D(G(z)): 0.4246 / 0.1430\n",
            "[2/1][45/59] Loss_D: 1.0240 Loss_G: 1.1730 D(x): 0.5318 D(G(z)): 0.2487 / 0.3465\n",
            "[2/1][46/59] Loss_D: 1.0474 Loss_G: 2.4561 D(x): 0.7544 D(G(z)): 0.4894 / 0.1127\n",
            "[2/1][47/59] Loss_D: 0.9664 Loss_G: 1.1004 D(x): 0.5131 D(G(z)): 0.1754 / 0.3651\n",
            "[2/1][48/59] Loss_D: 1.0224 Loss_G: 3.3303 D(x): 0.8461 D(G(z)): 0.5374 / 0.0511\n",
            "[2/1][49/59] Loss_D: 0.9907 Loss_G: 1.0104 D(x): 0.4764 D(G(z)): 0.1038 / 0.3963\n",
            "[2/1][50/59] Loss_D: 1.0696 Loss_G: 3.5889 D(x): 0.8744 D(G(z)): 0.5668 / 0.0372\n",
            "[2/1][51/59] Loss_D: 1.1211 Loss_G: 1.0355 D(x): 0.4136 D(G(z)): 0.0710 / 0.3926\n",
            "[2/1][52/59] Loss_D: 0.9223 Loss_G: 3.7684 D(x): 0.9051 D(G(z)): 0.5274 / 0.0341\n",
            "[2/1][53/59] Loss_D: 0.7552 Loss_G: 1.8319 D(x): 0.5603 D(G(z)): 0.0738 / 0.1997\n",
            "[2/1][54/59] Loss_D: 0.7983 Loss_G: 3.4605 D(x): 0.8942 D(G(z)): 0.4485 / 0.0454\n",
            "[2/1][55/59] Loss_D: 0.8608 Loss_G: 1.3785 D(x): 0.5594 D(G(z)): 0.1409 / 0.2864\n",
            "[2/1][56/59] Loss_D: 0.7714 Loss_G: 3.9592 D(x): 0.8615 D(G(z)): 0.4285 / 0.0239\n",
            "[2/1][57/59] Loss_D: 0.8063 Loss_G: 1.2678 D(x): 0.5328 D(G(z)): 0.0490 / 0.3160\n",
            "[2/1][58/59] Loss_D: 0.9340 Loss_G: 4.7247 D(x): 0.9421 D(G(z)): 0.5391 / 0.0160\n",
            "[3/1][0/59] Loss_D: 0.8500 Loss_G: 1.5234 D(x): 0.5230 D(G(z)): 0.0565 / 0.2614\n",
            "[3/1][1/59] Loss_D: 0.8300 Loss_G: 4.4829 D(x): 0.9357 D(G(z)): 0.4918 / 0.0187\n",
            "[3/1][2/59] Loss_D: 0.9775 Loss_G: 1.2565 D(x): 0.4564 D(G(z)): 0.0386 / 0.3355\n",
            "[3/1][3/59] Loss_D: 0.9466 Loss_G: 5.3074 D(x): 0.9715 D(G(z)): 0.5481 / 0.0090\n",
            "[3/1][4/59] Loss_D: 1.0014 Loss_G: 1.0549 D(x): 0.4476 D(G(z)): 0.0515 / 0.3971\n",
            "[3/1][5/59] Loss_D: 1.2220 Loss_G: 5.4247 D(x): 0.9669 D(G(z)): 0.6467 / 0.0112\n",
            "[3/1][6/59] Loss_D: 2.3271 Loss_G: 0.7970 D(x): 0.1662 D(G(z)): 0.0254 / 0.4896\n",
            "[3/1][7/59] Loss_D: 1.2411 Loss_G: 3.3559 D(x): 0.9622 D(G(z)): 0.6560 / 0.0550\n",
            "[3/1][8/59] Loss_D: 0.6527 Loss_G: 2.7541 D(x): 0.6749 D(G(z)): 0.1630 / 0.0901\n",
            "[3/1][9/59] Loss_D: 0.7669 Loss_G: 1.7581 D(x): 0.6958 D(G(z)): 0.2750 / 0.2126\n",
            "[3/1][10/59] Loss_D: 0.8783 Loss_G: 3.4009 D(x): 0.7892 D(G(z)): 0.4246 / 0.0535\n",
            "[3/1][11/59] Loss_D: 0.8898 Loss_G: 1.1977 D(x): 0.5011 D(G(z)): 0.0916 / 0.3365\n",
            "[3/1][12/59] Loss_D: 0.8034 Loss_G: 4.7795 D(x): 0.9475 D(G(z)): 0.4937 / 0.0179\n",
            "[3/1][13/59] Loss_D: 0.6890 Loss_G: 2.1639 D(x): 0.5803 D(G(z)): 0.0479 / 0.1477\n",
            "[3/1][14/59] Loss_D: 0.5744 Loss_G: 2.9003 D(x): 0.9193 D(G(z)): 0.3541 / 0.0825\n",
            "[3/1][15/59] Loss_D: 0.5381 Loss_G: 2.4568 D(x): 0.7724 D(G(z)): 0.2042 / 0.1136\n",
            "[3/1][16/59] Loss_D: 0.5770 Loss_G: 2.1026 D(x): 0.7673 D(G(z)): 0.2342 / 0.1487\n",
            "[3/1][17/59] Loss_D: 0.5103 Loss_G: 3.1704 D(x): 0.8372 D(G(z)): 0.2577 / 0.0543\n",
            "[3/1][18/59] Loss_D: 0.4413 Loss_G: 1.9925 D(x): 0.7394 D(G(z)): 0.1008 / 0.1640\n",
            "[3/1][19/59] Loss_D: 0.4811 Loss_G: 3.9845 D(x): 0.9200 D(G(z)): 0.3037 / 0.0278\n",
            "[3/1][20/59] Loss_D: 0.4628 Loss_G: 2.0911 D(x): 0.7073 D(G(z)): 0.0631 / 0.1532\n",
            "[3/1][21/59] Loss_D: 0.4451 Loss_G: 4.4765 D(x): 0.9334 D(G(z)): 0.2929 / 0.0195\n",
            "[3/1][22/59] Loss_D: 0.6941 Loss_G: 2.7650 D(x): 0.5959 D(G(z)): 0.0657 / 0.1083\n",
            "[3/1][23/59] Loss_D: 0.6017 Loss_G: 0.8290 D(x): 0.6815 D(G(z)): 0.1416 / 0.4671\n",
            "[3/1][24/59] Loss_D: 0.9561 Loss_G: 4.3390 D(x): 0.9528 D(G(z)): 0.5601 / 0.0276\n",
            "[3/1][25/59] Loss_D: 1.7521 Loss_G: 0.3933 D(x): 0.2323 D(G(z)): 0.0416 / 0.6992\n",
            "[3/1][26/59] Loss_D: 1.7170 Loss_G: 3.1568 D(x): 0.9777 D(G(z)): 0.7576 / 0.0650\n",
            "[3/1][27/59] Loss_D: 0.7916 Loss_G: 2.0703 D(x): 0.5481 D(G(z)): 0.0996 / 0.1549\n",
            "[3/1][28/59] Loss_D: 0.6230 Loss_G: 1.2619 D(x): 0.7310 D(G(z)): 0.2348 / 0.3189\n",
            "[3/1][29/59] Loss_D: 0.7322 Loss_G: 2.4645 D(x): 0.8573 D(G(z)): 0.4075 / 0.1056\n",
            "[3/1][30/59] Loss_D: 0.7134 Loss_G: 1.5343 D(x): 0.6154 D(G(z)): 0.1529 / 0.2488\n",
            "[3/1][31/59] Loss_D: 0.6575 Loss_G: 2.1092 D(x): 0.8237 D(G(z)): 0.3408 / 0.1467\n",
            "[3/1][32/59] Loss_D: 0.6066 Loss_G: 1.9719 D(x): 0.7362 D(G(z)): 0.2253 / 0.1676\n",
            "[3/1][33/59] Loss_D: 0.6259 Loss_G: 1.7636 D(x): 0.7444 D(G(z)): 0.2486 / 0.2018\n",
            "[3/1][34/59] Loss_D: 0.6071 Loss_G: 2.2736 D(x): 0.7904 D(G(z)): 0.2839 / 0.1269\n",
            "[3/1][35/59] Loss_D: 0.5635 Loss_G: 1.7042 D(x): 0.7275 D(G(z)): 0.1908 / 0.2152\n",
            "[3/1][36/59] Loss_D: 0.5597 Loss_G: 2.8298 D(x): 0.8548 D(G(z)): 0.3054 / 0.0785\n",
            "[3/1][37/59] Loss_D: 0.5625 Loss_G: 1.3793 D(x): 0.6774 D(G(z)): 0.1242 / 0.2920\n",
            "[3/1][38/59] Loss_D: 0.6413 Loss_G: 3.9006 D(x): 0.9210 D(G(z)): 0.3997 / 0.0281\n",
            "[3/1][39/59] Loss_D: 0.8047 Loss_G: 0.7388 D(x): 0.5046 D(G(z)): 0.0498 / 0.5146\n",
            "[3/1][40/59] Loss_D: 1.0874 Loss_G: 4.9836 D(x): 0.9676 D(G(z)): 0.6087 / 0.0099\n",
            "[3/1][41/59] Loss_D: 1.4118 Loss_G: 0.8540 D(x): 0.2932 D(G(z)): 0.0178 / 0.4712\n",
            "[3/1][42/59] Loss_D: 0.9751 Loss_G: 3.3019 D(x): 0.9656 D(G(z)): 0.5614 / 0.0482\n",
            "[3/1][43/59] Loss_D: 0.5264 Loss_G: 2.3553 D(x): 0.6769 D(G(z)): 0.0823 / 0.1181\n",
            "[3/1][44/59] Loss_D: 0.3986 Loss_G: 2.0066 D(x): 0.8383 D(G(z)): 0.1785 / 0.1653\n",
            "[3/1][45/59] Loss_D: 0.4169 Loss_G: 2.8591 D(x): 0.8809 D(G(z)): 0.2342 / 0.0718\n",
            "[3/1][46/59] Loss_D: 0.4241 Loss_G: 2.0615 D(x): 0.7674 D(G(z)): 0.1198 / 0.1572\n",
            "[3/1][47/59] Loss_D: 0.4175 Loss_G: 2.8003 D(x): 0.8789 D(G(z)): 0.2325 / 0.0794\n",
            "[3/1][48/59] Loss_D: 0.4141 Loss_G: 1.8186 D(x): 0.7688 D(G(z)): 0.1171 / 0.1951\n",
            "[3/1][49/59] Loss_D: 0.4712 Loss_G: 3.9607 D(x): 0.9164 D(G(z)): 0.2975 / 0.0283\n",
            "[3/1][50/59] Loss_D: 0.6861 Loss_G: 0.7046 D(x): 0.5766 D(G(z)): 0.0504 / 0.5367\n",
            "[3/1][51/59] Loss_D: 1.1217 Loss_G: 6.8520 D(x): 0.9764 D(G(z)): 0.6168 / 0.0028\n",
            "[3/1][52/59] Loss_D: 2.7464 Loss_G: 3.2855 D(x): 0.1033 D(G(z)): 0.0098 / 0.0564\n",
            "[3/1][53/59] Loss_D: 1.3442 Loss_G: 0.0706 D(x): 0.3263 D(G(z)): 0.0861 / 0.9349\n",
            "[3/1][54/59] Loss_D: 3.5579 Loss_G: 2.0283 D(x): 0.9924 D(G(z)): 0.9416 / 0.1612\n",
            "[3/1][55/59] Loss_D: 0.7995 Loss_G: 2.8389 D(x): 0.6404 D(G(z)): 0.2401 / 0.0790\n",
            "[3/1][56/59] Loss_D: 1.2225 Loss_G: 0.3937 D(x): 0.3884 D(G(z)): 0.1315 / 0.6897\n",
            "[3/1][57/59] Loss_D: 1.7997 Loss_G: 2.5294 D(x): 0.9266 D(G(z)): 0.7897 / 0.1040\n",
            "[3/1][58/59] Loss_D: 1.3003 Loss_G: 1.0614 D(x): 0.3782 D(G(z)): 0.1504 / 0.3800\n",
            "[4/1][0/59] Loss_D: 1.0176 Loss_G: 1.5321 D(x): 0.7471 D(G(z)): 0.4780 / 0.2431\n",
            "[4/1][1/59] Loss_D: 0.9939 Loss_G: 1.4192 D(x): 0.5967 D(G(z)): 0.3324 / 0.2731\n",
            "[4/1][2/59] Loss_D: 1.0124 Loss_G: 1.3198 D(x): 0.6250 D(G(z)): 0.3757 / 0.2977\n",
            "[4/1][3/59] Loss_D: 0.9921 Loss_G: 1.5256 D(x): 0.6471 D(G(z)): 0.3865 / 0.2466\n",
            "[4/1][4/59] Loss_D: 0.9284 Loss_G: 1.2660 D(x): 0.6129 D(G(z)): 0.3153 / 0.3131\n",
            "[4/1][5/59] Loss_D: 0.9494 Loss_G: 2.0070 D(x): 0.7201 D(G(z)): 0.4279 / 0.1590\n",
            "[4/1][6/59] Loss_D: 1.0104 Loss_G: 0.8352 D(x): 0.5235 D(G(z)): 0.2396 / 0.4686\n",
            "[4/1][7/59] Loss_D: 1.1417 Loss_G: 2.8973 D(x): 0.8190 D(G(z)): 0.5718 / 0.0746\n",
            "[4/1][8/59] Loss_D: 1.3657 Loss_G: 0.5642 D(x): 0.3400 D(G(z)): 0.1128 / 0.5998\n",
            "[4/1][9/59] Loss_D: 1.3589 Loss_G: 3.0840 D(x): 0.8975 D(G(z)): 0.6732 / 0.0574\n",
            "[4/1][10/59] Loss_D: 1.1300 Loss_G: 1.0722 D(x): 0.3912 D(G(z)): 0.0878 / 0.3732\n",
            "[4/1][11/59] Loss_D: 0.8949 Loss_G: 2.3408 D(x): 0.8425 D(G(z)): 0.4849 / 0.1186\n",
            "[4/1][12/59] Loss_D: 0.7839 Loss_G: 1.5887 D(x): 0.6085 D(G(z)): 0.1988 / 0.2348\n",
            "[4/1][13/59] Loss_D: 0.8685 Loss_G: 1.9593 D(x): 0.7426 D(G(z)): 0.3932 / 0.1690\n",
            "[4/1][14/59] Loss_D: 0.9505 Loss_G: 1.4922 D(x): 0.5953 D(G(z)): 0.2963 / 0.2583\n",
            "[4/1][15/59] Loss_D: 0.9616 Loss_G: 2.1747 D(x): 0.6957 D(G(z)): 0.4092 / 0.1412\n",
            "[4/1][16/59] Loss_D: 0.8968 Loss_G: 1.2852 D(x): 0.5706 D(G(z)): 0.2336 / 0.3135\n",
            "[4/1][17/59] Loss_D: 0.9268 Loss_G: 3.3700 D(x): 0.8181 D(G(z)): 0.4773 / 0.0502\n",
            "[4/1][18/59] Loss_D: 1.0960 Loss_G: 0.6723 D(x): 0.4266 D(G(z)): 0.1190 / 0.5477\n",
            "[4/1][19/59] Loss_D: 1.3757 Loss_G: 4.2073 D(x): 0.9163 D(G(z)): 0.6742 / 0.0213\n",
            "[4/1][20/59] Loss_D: 1.5656 Loss_G: 0.6633 D(x): 0.2671 D(G(z)): 0.0437 / 0.5507\n",
            "[4/1][21/59] Loss_D: 1.3992 Loss_G: 3.4999 D(x): 0.9164 D(G(z)): 0.6792 / 0.0503\n",
            "[4/1][22/59] Loss_D: 1.2013 Loss_G: 1.0869 D(x): 0.3902 D(G(z)): 0.0913 / 0.3812\n",
            "[4/1][23/59] Loss_D: 1.0548 Loss_G: 2.5469 D(x): 0.8401 D(G(z)): 0.5345 / 0.1071\n",
            "[4/1][24/59] Loss_D: 1.0269 Loss_G: 1.1076 D(x): 0.4950 D(G(z)): 0.1873 / 0.3673\n",
            "[4/1][25/59] Loss_D: 0.9648 Loss_G: 2.3248 D(x): 0.7702 D(G(z)): 0.4659 / 0.1246\n",
            "[4/1][26/59] Loss_D: 0.9101 Loss_G: 1.1126 D(x): 0.5355 D(G(z)): 0.1887 / 0.3686\n",
            "[4/1][27/59] Loss_D: 0.8523 Loss_G: 2.8637 D(x): 0.8226 D(G(z)): 0.4427 / 0.0793\n",
            "[4/1][28/59] Loss_D: 0.8542 Loss_G: 1.0358 D(x): 0.5386 D(G(z)): 0.1258 / 0.3980\n",
            "[4/1][29/59] Loss_D: 1.0276 Loss_G: 3.4827 D(x): 0.8760 D(G(z)): 0.5452 / 0.0456\n",
            "[4/1][30/59] Loss_D: 1.2261 Loss_G: 0.6065 D(x): 0.3855 D(G(z)): 0.0864 / 0.5830\n",
            "[4/1][31/59] Loss_D: 1.4297 Loss_G: 3.4049 D(x): 0.9200 D(G(z)): 0.6894 / 0.0474\n",
            "[4/1][32/59] Loss_D: 1.1772 Loss_G: 1.1114 D(x): 0.3910 D(G(z)): 0.0920 / 0.3721\n",
            "[4/1][33/59] Loss_D: 0.8955 Loss_G: 2.4827 D(x): 0.8681 D(G(z)): 0.4894 / 0.1040\n",
            "[4/1][34/59] Loss_D: 0.7347 Loss_G: 1.7177 D(x): 0.6315 D(G(z)): 0.1932 / 0.2132\n",
            "[4/1][35/59] Loss_D: 0.8004 Loss_G: 2.1646 D(x): 0.7595 D(G(z)): 0.3667 / 0.1415\n",
            "[4/1][36/59] Loss_D: 0.8374 Loss_G: 1.5511 D(x): 0.6295 D(G(z)): 0.2627 / 0.2509\n",
            "[4/1][37/59] Loss_D: 0.8588 Loss_G: 2.4632 D(x): 0.7416 D(G(z)): 0.3877 / 0.1096\n",
            "[4/1][38/59] Loss_D: 0.8771 Loss_G: 0.8919 D(x): 0.5467 D(G(z)): 0.1753 / 0.4436\n",
            "[4/1][39/59] Loss_D: 0.9762 Loss_G: 3.8483 D(x): 0.8871 D(G(z)): 0.5379 / 0.0292\n",
            "[4/1][40/59] Loss_D: 1.2783 Loss_G: 0.5281 D(x): 0.3539 D(G(z)): 0.0591 / 0.6170\n",
            "[4/1][41/59] Loss_D: 1.4488 Loss_G: 3.5442 D(x): 0.9443 D(G(z)): 0.7136 / 0.0389\n",
            "[4/1][42/59] Loss_D: 1.4535 Loss_G: 0.7778 D(x): 0.3033 D(G(z)): 0.0881 / 0.4973\n",
            "[4/1][43/59] Loss_D: 1.1437 Loss_G: 3.0147 D(x): 0.8856 D(G(z)): 0.5991 / 0.0675\n",
            "[4/1][44/59] Loss_D: 1.0692 Loss_G: 0.8870 D(x): 0.4396 D(G(z)): 0.1190 / 0.4531\n",
            "[4/1][45/59] Loss_D: 0.9880 Loss_G: 2.6242 D(x): 0.8754 D(G(z)): 0.5322 / 0.0957\n",
            "[4/1][46/59] Loss_D: 0.7943 Loss_G: 1.4470 D(x): 0.5758 D(G(z)): 0.1461 / 0.2710\n",
            "[4/1][47/59] Loss_D: 0.7222 Loss_G: 2.0653 D(x): 0.8139 D(G(z)): 0.3712 / 0.1526\n",
            "[4/1][48/59] Loss_D: 0.7168 Loss_G: 1.6130 D(x): 0.6743 D(G(z)): 0.2358 / 0.2296\n",
            "[4/1][49/59] Loss_D: 0.7114 Loss_G: 2.1096 D(x): 0.7578 D(G(z)): 0.3205 / 0.1482\n",
            "[4/1][50/59] Loss_D: 0.7133 Loss_G: 1.3211 D(x): 0.6613 D(G(z)): 0.2204 / 0.2981\n",
            "[4/1][51/59] Loss_D: 0.7146 Loss_G: 3.1045 D(x): 0.8251 D(G(z)): 0.3790 / 0.0608\n",
            "[4/1][52/59] Loss_D: 0.8977 Loss_G: 0.5041 D(x): 0.4962 D(G(z)): 0.0998 / 0.6275\n",
            "[4/1][53/59] Loss_D: 1.3247 Loss_G: 4.3275 D(x): 0.9539 D(G(z)): 0.6867 / 0.0225\n",
            "[4/1][54/59] Loss_D: 1.5847 Loss_G: 0.8836 D(x): 0.2644 D(G(z)): 0.0390 / 0.4563\n",
            "[4/1][55/59] Loss_D: 0.9996 Loss_G: 2.7374 D(x): 0.9082 D(G(z)): 0.5521 / 0.0922\n",
            "[4/1][56/59] Loss_D: 0.8885 Loss_G: 0.9766 D(x): 0.5342 D(G(z)): 0.1586 / 0.4124\n",
            "[4/1][57/59] Loss_D: 0.9580 Loss_G: 2.7818 D(x): 0.8743 D(G(z)): 0.5229 / 0.0792\n",
            "[4/1][58/59] Loss_D: 0.9956 Loss_G: 0.9255 D(x): 0.4607 D(G(z)): 0.1283 / 0.4292\n",
            "[5/1][0/59] Loss_D: 0.9367 Loss_G: 2.4964 D(x): 0.8690 D(G(z)): 0.5120 / 0.0997\n",
            "[5/1][1/59] Loss_D: 0.8243 Loss_G: 1.2531 D(x): 0.5516 D(G(z)): 0.1544 / 0.3199\n",
            "[5/1][2/59] Loss_D: 0.7327 Loss_G: 2.8955 D(x): 0.8495 D(G(z)): 0.4043 / 0.0729\n",
            "[5/1][3/59] Loss_D: 0.7737 Loss_G: 0.8985 D(x): 0.5592 D(G(z)): 0.1214 / 0.4382\n",
            "[5/1][4/59] Loss_D: 0.9379 Loss_G: 4.1126 D(x): 0.9072 D(G(z)): 0.5383 / 0.0218\n",
            "[5/1][5/59] Loss_D: 2.0489 Loss_G: 0.3571 D(x): 0.1590 D(G(z)): 0.0367 / 0.7222\n",
            "[5/1][6/59] Loss_D: 1.7881 Loss_G: 9.2005 D(x): 0.9685 D(G(z)): 0.7746 / 0.0002\n",
            "[5/1][7/59] Loss_D: 6.9007 Loss_G: 0.3169 D(x): 0.0029 D(G(z)): 0.0035 / 0.7455\n",
            "[5/1][8/59] Loss_D: 1.6494 Loss_G: 2.3508 D(x): 0.9256 D(G(z)): 0.7467 / 0.1256\n",
            "[5/1][9/59] Loss_D: 1.1495 Loss_G: 1.5998 D(x): 0.4492 D(G(z)): 0.1972 / 0.2411\n",
            "[5/1][10/59] Loss_D: 1.1077 Loss_G: 1.0987 D(x): 0.6243 D(G(z)): 0.4125 / 0.3655\n",
            "[5/1][11/59] Loss_D: 1.1485 Loss_G: 1.6580 D(x): 0.6735 D(G(z)): 0.4933 / 0.2107\n",
            "[5/1][12/59] Loss_D: 1.1952 Loss_G: 0.7795 D(x): 0.4816 D(G(z)): 0.3174 / 0.4788\n",
            "[5/1][13/59] Loss_D: 1.2277 Loss_G: 1.9929 D(x): 0.7376 D(G(z)): 0.5746 / 0.1624\n",
            "[5/1][14/59] Loss_D: 1.3182 Loss_G: 0.5537 D(x): 0.3895 D(G(z)): 0.2356 / 0.5955\n",
            "[5/1][15/59] Loss_D: 1.4019 Loss_G: 2.1841 D(x): 0.8110 D(G(z)): 0.6671 / 0.1423\n",
            "[5/1][16/59] Loss_D: 1.3834 Loss_G: 0.7606 D(x): 0.3664 D(G(z)): 0.2155 / 0.4953\n",
            "[5/1][17/59] Loss_D: 1.2611 Loss_G: 1.5558 D(x): 0.7374 D(G(z)): 0.5747 / 0.2414\n",
            "[5/1][18/59] Loss_D: 1.1343 Loss_G: 1.1904 D(x): 0.5137 D(G(z)): 0.3186 / 0.3311\n",
            "[5/1][19/59] Loss_D: 1.0577 Loss_G: 1.3767 D(x): 0.6377 D(G(z)): 0.4172 / 0.2845\n",
            "[5/1][20/59] Loss_D: 1.0070 Loss_G: 1.3749 D(x): 0.6082 D(G(z)): 0.3578 / 0.2859\n",
            "[5/1][21/59] Loss_D: 1.0043 Loss_G: 1.4587 D(x): 0.6475 D(G(z)): 0.3930 / 0.2650\n",
            "[5/1][22/59] Loss_D: 0.9816 Loss_G: 1.4363 D(x): 0.6318 D(G(z)): 0.3640 / 0.2713\n",
            "[5/1][23/59] Loss_D: 0.9854 Loss_G: 1.3244 D(x): 0.6222 D(G(z)): 0.3586 / 0.2922\n",
            "[5/1][24/59] Loss_D: 0.9477 Loss_G: 1.6115 D(x): 0.6624 D(G(z)): 0.3819 / 0.2209\n",
            "[5/1][25/59] Loss_D: 0.8648 Loss_G: 1.3533 D(x): 0.6229 D(G(z)): 0.2942 / 0.2827\n",
            "[5/1][26/59] Loss_D: 0.8566 Loss_G: 1.8961 D(x): 0.7309 D(G(z)): 0.3924 / 0.1721\n",
            "[5/1][27/59] Loss_D: 0.9033 Loss_G: 1.0942 D(x): 0.5935 D(G(z)): 0.2753 / 0.3650\n",
            "[5/1][28/59] Loss_D: 0.9935 Loss_G: 2.1122 D(x): 0.7525 D(G(z)): 0.4700 / 0.1453\n",
            "[5/1][29/59] Loss_D: 1.0961 Loss_G: 0.6617 D(x): 0.4646 D(G(z)): 0.2193 / 0.5421\n",
            "[5/1][30/59] Loss_D: 1.1786 Loss_G: 2.9578 D(x): 0.8540 D(G(z)): 0.6059 / 0.0679\n",
            "[5/1][31/59] Loss_D: 1.3150 Loss_G: 0.6784 D(x): 0.3371 D(G(z)): 0.1051 / 0.5406\n",
            "[5/1][32/59] Loss_D: 1.1896 Loss_G: 2.5212 D(x): 0.8885 D(G(z)): 0.6218 / 0.0976\n",
            "[5/1][33/59] Loss_D: 1.1349 Loss_G: 0.7699 D(x): 0.4066 D(G(z)): 0.1348 / 0.4934\n",
            "[5/1][34/59] Loss_D: 1.1325 Loss_G: 2.2018 D(x): 0.8428 D(G(z)): 0.5812 / 0.1295\n",
            "[5/1][35/59] Loss_D: 1.0289 Loss_G: 0.9268 D(x): 0.4740 D(G(z)): 0.1806 / 0.4208\n",
            "[5/1][36/59] Loss_D: 0.9602 Loss_G: 2.0647 D(x): 0.7969 D(G(z)): 0.4914 / 0.1468\n",
            "[5/1][37/59] Loss_D: 0.9187 Loss_G: 0.9601 D(x): 0.5258 D(G(z)): 0.1903 / 0.4063\n",
            "[5/1][38/59] Loss_D: 0.8513 Loss_G: 2.1317 D(x): 0.8128 D(G(z)): 0.4484 / 0.1409\n",
            "[5/1][39/59] Loss_D: 0.8162 Loss_G: 1.0981 D(x): 0.5802 D(G(z)): 0.1934 / 0.3625\n",
            "[5/1][40/59] Loss_D: 0.8217 Loss_G: 2.1814 D(x): 0.8133 D(G(z)): 0.4302 / 0.1320\n",
            "[5/1][41/59] Loss_D: 0.8566 Loss_G: 0.9387 D(x): 0.5542 D(G(z)): 0.1851 / 0.4214\n",
            "[5/1][42/59] Loss_D: 0.9605 Loss_G: 2.5012 D(x): 0.8331 D(G(z)): 0.5108 / 0.1002\n",
            "[5/1][43/59] Loss_D: 1.0247 Loss_G: 0.7809 D(x): 0.4473 D(G(z)): 0.1281 / 0.4854\n",
            "[5/1][44/59] Loss_D: 0.9481 Loss_G: 2.6567 D(x): 0.8702 D(G(z)): 0.5233 / 0.0900\n",
            "[5/1][45/59] Loss_D: 1.2588 Loss_G: 0.3311 D(x): 0.3597 D(G(z)): 0.1135 / 0.7365\n",
            "[5/1][46/59] Loss_D: 1.6742 Loss_G: 3.4807 D(x): 0.9545 D(G(z)): 0.7670 / 0.0410\n",
            "[5/1][47/59] Loss_D: 1.7800 Loss_G: 0.7053 D(x): 0.2127 D(G(z)): 0.0613 / 0.5228\n",
            "[5/1][48/59] Loss_D: 1.1221 Loss_G: 1.9882 D(x): 0.8256 D(G(z)): 0.5656 / 0.1586\n",
            "[5/1][49/59] Loss_D: 1.1861 Loss_G: 0.6404 D(x): 0.4316 D(G(z)): 0.2172 / 0.5458\n",
            "[5/1][50/59] Loss_D: 1.2040 Loss_G: 2.0123 D(x): 0.8036 D(G(z)): 0.5982 / 0.1556\n",
            "[5/1][51/59] Loss_D: 1.2107 Loss_G: 0.5857 D(x): 0.4005 D(G(z)): 0.1892 / 0.5738\n",
            "[5/1][52/59] Loss_D: 1.1682 Loss_G: 2.0915 D(x): 0.8490 D(G(z)): 0.6058 / 0.1417\n",
            "[5/1][53/59] Loss_D: 1.0058 Loss_G: 0.9497 D(x): 0.4795 D(G(z)): 0.1871 / 0.4093\n",
            "[5/1][54/59] Loss_D: 0.9203 Loss_G: 1.7006 D(x): 0.7929 D(G(z)): 0.4747 / 0.2038\n",
            "[5/1][55/59] Loss_D: 0.8544 Loss_G: 1.1539 D(x): 0.5997 D(G(z)): 0.2570 / 0.3400\n",
            "[5/1][56/59] Loss_D: 0.8538 Loss_G: 1.7526 D(x): 0.7458 D(G(z)): 0.4067 / 0.1949\n",
            "[5/1][57/59] Loss_D: 0.8525 Loss_G: 1.0497 D(x): 0.5962 D(G(z)): 0.2530 / 0.3727\n",
            "[5/1][58/59] Loss_D: 0.9017 Loss_G: 2.4295 D(x): 0.7761 D(G(z)): 0.4521 / 0.1070\n",
            "[6/1][0/59] Loss_D: 1.0571 Loss_G: 0.4986 D(x): 0.4358 D(G(z)): 0.1285 / 0.6246\n",
            "[6/1][1/59] Loss_D: 1.2673 Loss_G: 2.9920 D(x): 0.9064 D(G(z)): 0.6613 / 0.0616\n",
            "[6/1][2/59] Loss_D: 1.2909 Loss_G: 0.7580 D(x): 0.3378 D(G(z)): 0.0841 / 0.4959\n",
            "[6/1][3/59] Loss_D: 1.0048 Loss_G: 2.0301 D(x): 0.8627 D(G(z)): 0.5439 / 0.1531\n",
            "[6/1][4/59] Loss_D: 0.8823 Loss_G: 1.0977 D(x): 0.5506 D(G(z)): 0.2021 / 0.3581\n",
            "[6/1][5/59] Loss_D: 0.8186 Loss_G: 1.6696 D(x): 0.7785 D(G(z)): 0.4102 / 0.2108\n",
            "[6/1][6/59] Loss_D: 0.7944 Loss_G: 1.3015 D(x): 0.6341 D(G(z)): 0.2584 / 0.2960\n",
            "[6/1][7/59] Loss_D: 0.7184 Loss_G: 1.5559 D(x): 0.7482 D(G(z)): 0.3277 / 0.2312\n",
            "[6/1][8/59] Loss_D: 0.6955 Loss_G: 1.4563 D(x): 0.7007 D(G(z)): 0.2655 / 0.2562\n",
            "[6/1][9/59] Loss_D: 0.6728 Loss_G: 1.5877 D(x): 0.7367 D(G(z)): 0.2857 / 0.2284\n",
            "[6/1][10/59] Loss_D: 0.6685 Loss_G: 1.6090 D(x): 0.7322 D(G(z)): 0.2779 / 0.2250\n",
            "[6/1][11/59] Loss_D: 0.6690 Loss_G: 1.5144 D(x): 0.7247 D(G(z)): 0.2673 / 0.2419\n",
            "[6/1][12/59] Loss_D: 0.6737 Loss_G: 1.6481 D(x): 0.7387 D(G(z)): 0.2875 / 0.2122\n",
            "[6/1][13/59] Loss_D: 0.6658 Loss_G: 1.3215 D(x): 0.7037 D(G(z)): 0.2460 / 0.2946\n",
            "[6/1][14/59] Loss_D: 0.6535 Loss_G: 2.1522 D(x): 0.7946 D(G(z)): 0.3252 / 0.1344\n",
            "[6/1][15/59] Loss_D: 0.7287 Loss_G: 0.7524 D(x): 0.6017 D(G(z)): 0.1663 / 0.4929\n",
            "[6/1][16/59] Loss_D: 0.9053 Loss_G: 3.5026 D(x): 0.8966 D(G(z)): 0.5249 / 0.0378\n",
            "[6/1][17/59] Loss_D: 1.4365 Loss_G: 1.3903 D(x): 0.2879 D(G(z)): 0.0483 / 0.2818\n",
            "[6/1][18/59] Loss_D: 1.0392 Loss_G: 0.5597 D(x): 0.6093 D(G(z)): 0.3721 / 0.5952\n",
            "[6/1][19/59] Loss_D: 1.4853 Loss_G: 5.0169 D(x): 0.8080 D(G(z)): 0.6831 / 0.0103\n",
            "[6/1][20/59] Loss_D: 4.2803 Loss_G: 0.1822 D(x): 0.0220 D(G(z)): 0.0147 / 0.8424\n",
            "[6/1][21/59] Loss_D: 2.3182 Loss_G: 2.0200 D(x): 0.9346 D(G(z)): 0.8582 / 0.1682\n",
            "[6/1][22/59] Loss_D: 1.3764 Loss_G: 1.0406 D(x): 0.3810 D(G(z)): 0.2396 / 0.3899\n",
            "[6/1][23/59] Loss_D: 1.2163 Loss_G: 1.0630 D(x): 0.6184 D(G(z)): 0.4735 / 0.3746\n",
            "[6/1][24/59] Loss_D: 1.1718 Loss_G: 1.2600 D(x): 0.5948 D(G(z)): 0.4389 / 0.3084\n",
            "[6/1][25/59] Loss_D: 1.0996 Loss_G: 1.0328 D(x): 0.5623 D(G(z)): 0.3682 / 0.3838\n",
            "[6/1][26/59] Loss_D: 1.0638 Loss_G: 1.4031 D(x): 0.6562 D(G(z)): 0.4376 / 0.2750\n",
            "[6/1][27/59] Loss_D: 1.0626 Loss_G: 1.0711 D(x): 0.5742 D(G(z)): 0.3532 / 0.3738\n",
            "[6/1][28/59] Loss_D: 1.0961 Loss_G: 1.3792 D(x): 0.6556 D(G(z)): 0.4496 / 0.2837\n",
            "[6/1][29/59] Loss_D: 1.1786 Loss_G: 0.8522 D(x): 0.5202 D(G(z)): 0.3570 / 0.4531\n",
            "[6/1][30/59] Loss_D: 1.1227 Loss_G: 1.8806 D(x): 0.7155 D(G(z)): 0.5132 / 0.1757\n",
            "[6/1][31/59] Loss_D: 1.2194 Loss_G: 0.4836 D(x): 0.4136 D(G(z)): 0.2174 / 0.6350\n",
            "[6/1][32/59] Loss_D: 1.4016 Loss_G: 2.6863 D(x): 0.8540 D(G(z)): 0.6796 / 0.0826\n",
            "[6/1][33/59] Loss_D: 1.5096 Loss_G: 0.5124 D(x): 0.2809 D(G(z)): 0.1120 / 0.6278\n",
            "[6/1][34/59] Loss_D: 1.4661 Loss_G: 2.1118 D(x): 0.8661 D(G(z)): 0.6878 / 0.1533\n",
            "[6/1][35/59] Loss_D: 1.2760 Loss_G: 0.8726 D(x): 0.4038 D(G(z)): 0.2160 / 0.4471\n",
            "[6/1][36/59] Loss_D: 1.1411 Loss_G: 1.6057 D(x): 0.7165 D(G(z)): 0.5156 / 0.2227\n",
            "[6/1][37/59] Loss_D: 1.1169 Loss_G: 0.9628 D(x): 0.5000 D(G(z)): 0.2892 / 0.4097\n",
            "[6/1][38/59] Loss_D: 1.0281 Loss_G: 1.7514 D(x): 0.7007 D(G(z)): 0.4566 / 0.1986\n",
            "[6/1][39/59] Loss_D: 0.9389 Loss_G: 0.9818 D(x): 0.5595 D(G(z)): 0.2614 / 0.4019\n",
            "[6/1][40/59] Loss_D: 0.9588 Loss_G: 2.1039 D(x): 0.7557 D(G(z)): 0.4630 / 0.1445\n",
            "[6/1][41/59] Loss_D: 1.0626 Loss_G: 0.5843 D(x): 0.4783 D(G(z)): 0.2213 / 0.5803\n",
            "[6/1][42/59] Loss_D: 1.2630 Loss_G: 2.6127 D(x): 0.8503 D(G(z)): 0.6301 / 0.0894\n",
            "[6/1][43/59] Loss_D: 1.4525 Loss_G: 0.4990 D(x): 0.3065 D(G(z)): 0.1353 / 0.6261\n",
            "[6/1][44/59] Loss_D: 1.3251 Loss_G: 2.1915 D(x): 0.8831 D(G(z)): 0.6671 / 0.1337\n",
            "[6/1][45/59] Loss_D: 1.1307 Loss_G: 0.9971 D(x): 0.4385 D(G(z)): 0.1880 / 0.4054\n",
            "[6/1][46/59] Loss_D: 0.9794 Loss_G: 1.6181 D(x): 0.7662 D(G(z)): 0.4702 / 0.2264\n",
            "[6/1][47/59] Loss_D: 0.9523 Loss_G: 1.3042 D(x): 0.5920 D(G(z)): 0.3089 / 0.3048\n",
            "[6/1][48/59] Loss_D: 0.9151 Loss_G: 1.4260 D(x): 0.6661 D(G(z)): 0.3652 / 0.2657\n",
            "[6/1][49/59] Loss_D: 0.9196 Loss_G: 1.1784 D(x): 0.6231 D(G(z)): 0.3241 / 0.3359\n",
            "[6/1][50/59] Loss_D: 0.8926 Loss_G: 1.8393 D(x): 0.7163 D(G(z)): 0.4004 / 0.1823\n",
            "[6/1][51/59] Loss_D: 0.9080 Loss_G: 0.6788 D(x): 0.5559 D(G(z)): 0.2294 / 0.5267\n",
            "[6/1][52/59] Loss_D: 1.0595 Loss_G: 2.9749 D(x): 0.8680 D(G(z)): 0.5765 / 0.0648\n",
            "[6/1][53/59] Loss_D: 1.3619 Loss_G: 0.4668 D(x): 0.3154 D(G(z)): 0.0822 / 0.6482\n",
            "[6/1][54/59] Loss_D: 1.4275 Loss_G: 2.9617 D(x): 0.9064 D(G(z)): 0.6957 / 0.0718\n",
            "[6/1][55/59] Loss_D: 1.6383 Loss_G: 0.4541 D(x): 0.2630 D(G(z)): 0.1029 / 0.6586\n",
            "[6/1][56/59] Loss_D: 1.3866 Loss_G: 2.2457 D(x): 0.8915 D(G(z)): 0.6846 / 0.1231\n",
            "[6/1][57/59] Loss_D: 1.2103 Loss_G: 0.7804 D(x): 0.3834 D(G(z)): 0.1585 / 0.4898\n",
            "[6/1][58/59] Loss_D: 1.0944 Loss_G: 1.6142 D(x): 0.7918 D(G(z)): 0.5376 / 0.2249\n",
            "[7/1][0/59] Loss_D: 0.9381 Loss_G: 1.2135 D(x): 0.5777 D(G(z)): 0.2868 / 0.3268\n",
            "[7/1][1/59] Loss_D: 0.9480 Loss_G: 1.3477 D(x): 0.6818 D(G(z)): 0.3961 / 0.2876\n",
            "[7/1][2/59] Loss_D: 0.9679 Loss_G: 1.3343 D(x): 0.6271 D(G(z)): 0.3599 / 0.2882\n",
            "[7/1][3/59] Loss_D: 0.9228 Loss_G: 1.4084 D(x): 0.6487 D(G(z)): 0.3543 / 0.2710\n",
            "[7/1][4/59] Loss_D: 0.8575 Loss_G: 1.3999 D(x): 0.6552 D(G(z)): 0.3202 / 0.2701\n",
            "[7/1][5/59] Loss_D: 0.7962 Loss_G: 1.6155 D(x): 0.6988 D(G(z)): 0.3257 / 0.2261\n",
            "[7/1][6/59] Loss_D: 0.8022 Loss_G: 1.3093 D(x): 0.6627 D(G(z)): 0.2942 / 0.2965\n",
            "[7/1][7/59] Loss_D: 0.7446 Loss_G: 1.8710 D(x): 0.7569 D(G(z)): 0.3476 / 0.1795\n",
            "[7/1][8/59] Loss_D: 0.7913 Loss_G: 1.0742 D(x): 0.6308 D(G(z)): 0.2433 / 0.3683\n",
            "[7/1][9/59] Loss_D: 0.8238 Loss_G: 2.4042 D(x): 0.8097 D(G(z)): 0.4314 / 0.1036\n",
            "[7/1][10/59] Loss_D: 0.9576 Loss_G: 0.4880 D(x): 0.4735 D(G(z)): 0.1395 / 0.6302\n",
            "[7/1][11/59] Loss_D: 1.2551 Loss_G: 3.8371 D(x): 0.9135 D(G(z)): 0.6594 / 0.0275\n",
            "[7/1][12/59] Loss_D: 2.1895 Loss_G: 0.6432 D(x): 0.1468 D(G(z)): 0.0459 / 0.5484\n",
            "[7/1][13/59] Loss_D: 1.1077 Loss_G: 2.8489 D(x): 0.8540 D(G(z)): 0.5840 / 0.0706\n",
            "[7/1][14/59] Loss_D: 1.8873 Loss_G: 0.2237 D(x): 0.1960 D(G(z)): 0.0934 / 0.8120\n",
            "[7/1][15/59] Loss_D: 2.1739 Loss_G: 1.8768 D(x): 0.9200 D(G(z)): 0.8408 / 0.1811\n",
            "[7/1][16/59] Loss_D: 1.3820 Loss_G: 0.9700 D(x): 0.3665 D(G(z)): 0.2255 / 0.4051\n",
            "[7/1][17/59] Loss_D: 1.1504 Loss_G: 0.8328 D(x): 0.6062 D(G(z)): 0.4429 / 0.4541\n",
            "[7/1][18/59] Loss_D: 1.1312 Loss_G: 1.4847 D(x): 0.6661 D(G(z)): 0.4878 / 0.2481\n",
            "[7/1][19/59] Loss_D: 1.1894 Loss_G: 0.6891 D(x): 0.4677 D(G(z)): 0.3071 / 0.5194\n",
            "[7/1][20/59] Loss_D: 1.1998 Loss_G: 1.6218 D(x): 0.7233 D(G(z)): 0.5593 / 0.2193\n",
            "[7/1][21/59] Loss_D: 1.2462 Loss_G: 0.6152 D(x): 0.4215 D(G(z)): 0.2678 / 0.5588\n",
            "[7/1][22/59] Loss_D: 1.2577 Loss_G: 1.7028 D(x): 0.7425 D(G(z)): 0.5900 / 0.2039\n",
            "[7/1][23/59] Loss_D: 1.2716 Loss_G: 0.6394 D(x): 0.4076 D(G(z)): 0.2488 / 0.5436\n",
            "[7/1][24/59] Loss_D: 1.2265 Loss_G: 1.6404 D(x): 0.7484 D(G(z)): 0.5813 / 0.2190\n",
            "[7/1][25/59] Loss_D: 1.1518 Loss_G: 0.8199 D(x): 0.4568 D(G(z)): 0.2575 / 0.4654\n",
            "[7/1][26/59] Loss_D: 1.1338 Loss_G: 1.5767 D(x): 0.7300 D(G(z)): 0.5261 / 0.2317\n",
            "[7/1][27/59] Loss_D: 1.1297 Loss_G: 0.8547 D(x): 0.4962 D(G(z)): 0.2956 / 0.4474\n",
            "[7/1][28/59] Loss_D: 1.1815 Loss_G: 1.3674 D(x): 0.6713 D(G(z)): 0.5104 / 0.2737\n",
            "[7/1][29/59] Loss_D: 1.1116 Loss_G: 0.8946 D(x): 0.5102 D(G(z)): 0.3120 / 0.4325\n",
            "[7/1][30/59] Loss_D: 1.0897 Loss_G: 1.6614 D(x): 0.6906 D(G(z)): 0.4824 / 0.2167\n",
            "[7/1][31/59] Loss_D: 1.0967 Loss_G: 0.7192 D(x): 0.4889 D(G(z)): 0.2661 / 0.5129\n",
            "[7/1][32/59] Loss_D: 1.1996 Loss_G: 2.1460 D(x): 0.7728 D(G(z)): 0.5759 / 0.1396\n",
            "[7/1][33/59] Loss_D: 1.3355 Loss_G: 0.4572 D(x): 0.3593 D(G(z)): 0.1816 / 0.6500\n",
            "[7/1][34/59] Loss_D: 1.4355 Loss_G: 2.3170 D(x): 0.8486 D(G(z)): 0.6871 / 0.1192\n",
            "[7/1][35/59] Loss_D: 1.3714 Loss_G: 0.6116 D(x): 0.3319 D(G(z)): 0.1455 / 0.5609\n",
            "[7/1][36/59] Loss_D: 1.2181 Loss_G: 1.7903 D(x): 0.8179 D(G(z)): 0.6059 / 0.1904\n",
            "[7/1][37/59] Loss_D: 1.0328 Loss_G: 1.0152 D(x): 0.4990 D(G(z)): 0.2381 / 0.3887\n",
            "[7/1][38/59] Loss_D: 0.9898 Loss_G: 1.3487 D(x): 0.7084 D(G(z)): 0.4434 / 0.2863\n",
            "[7/1][39/59] Loss_D: 0.9981 Loss_G: 1.2037 D(x): 0.6038 D(G(z)): 0.3527 / 0.3250\n",
            "[7/1][40/59] Loss_D: 1.0165 Loss_G: 1.2400 D(x): 0.6290 D(G(z)): 0.3913 / 0.3132\n",
            "[7/1][41/59] Loss_D: 1.0185 Loss_G: 1.1849 D(x): 0.6079 D(G(z)): 0.3678 / 0.3303\n",
            "[7/1][42/59] Loss_D: 0.9237 Loss_G: 1.5094 D(x): 0.6672 D(G(z)): 0.3754 / 0.2480\n",
            "[7/1][43/59] Loss_D: 0.8813 Loss_G: 1.1599 D(x): 0.6186 D(G(z)): 0.2960 / 0.3461\n",
            "[7/1][44/59] Loss_D: 0.9179 Loss_G: 1.8203 D(x): 0.7275 D(G(z)): 0.4179 / 0.1871\n",
            "[7/1][45/59] Loss_D: 1.0058 Loss_G: 0.7396 D(x): 0.5323 D(G(z)): 0.2657 / 0.5003\n",
            "[7/1][46/59] Loss_D: 1.1387 Loss_G: 2.6662 D(x): 0.7938 D(G(z)): 0.5666 / 0.0898\n",
            "[7/1][47/59] Loss_D: 1.5331 Loss_G: 0.2513 D(x): 0.2885 D(G(z)): 0.1233 / 0.7879\n",
            "[7/1][48/59] Loss_D: 1.8339 Loss_G: 3.2026 D(x): 0.9401 D(G(z)): 0.8015 / 0.0526\n",
            "[7/1][49/59] Loss_D: 1.6048 Loss_G: 0.7601 D(x): 0.2592 D(G(z)): 0.0781 / 0.5011\n",
            "[7/1][50/59] Loss_D: 1.0963 Loss_G: 1.5959 D(x): 0.8382 D(G(z)): 0.5632 / 0.2338\n",
            "[7/1][51/59] Loss_D: 0.9231 Loss_G: 1.4639 D(x): 0.6150 D(G(z)): 0.3147 / 0.2670\n",
            "[7/1][52/59] Loss_D: 0.9257 Loss_G: 1.1309 D(x): 0.6213 D(G(z)): 0.3215 / 0.3487\n",
            "[7/1][53/59] Loss_D: 0.9248 Loss_G: 1.7501 D(x): 0.7130 D(G(z)): 0.4133 / 0.1982\n",
            "[7/1][54/59] Loss_D: 0.8849 Loss_G: 0.9147 D(x): 0.5675 D(G(z)): 0.2345 / 0.4269\n",
            "[7/1][55/59] Loss_D: 0.9409 Loss_G: 2.5471 D(x): 0.8170 D(G(z)): 0.4933 / 0.0985\n",
            "[7/1][56/59] Loss_D: 1.0020 Loss_G: 0.6320 D(x): 0.4554 D(G(z)): 0.1328 / 0.5607\n",
            "[7/1][57/59] Loss_D: 1.1377 Loss_G: 2.5506 D(x): 0.8888 D(G(z)): 0.6043 / 0.1016\n",
            "[7/1][58/59] Loss_D: 1.1374 Loss_G: 0.7876 D(x): 0.4084 D(G(z)): 0.1416 / 0.4831\n",
            "[8/1][0/59] Loss_D: 1.0507 Loss_G: 2.4455 D(x): 0.8417 D(G(z)): 0.5510 / 0.1045\n",
            "[8/1][1/59] Loss_D: 1.2044 Loss_G: 0.5136 D(x): 0.3829 D(G(z)): 0.1442 / 0.6199\n",
            "[8/1][2/59] Loss_D: 1.2480 Loss_G: 2.8988 D(x): 0.9103 D(G(z)): 0.6509 / 0.0704\n",
            "[8/1][3/59] Loss_D: 1.5798 Loss_G: 0.4582 D(x): 0.2560 D(G(z)): 0.0948 / 0.6538\n",
            "[8/1][4/59] Loss_D: 1.4288 Loss_G: 2.3244 D(x): 0.9111 D(G(z)): 0.7002 / 0.1212\n",
            "[8/1][5/59] Loss_D: 1.1114 Loss_G: 0.9837 D(x): 0.4191 D(G(z)): 0.1538 / 0.4047\n",
            "[8/1][6/59] Loss_D: 0.9206 Loss_G: 1.6909 D(x): 0.7906 D(G(z)): 0.4689 / 0.2094\n",
            "[8/1][7/59] Loss_D: 0.8682 Loss_G: 1.1685 D(x): 0.5974 D(G(z)): 0.2637 / 0.3359\n",
            "[8/1][8/59] Loss_D: 0.8047 Loss_G: 1.6219 D(x): 0.7471 D(G(z)): 0.3770 / 0.2213\n",
            "[8/1][9/59] Loss_D: 0.7749 Loss_G: 1.1970 D(x): 0.6456 D(G(z)): 0.2557 / 0.3270\n",
            "[8/1][10/59] Loss_D: 0.7899 Loss_G: 2.0091 D(x): 0.7814 D(G(z)): 0.3944 / 0.1553\n",
            "[8/1][11/59] Loss_D: 0.8297 Loss_G: 0.8381 D(x): 0.5677 D(G(z)): 0.1935 / 0.4561\n",
            "[8/1][12/59] Loss_D: 0.9058 Loss_G: 2.7749 D(x): 0.8538 D(G(z)): 0.5019 / 0.0772\n",
            "[8/1][13/59] Loss_D: 1.1865 Loss_G: 0.5741 D(x): 0.3714 D(G(z)): 0.0969 / 0.5817\n",
            "[8/1][14/59] Loss_D: 1.1819 Loss_G: 2.6583 D(x): 0.8667 D(G(z)): 0.6194 / 0.0792\n",
            "[8/1][15/59] Loss_D: 1.6290 Loss_G: 0.2635 D(x): 0.2414 D(G(z)): 0.1019 / 0.7786\n",
            "[8/1][16/59] Loss_D: 1.8054 Loss_G: 2.3751 D(x): 0.9308 D(G(z)): 0.7932 / 0.1147\n",
            "[8/1][17/59] Loss_D: 1.3257 Loss_G: 0.9134 D(x): 0.3508 D(G(z)): 0.1534 / 0.4265\n",
            "[8/1][18/59] Loss_D: 1.0394 Loss_G: 1.2840 D(x): 0.7234 D(G(z)): 0.4792 / 0.3009\n",
            "[8/1][19/59] Loss_D: 0.9520 Loss_G: 1.3163 D(x): 0.6195 D(G(z)): 0.3461 / 0.2895\n",
            "[8/1][20/59] Loss_D: 0.9162 Loss_G: 1.1425 D(x): 0.6284 D(G(z)): 0.3374 / 0.3401\n",
            "[8/1][21/59] Loss_D: 0.8670 Loss_G: 1.6799 D(x): 0.7201 D(G(z)): 0.3912 / 0.2079\n",
            "[8/1][22/59] Loss_D: 0.8341 Loss_G: 1.1265 D(x): 0.6200 D(G(z)): 0.2674 / 0.3519\n",
            "[8/1][23/59] Loss_D: 0.8114 Loss_G: 2.2111 D(x): 0.7972 D(G(z)): 0.4162 / 0.1314\n",
            "[8/1][24/59] Loss_D: 0.9680 Loss_G: 0.7669 D(x): 0.5060 D(G(z)): 0.1996 / 0.4902\n",
            "[8/1][25/59] Loss_D: 0.9914 Loss_G: 2.6972 D(x): 0.8480 D(G(z)): 0.5344 / 0.0812\n",
            "[8/1][26/59] Loss_D: 1.4580 Loss_G: 0.3357 D(x): 0.2941 D(G(z)): 0.1180 / 0.7287\n",
            "[8/1][27/59] Loss_D: 1.6839 Loss_G: 2.7370 D(x): 0.9221 D(G(z)): 0.7644 / 0.0846\n",
            "[8/1][28/59] Loss_D: 1.3392 Loss_G: 0.8432 D(x): 0.3292 D(G(z)): 0.1181 / 0.4678\n",
            "[8/1][29/59] Loss_D: 1.0432 Loss_G: 1.7147 D(x): 0.7955 D(G(z)): 0.5200 / 0.2059\n",
            "[8/1][30/59] Loss_D: 0.8152 Loss_G: 1.6166 D(x): 0.6324 D(G(z)): 0.2675 / 0.2225\n",
            "[8/1][31/59] Loss_D: 0.7832 Loss_G: 1.1189 D(x): 0.6513 D(G(z)): 0.2697 / 0.3616\n",
            "[8/1][32/59] Loss_D: 0.8672 Loss_G: 2.2365 D(x): 0.8018 D(G(z)): 0.4418 / 0.1270\n",
            "[8/1][33/59] Loss_D: 0.8770 Loss_G: 0.8412 D(x): 0.5323 D(G(z)): 0.1704 / 0.4588\n",
            "[8/1][34/59] Loss_D: 0.9700 Loss_G: 2.7276 D(x): 0.8580 D(G(z)): 0.5289 / 0.0861\n",
            "[8/1][35/59] Loss_D: 1.1833 Loss_G: 0.5812 D(x): 0.3800 D(G(z)): 0.1177 / 0.5821\n",
            "[8/1][36/59] Loss_D: 1.1554 Loss_G: 3.5401 D(x): 0.9057 D(G(z)): 0.6201 / 0.0373\n",
            "[8/1][37/59] Loss_D: 1.9825 Loss_G: 0.4242 D(x): 0.1804 D(G(z)): 0.0708 / 0.6726\n",
            "[8/1][38/59] Loss_D: 1.3011 Loss_G: 2.9507 D(x): 0.9041 D(G(z)): 0.6639 / 0.0622\n",
            "[8/1][39/59] Loss_D: 1.9941 Loss_G: 0.2718 D(x): 0.1747 D(G(z)): 0.0787 / 0.7708\n",
            "[8/1][40/59] Loss_D: 1.7778 Loss_G: 1.8433 D(x): 0.9157 D(G(z)): 0.7909 / 0.1866\n",
            "[8/1][41/59] Loss_D: 1.1693 Loss_G: 1.1857 D(x): 0.4523 D(G(z)): 0.2425 / 0.3280\n",
            "[8/1][42/59] Loss_D: 1.0646 Loss_G: 0.8385 D(x): 0.5976 D(G(z)): 0.3918 / 0.4583\n",
            "[8/1][43/59] Loss_D: 1.1136 Loss_G: 1.5487 D(x): 0.7150 D(G(z)): 0.5082 / 0.2353\n",
            "[8/1][44/59] Loss_D: 1.1065 Loss_G: 0.7877 D(x): 0.4922 D(G(z)): 0.2916 / 0.4740\n",
            "[8/1][45/59] Loss_D: 1.1197 Loss_G: 1.7472 D(x): 0.7357 D(G(z)): 0.5303 / 0.1977\n",
            "[8/1][46/59] Loss_D: 1.1586 Loss_G: 0.7061 D(x): 0.4479 D(G(z)): 0.2538 / 0.5175\n",
            "[8/1][47/59] Loss_D: 1.2004 Loss_G: 1.9028 D(x): 0.7788 D(G(z)): 0.5840 / 0.1710\n",
            "[8/1][48/59] Loss_D: 1.2035 Loss_G: 0.7396 D(x): 0.4136 D(G(z)): 0.2222 / 0.5011\n",
            "[8/1][49/59] Loss_D: 1.1671 Loss_G: 1.7104 D(x): 0.7516 D(G(z)): 0.5539 / 0.2028\n",
            "[8/1][50/59] Loss_D: 1.0807 Loss_G: 0.9075 D(x): 0.4840 D(G(z)): 0.2528 / 0.4243\n",
            "[8/1][51/59] Loss_D: 0.9964 Loss_G: 1.5265 D(x): 0.7328 D(G(z)): 0.4715 / 0.2421\n",
            "[8/1][52/59] Loss_D: 0.9572 Loss_G: 1.0353 D(x): 0.5677 D(G(z)): 0.2887 / 0.3826\n",
            "[8/1][53/59] Loss_D: 0.9363 Loss_G: 1.5849 D(x): 0.7322 D(G(z)): 0.4378 / 0.2292\n",
            "[8/1][54/59] Loss_D: 0.9956 Loss_G: 1.0000 D(x): 0.5571 D(G(z)): 0.2977 / 0.3944\n",
            "[8/1][55/59] Loss_D: 0.9495 Loss_G: 1.8091 D(x): 0.7393 D(G(z)): 0.4475 / 0.1860\n",
            "[8/1][56/59] Loss_D: 0.9310 Loss_G: 0.8051 D(x): 0.5358 D(G(z)): 0.2242 / 0.4700\n",
            "[8/1][57/59] Loss_D: 1.0139 Loss_G: 2.3560 D(x): 0.8161 D(G(z)): 0.5285 / 0.1154\n",
            "[8/1][58/59] Loss_D: 1.1282 Loss_G: 0.5497 D(x): 0.4098 D(G(z)): 0.1520 / 0.5998\n",
            "[9/1][0/59] Loss_D: 1.2650 Loss_G: 2.6100 D(x): 0.8862 D(G(z)): 0.6435 / 0.0884\n",
            "[9/1][1/59] Loss_D: 1.2567 Loss_G: 0.6303 D(x): 0.3516 D(G(z)): 0.1251 / 0.5573\n",
            "[9/1][2/59] Loss_D: 1.1380 Loss_G: 2.1997 D(x): 0.8577 D(G(z)): 0.5943 / 0.1313\n",
            "[9/1][3/59] Loss_D: 1.0902 Loss_G: 0.7575 D(x): 0.4337 D(G(z)): 0.1666 / 0.4930\n",
            "[9/1][4/59] Loss_D: 1.0341 Loss_G: 2.1143 D(x): 0.8321 D(G(z)): 0.5459 / 0.1455\n",
            "[9/1][5/59] Loss_D: 1.1103 Loss_G: 0.6500 D(x): 0.4416 D(G(z)): 0.1865 / 0.5464\n",
            "[9/1][6/59] Loss_D: 1.1361 Loss_G: 2.5068 D(x): 0.8673 D(G(z)): 0.5973 / 0.0965\n",
            "[9/1][7/59] Loss_D: 1.1543 Loss_G: 0.6737 D(x): 0.3952 D(G(z)): 0.1367 / 0.5351\n",
            "[9/1][8/59] Loss_D: 1.0206 Loss_G: 2.1980 D(x): 0.8679 D(G(z)): 0.5565 / 0.1286\n",
            "[9/1][9/59] Loss_D: 1.0410 Loss_G: 0.8397 D(x): 0.4619 D(G(z)): 0.1856 / 0.4636\n",
            "[9/1][10/59] Loss_D: 1.0168 Loss_G: 2.5333 D(x): 0.8211 D(G(z)): 0.5241 / 0.1021\n",
            "[9/1][11/59] Loss_D: 1.0981 Loss_G: 0.5386 D(x): 0.4162 D(G(z)): 0.1248 / 0.6066\n",
            "[9/1][12/59] Loss_D: 1.2360 Loss_G: 2.6970 D(x): 0.8952 D(G(z)): 0.6448 / 0.0809\n",
            "[9/1][13/59] Loss_D: 1.3425 Loss_G: 0.5884 D(x): 0.3256 D(G(z)): 0.1133 / 0.5759\n",
            "[9/1][14/59] Loss_D: 1.1951 Loss_G: 2.5540 D(x): 0.8704 D(G(z)): 0.6197 / 0.1018\n",
            "[9/1][15/59] Loss_D: 1.1135 Loss_G: 0.7216 D(x): 0.4124 D(G(z)): 0.1301 / 0.5102\n",
            "[9/1][16/59] Loss_D: 0.9734 Loss_G: 2.1187 D(x): 0.8670 D(G(z)): 0.5391 / 0.1381\n",
            "[9/1][17/59] Loss_D: 0.8646 Loss_G: 1.1736 D(x): 0.5571 D(G(z)): 0.2090 / 0.3349\n",
            "[9/1][18/59] Loss_D: 0.8141 Loss_G: 1.7325 D(x): 0.7693 D(G(z)): 0.3984 / 0.2026\n",
            "[9/1][19/59] Loss_D: 0.7912 Loss_G: 1.0577 D(x): 0.6108 D(G(z)): 0.2223 / 0.3695\n",
            "[9/1][20/59] Loss_D: 0.8598 Loss_G: 2.2531 D(x): 0.8129 D(G(z)): 0.4561 / 0.1230\n",
            "[9/1][21/59] Loss_D: 0.8864 Loss_G: 0.8419 D(x): 0.5080 D(G(z)): 0.1478 / 0.4577\n",
            "[9/1][22/59] Loss_D: 0.9298 Loss_G: 2.4240 D(x): 0.8578 D(G(z)): 0.5120 / 0.1034\n",
            "[9/1][23/59] Loss_D: 0.9759 Loss_G: 0.8615 D(x): 0.4670 D(G(z)): 0.1474 / 0.4477\n",
            "[9/1][24/59] Loss_D: 0.8875 Loss_G: 2.8622 D(x): 0.8387 D(G(z)): 0.4803 / 0.0741\n",
            "[9/1][25/59] Loss_D: 1.4641 Loss_G: 0.3836 D(x): 0.2994 D(G(z)): 0.1176 / 0.6947\n",
            "[9/1][26/59] Loss_D: 1.3087 Loss_G: 3.2200 D(x): 0.9249 D(G(z)): 0.6822 / 0.0530\n",
            "[9/1][27/59] Loss_D: 2.0998 Loss_G: 0.3633 D(x): 0.1612 D(G(z)): 0.0757 / 0.7092\n",
            "[9/1][28/59] Loss_D: 1.5895 Loss_G: 1.6379 D(x): 0.8844 D(G(z)): 0.7398 / 0.2190\n",
            "[9/1][29/59] Loss_D: 1.1386 Loss_G: 1.1242 D(x): 0.4778 D(G(z)): 0.2801 / 0.3441\n",
            "[9/1][30/59] Loss_D: 1.0622 Loss_G: 1.0329 D(x): 0.6107 D(G(z)): 0.4082 / 0.3786\n",
            "[9/1][31/59] Loss_D: 1.0669 Loss_G: 1.6272 D(x): 0.6614 D(G(z)): 0.4511 / 0.2240\n",
            "[9/1][32/59] Loss_D: 1.0837 Loss_G: 0.9050 D(x): 0.5224 D(G(z)): 0.2996 / 0.4280\n",
            "[9/1][33/59] Loss_D: 1.0606 Loss_G: 1.8109 D(x): 0.7293 D(G(z)): 0.4971 / 0.1888\n",
            "[9/1][34/59] Loss_D: 1.1269 Loss_G: 0.7114 D(x): 0.4642 D(G(z)): 0.2472 / 0.5134\n",
            "[9/1][35/59] Loss_D: 1.1344 Loss_G: 2.2497 D(x): 0.8077 D(G(z)): 0.5654 / 0.1287\n",
            "[9/1][36/59] Loss_D: 1.1435 Loss_G: 0.7301 D(x): 0.4308 D(G(z)): 0.1833 / 0.5108\n",
            "[9/1][37/59] Loss_D: 1.1383 Loss_G: 2.0572 D(x): 0.8381 D(G(z)): 0.5893 / 0.1469\n",
            "[9/1][38/59] Loss_D: 1.0149 Loss_G: 0.9405 D(x): 0.4756 D(G(z)): 0.1915 / 0.4182\n",
            "[9/1][39/59] Loss_D: 0.9820 Loss_G: 1.8403 D(x): 0.7791 D(G(z)): 0.4896 / 0.1868\n",
            "[9/1][40/59] Loss_D: 0.9089 Loss_G: 1.0141 D(x): 0.5516 D(G(z)): 0.2227 / 0.3885\n",
            "[9/1][41/59] Loss_D: 0.9378 Loss_G: 2.0693 D(x): 0.7882 D(G(z)): 0.4742 / 0.1454\n",
            "[9/1][42/59] Loss_D: 0.9587 Loss_G: 0.7126 D(x): 0.4874 D(G(z)): 0.1694 / 0.5120\n",
            "[9/1][43/59] Loss_D: 1.0017 Loss_G: 2.6735 D(x): 0.8587 D(G(z)): 0.5478 / 0.0822\n",
            "[9/1][44/59] Loss_D: 1.1681 Loss_G: 0.5523 D(x): 0.3798 D(G(z)): 0.1141 / 0.5946\n",
            "[9/1][45/59] Loss_D: 1.2460 Loss_G: 2.8826 D(x): 0.8853 D(G(z)): 0.6457 / 0.0674\n",
            "[9/1][46/59] Loss_D: 1.6512 Loss_G: 0.3472 D(x): 0.2346 D(G(z)): 0.0965 / 0.7184\n",
            "[9/1][47/59] Loss_D: 1.5084 Loss_G: 2.4389 D(x): 0.9165 D(G(z)): 0.7307 / 0.1089\n",
            "[9/1][48/59] Loss_D: 1.0977 Loss_G: 1.0813 D(x): 0.4501 D(G(z)): 0.1889 / 0.3678\n",
            "[9/1][49/59] Loss_D: 0.9537 Loss_G: 1.4349 D(x): 0.7283 D(G(z)): 0.4357 / 0.2700\n",
            "[9/1][50/59] Loss_D: 0.8542 Loss_G: 1.7430 D(x): 0.6795 D(G(z)): 0.3427 / 0.1958\n",
            "[9/1][51/59] Loss_D: 0.7881 Loss_G: 1.2103 D(x): 0.6381 D(G(z)): 0.2595 / 0.3249\n",
            "[9/1][52/59] Loss_D: 0.8304 Loss_G: 1.8135 D(x): 0.7589 D(G(z)): 0.4004 / 0.1905\n",
            "[9/1][53/59] Loss_D: 0.8331 Loss_G: 1.0543 D(x): 0.6007 D(G(z)): 0.2392 / 0.3735\n",
            "[9/1][54/59] Loss_D: 0.8057 Loss_G: 2.3889 D(x): 0.8108 D(G(z)): 0.4266 / 0.1064\n",
            "[9/1][55/59] Loss_D: 0.9791 Loss_G: 0.6406 D(x): 0.4730 D(G(z)): 0.1561 / 0.5485\n",
            "[9/1][56/59] Loss_D: 1.1118 Loss_G: 3.2086 D(x): 0.8846 D(G(z)): 0.6017 / 0.0507\n",
            "[9/1][57/59] Loss_D: 1.7374 Loss_G: 0.3428 D(x): 0.2253 D(G(z)): 0.0745 / 0.7249\n",
            "[9/1][58/59] Loss_D: 1.6346 Loss_G: 2.3118 D(x): 0.9176 D(G(z)): 0.7541 / 0.1175\n",
            "[10/1][0/59] Loss_D: 1.2098 Loss_G: 0.7967 D(x): 0.3990 D(G(z)): 0.1870 / 0.4754\n",
            "[10/1][1/59] Loss_D: 1.1316 Loss_G: 2.0769 D(x): 0.7462 D(G(z)): 0.5327 / 0.1472\n",
            "[10/1][2/59] Loss_D: 1.0239 Loss_G: 0.9700 D(x): 0.4824 D(G(z)): 0.1974 / 0.4088\n",
            "[10/1][3/59] Loss_D: 0.8958 Loss_G: 2.2578 D(x): 0.8074 D(G(z)): 0.4665 / 0.1217\n",
            "[10/1][4/59] Loss_D: 1.0564 Loss_G: 0.4935 D(x): 0.4548 D(G(z)): 0.1829 / 0.6306\n",
            "[10/1][5/59] Loss_D: 1.2470 Loss_G: 2.7985 D(x): 0.9115 D(G(z)): 0.6543 / 0.0784\n",
            "[10/1][6/59] Loss_D: 1.2776 Loss_G: 0.7472 D(x): 0.3514 D(G(z)): 0.1113 / 0.5041\n",
            "[10/1][7/59] Loss_D: 1.1035 Loss_G: 2.1450 D(x): 0.8469 D(G(z)): 0.5648 / 0.1409\n",
            "[10/1][8/59] Loss_D: 1.0082 Loss_G: 0.8943 D(x): 0.4810 D(G(z)): 0.1876 / 0.4414\n",
            "[10/1][9/59] Loss_D: 0.9262 Loss_G: 1.7191 D(x): 0.7953 D(G(z)): 0.4709 / 0.2026\n",
            "[10/1][10/59] Loss_D: 0.7961 Loss_G: 1.4397 D(x): 0.6257 D(G(z)): 0.2489 / 0.2617\n",
            "[10/1][11/59] Loss_D: 0.7414 Loss_G: 1.4070 D(x): 0.7024 D(G(z)): 0.2967 / 0.2737\n",
            "[10/1][12/59] Loss_D: 0.7328 Loss_G: 1.7754 D(x): 0.7443 D(G(z)): 0.3296 / 0.1920\n",
            "[10/1][13/59] Loss_D: 0.7154 Loss_G: 1.0593 D(x): 0.6454 D(G(z)): 0.2161 / 0.3713\n",
            "[10/1][14/59] Loss_D: 0.8514 Loss_G: 2.2233 D(x): 0.8114 D(G(z)): 0.4481 / 0.1277\n",
            "[10/1][15/59] Loss_D: 0.9220 Loss_G: 0.8364 D(x): 0.4924 D(G(z)): 0.1461 / 0.4595\n",
            "[10/1][16/59] Loss_D: 0.9421 Loss_G: 2.9354 D(x): 0.8785 D(G(z)): 0.5314 / 0.0648\n",
            "[10/1][17/59] Loss_D: 1.3605 Loss_G: 0.5912 D(x): 0.3135 D(G(z)): 0.0979 / 0.5774\n",
            "[10/1][18/59] Loss_D: 1.1037 Loss_G: 3.3660 D(x): 0.9091 D(G(z)): 0.6038 / 0.0476\n",
            "[10/1][19/59] Loss_D: 1.8471 Loss_G: 0.2366 D(x): 0.2110 D(G(z)): 0.0890 / 0.7990\n",
            "[10/1][20/59] Loss_D: 2.0011 Loss_G: 2.1525 D(x): 0.9336 D(G(z)): 0.8197 / 0.1364\n",
            "[10/1][21/59] Loss_D: 1.3403 Loss_G: 0.9020 D(x): 0.3508 D(G(z)): 0.1834 / 0.4314\n",
            "[10/1][22/59] Loss_D: 1.0764 Loss_G: 1.2319 D(x): 0.7169 D(G(z)): 0.4957 / 0.3192\n",
            "[10/1][23/59] Loss_D: 1.0176 Loss_G: 1.3650 D(x): 0.6240 D(G(z)): 0.3833 / 0.2778\n",
            "[10/1][24/59] Loss_D: 1.0759 Loss_G: 1.0589 D(x): 0.5593 D(G(z)): 0.3524 / 0.3786\n",
            "[10/1][25/59] Loss_D: 1.0484 Loss_G: 1.5776 D(x): 0.6844 D(G(z)): 0.4532 / 0.2272\n",
            "[10/1][26/59] Loss_D: 1.0051 Loss_G: 1.0928 D(x): 0.5442 D(G(z)): 0.2912 / 0.3597\n",
            "[10/1][27/59] Loss_D: 0.9113 Loss_G: 1.7932 D(x): 0.7385 D(G(z)): 0.4303 / 0.1932\n",
            "[10/1][28/59] Loss_D: 0.8912 Loss_G: 1.0370 D(x): 0.5731 D(G(z)): 0.2443 / 0.3867\n",
            "[10/1][29/59] Loss_D: 0.9389 Loss_G: 2.1398 D(x): 0.7908 D(G(z)): 0.4711 / 0.1434\n",
            "[10/1][30/59] Loss_D: 0.9800 Loss_G: 0.8478 D(x): 0.5065 D(G(z)): 0.2051 / 0.4547\n",
            "[10/1][31/59] Loss_D: 0.9135 Loss_G: 2.3605 D(x): 0.8362 D(G(z)): 0.4925 / 0.1128\n",
            "[10/1][32/59] Loss_D: 0.9679 Loss_G: 0.7590 D(x): 0.4854 D(G(z)): 0.1651 / 0.4920\n",
            "[10/1][33/59] Loss_D: 1.0013 Loss_G: 2.3919 D(x): 0.8611 D(G(z)): 0.5452 / 0.1090\n",
            "[10/1][34/59] Loss_D: 0.9535 Loss_G: 1.1049 D(x): 0.4780 D(G(z)): 0.1434 / 0.3603\n",
            "[10/1][35/59] Loss_D: 0.8235 Loss_G: 2.2402 D(x): 0.8055 D(G(z)): 0.4290 / 0.1247\n",
            "[10/1][36/59] Loss_D: 0.9987 Loss_G: 0.5323 D(x): 0.4800 D(G(z)): 0.1827 / 0.6041\n",
            "[10/1][37/59] Loss_D: 1.2102 Loss_G: 3.4896 D(x): 0.9107 D(G(z)): 0.6451 / 0.0381\n",
            "[10/1][38/59] Loss_D: 1.9582 Loss_G: 0.4182 D(x): 0.1781 D(G(z)): 0.0586 / 0.6741\n",
            "[10/1][39/59] Loss_D: 1.4252 Loss_G: 2.0435 D(x): 0.9133 D(G(z)): 0.7058 / 0.1539\n",
            "[10/1][40/59] Loss_D: 1.0016 Loss_G: 1.2379 D(x): 0.5099 D(G(z)): 0.2261 / 0.3163\n",
            "[10/1][41/59] Loss_D: 0.9130 Loss_G: 1.4890 D(x): 0.7014 D(G(z)): 0.3968 / 0.2527\n",
            "[10/1][42/59] Loss_D: 0.8524 Loss_G: 1.5003 D(x): 0.6532 D(G(z)): 0.3172 / 0.2476\n",
            "[10/1][43/59] Loss_D: 0.7842 Loss_G: 1.6556 D(x): 0.7055 D(G(z)): 0.3262 / 0.2146\n",
            "[10/1][44/59] Loss_D: 0.8013 Loss_G: 1.2151 D(x): 0.6701 D(G(z)): 0.3021 / 0.3276\n",
            "[10/1][45/59] Loss_D: 0.7789 Loss_G: 2.1723 D(x): 0.7639 D(G(z)): 0.3717 / 0.1339\n",
            "[10/1][46/59] Loss_D: 0.8758 Loss_G: 0.6344 D(x): 0.5358 D(G(z)): 0.1782 / 0.5527\n",
            "[10/1][47/59] Loss_D: 1.1449 Loss_G: 2.9485 D(x): 0.8977 D(G(z)): 0.6176 / 0.0628\n",
            "[10/1][48/59] Loss_D: 1.3631 Loss_G: 0.6411 D(x): 0.3083 D(G(z)): 0.0814 / 0.5491\n",
            "[10/1][49/59] Loss_D: 1.1500 Loss_G: 2.8420 D(x): 0.8704 D(G(z)): 0.6058 / 0.0746\n",
            "[10/1][50/59] Loss_D: 1.2376 Loss_G: 0.3708 D(x): 0.3642 D(G(z)): 0.1129 / 0.7048\n",
            "[10/1][51/59] Loss_D: 1.5118 Loss_G: 2.9592 D(x): 0.9212 D(G(z)): 0.7300 / 0.0702\n",
            "[10/1][52/59] Loss_D: 1.5983 Loss_G: 0.6037 D(x): 0.2593 D(G(z)): 0.0987 / 0.5752\n",
            "[10/1][53/59] Loss_D: 1.2458 Loss_G: 1.9243 D(x): 0.8301 D(G(z)): 0.6097 / 0.1652\n",
            "[10/1][54/59] Loss_D: 1.0420 Loss_G: 1.1055 D(x): 0.4983 D(G(z)): 0.2372 / 0.3608\n",
            "[10/1][55/59] Loss_D: 0.9596 Loss_G: 1.5176 D(x): 0.7181 D(G(z)): 0.4360 / 0.2453\n",
            "[10/1][56/59] Loss_D: 0.8524 Loss_G: 1.3833 D(x): 0.6277 D(G(z)): 0.2877 / 0.2779\n",
            "[10/1][57/59] Loss_D: 0.8529 Loss_G: 1.6821 D(x): 0.7110 D(G(z)): 0.3699 / 0.2159\n",
            "[10/1][58/59] Loss_D: 0.8138 Loss_G: 1.2720 D(x): 0.6459 D(G(z)): 0.2808 / 0.3066\n",
            "[11/1][0/59] Loss_D: 0.8075 Loss_G: 1.6900 D(x): 0.7333 D(G(z)): 0.3631 / 0.2085\n",
            "[11/1][1/59] Loss_D: 0.7745 Loss_G: 1.2973 D(x): 0.6487 D(G(z)): 0.2581 / 0.2977\n",
            "[11/1][2/59] Loss_D: 0.7467 Loss_G: 2.0719 D(x): 0.7694 D(G(z)): 0.3616 / 0.1444\n",
            "[11/1][3/59] Loss_D: 0.8180 Loss_G: 0.8016 D(x): 0.5748 D(G(z)): 0.1879 / 0.4755\n",
            "[11/1][4/59] Loss_D: 1.0086 Loss_G: 2.8301 D(x): 0.8663 D(G(z)): 0.5517 / 0.0713\n",
            "[11/1][5/59] Loss_D: 1.2807 Loss_G: 0.6216 D(x): 0.3352 D(G(z)): 0.0976 / 0.5598\n",
            "[11/1][6/59] Loss_D: 1.1583 Loss_G: 3.1968 D(x): 0.8806 D(G(z)): 0.6127 / 0.0526\n",
            "[11/1][7/59] Loss_D: 1.4955 Loss_G: 0.3811 D(x): 0.2865 D(G(z)): 0.1006 / 0.6959\n",
            "[11/1][8/59] Loss_D: 1.3478 Loss_G: 3.3761 D(x): 0.9424 D(G(z)): 0.6957 / 0.0440\n",
            "[11/1][9/59] Loss_D: 1.7590 Loss_G: 0.3764 D(x): 0.2203 D(G(z)): 0.0776 / 0.7042\n",
            "[11/1][10/59] Loss_D: 1.5820 Loss_G: 2.2655 D(x): 0.9409 D(G(z)): 0.7462 / 0.1262\n",
            "[11/1][11/59] Loss_D: 1.2310 Loss_G: 1.0056 D(x): 0.3905 D(G(z)): 0.1738 / 0.3941\n",
            "[11/1][12/59] Loss_D: 1.0995 Loss_G: 1.4125 D(x): 0.7362 D(G(z)): 0.5096 / 0.2807\n",
            "[11/1][13/59] Loss_D: 1.0460 Loss_G: 1.3425 D(x): 0.5854 D(G(z)): 0.3555 / 0.2834\n",
            "[11/1][14/59] Loss_D: 0.9659 Loss_G: 1.2906 D(x): 0.6171 D(G(z)): 0.3501 / 0.2973\n",
            "[11/1][15/59] Loss_D: 0.8950 Loss_G: 1.6028 D(x): 0.6737 D(G(z)): 0.3684 / 0.2300\n",
            "[11/1][16/59] Loss_D: 0.8630 Loss_G: 1.2065 D(x): 0.6260 D(G(z)): 0.2910 / 0.3269\n",
            "[11/1][17/59] Loss_D: 0.8633 Loss_G: 1.7061 D(x): 0.7293 D(G(z)): 0.3936 / 0.2038\n",
            "[11/1][18/59] Loss_D: 0.8681 Loss_G: 1.0526 D(x): 0.5953 D(G(z)): 0.2610 / 0.3794\n",
            "[11/1][19/59] Loss_D: 0.8657 Loss_G: 2.0273 D(x): 0.7817 D(G(z)): 0.4329 / 0.1524\n",
            "[11/1][20/59] Loss_D: 0.8847 Loss_G: 0.9367 D(x): 0.5559 D(G(z)): 0.2148 / 0.4207\n",
            "[11/1][21/59] Loss_D: 0.9056 Loss_G: 2.3605 D(x): 0.8314 D(G(z)): 0.4851 / 0.1180\n",
            "[11/1][22/59] Loss_D: 0.9757 Loss_G: 0.7397 D(x): 0.4807 D(G(z)): 0.1499 / 0.5042\n",
            "[11/1][23/59] Loss_D: 1.0046 Loss_G: 2.7476 D(x): 0.8663 D(G(z)): 0.5483 / 0.0752\n",
            "[11/1][24/59] Loss_D: 1.0607 Loss_G: 0.7166 D(x): 0.4168 D(G(z)): 0.1027 / 0.5103\n",
            "[11/1][25/59] Loss_D: 1.0853 Loss_G: 2.7825 D(x): 0.8880 D(G(z)): 0.5929 / 0.0734\n",
            "[11/1][26/59] Loss_D: 1.3398 Loss_G: 0.5090 D(x): 0.3304 D(G(z)): 0.1196 / 0.6232\n",
            "[11/1][27/59] Loss_D: 1.2558 Loss_G: 2.8036 D(x): 0.8902 D(G(z)): 0.6428 / 0.0787\n",
            "[11/1][28/59] Loss_D: 1.1685 Loss_G: 0.8748 D(x): 0.4159 D(G(z)): 0.1619 / 0.4506\n",
            "[11/1][29/59] Loss_D: 0.8866 Loss_G: 2.7123 D(x): 0.8455 D(G(z)): 0.4816 / 0.0883\n",
            "[11/1][30/59] Loss_D: 0.8813 Loss_G: 1.0372 D(x): 0.5534 D(G(z)): 0.1880 / 0.3833\n",
            "[11/1][31/59] Loss_D: 0.8165 Loss_G: 2.4657 D(x): 0.8261 D(G(z)): 0.4371 / 0.1109\n",
            "[11/1][32/59] Loss_D: 0.9000 Loss_G: 1.0162 D(x): 0.5405 D(G(z)): 0.1870 / 0.3889\n",
            "[11/1][33/59] Loss_D: 0.8632 Loss_G: 2.5349 D(x): 0.8452 D(G(z)): 0.4747 / 0.0964\n",
            "[11/1][34/59] Loss_D: 1.0027 Loss_G: 0.7562 D(x): 0.4630 D(G(z)): 0.1369 / 0.4944\n",
            "[11/1][35/59] Loss_D: 1.0278 Loss_G: 2.9470 D(x): 0.8795 D(G(z)): 0.5621 / 0.0665\n",
            "[11/1][36/59] Loss_D: 1.3080 Loss_G: 0.6056 D(x): 0.3341 D(G(z)): 0.1100 / 0.5747\n",
            "[11/1][37/59] Loss_D: 1.2514 Loss_G: 2.9908 D(x): 0.8883 D(G(z)): 0.6406 / 0.0653\n",
            "[11/1][38/59] Loss_D: 1.0972 Loss_G: 0.9089 D(x): 0.4202 D(G(z)): 0.1279 / 0.4499\n",
            "[11/1][39/59] Loss_D: 0.9624 Loss_G: 2.4087 D(x): 0.8580 D(G(z)): 0.5141 / 0.1079\n",
            "[11/1][40/59] Loss_D: 0.9447 Loss_G: 0.7800 D(x): 0.4974 D(G(z)): 0.1657 / 0.4894\n",
            "[11/1][41/59] Loss_D: 1.0398 Loss_G: 2.6867 D(x): 0.8871 D(G(z)): 0.5697 / 0.0840\n",
            "[11/1][42/59] Loss_D: 1.0401 Loss_G: 0.9535 D(x): 0.4396 D(G(z)): 0.1334 / 0.4150\n",
            "[11/1][43/59] Loss_D: 0.9147 Loss_G: 2.2748 D(x): 0.8364 D(G(z)): 0.4934 / 0.1223\n",
            "[11/1][44/59] Loss_D: 0.9072 Loss_G: 0.9507 D(x): 0.5220 D(G(z)): 0.1755 / 0.4174\n",
            "[11/1][45/59] Loss_D: 0.8283 Loss_G: 2.7142 D(x): 0.8457 D(G(z)): 0.4567 / 0.0802\n",
            "[11/1][46/59] Loss_D: 0.8928 Loss_G: 0.8589 D(x): 0.5062 D(G(z)): 0.1414 / 0.4530\n",
            "[11/1][47/59] Loss_D: 0.9547 Loss_G: 3.1289 D(x): 0.8756 D(G(z)): 0.5312 / 0.0534\n",
            "[11/1][48/59] Loss_D: 1.2232 Loss_G: 0.6546 D(x): 0.3617 D(G(z)): 0.0977 / 0.5431\n",
            "[11/1][49/59] Loss_D: 1.1215 Loss_G: 2.9331 D(x): 0.9062 D(G(z)): 0.6060 / 0.0682\n",
            "[11/1][50/59] Loss_D: 1.3285 Loss_G: 0.5347 D(x): 0.3355 D(G(z)): 0.1181 / 0.6107\n",
            "[11/1][51/59] Loss_D: 1.3311 Loss_G: 2.6105 D(x): 0.9064 D(G(z)): 0.6663 / 0.0920\n",
            "[11/1][52/59] Loss_D: 1.1095 Loss_G: 0.7259 D(x): 0.4330 D(G(z)): 0.1555 / 0.5123\n",
            "[11/1][53/59] Loss_D: 1.1676 Loss_G: 2.9468 D(x): 0.8472 D(G(z)): 0.5962 / 0.0719\n",
            "[11/1][54/59] Loss_D: 1.3138 Loss_G: 0.7027 D(x): 0.3473 D(G(z)): 0.1131 / 0.5300\n",
            "[11/1][55/59] Loss_D: 1.1681 Loss_G: 2.5322 D(x): 0.8424 D(G(z)): 0.5859 / 0.0938\n",
            "[11/1][56/59] Loss_D: 0.9794 Loss_G: 1.0274 D(x): 0.4782 D(G(z)): 0.1661 / 0.3888\n",
            "[11/1][57/59] Loss_D: 0.7998 Loss_G: 2.0119 D(x): 0.8193 D(G(z)): 0.4235 / 0.1567\n",
            "[11/1][58/59] Loss_D: 0.7411 Loss_G: 1.5435 D(x): 0.6708 D(G(z)): 0.2589 / 0.2411\n",
            "[12/1][0/59] Loss_D: 0.7407 Loss_G: 1.6267 D(x): 0.7116 D(G(z)): 0.2988 / 0.2257\n",
            "[12/1][1/59] Loss_D: 0.6823 Loss_G: 2.1115 D(x): 0.7610 D(G(z)): 0.3100 / 0.1389\n",
            "[12/1][2/59] Loss_D: 0.7257 Loss_G: 1.1453 D(x): 0.6499 D(G(z)): 0.2233 / 0.3480\n",
            "[12/1][3/59] Loss_D: 0.7565 Loss_G: 2.7313 D(x): 0.8328 D(G(z)): 0.4129 / 0.0762\n",
            "[12/1][4/59] Loss_D: 0.8580 Loss_G: 0.8993 D(x): 0.5114 D(G(z)): 0.1200 / 0.4351\n",
            "[12/1][5/59] Loss_D: 0.9696 Loss_G: 3.5360 D(x): 0.8927 D(G(z)): 0.5460 / 0.0396\n",
            "[12/1][6/59] Loss_D: 1.3714 Loss_G: 0.7213 D(x): 0.3130 D(G(z)): 0.0725 / 0.5140\n",
            "[12/1][7/59] Loss_D: 1.0231 Loss_G: 3.3052 D(x): 0.8963 D(G(z)): 0.5688 / 0.0449\n",
            "[12/1][8/59] Loss_D: 1.3850 Loss_G: 0.3353 D(x): 0.3193 D(G(z)): 0.1305 / 0.7365\n",
            "[12/1][9/59] Loss_D: 2.0076 Loss_G: 3.6136 D(x): 0.9319 D(G(z)): 0.8179 / 0.0408\n",
            "[12/1][10/59] Loss_D: 1.7614 Loss_G: 0.4096 D(x): 0.2336 D(G(z)): 0.0874 / 0.6862\n",
            "[12/1][11/59] Loss_D: 1.8133 Loss_G: 2.7447 D(x): 0.9128 D(G(z)): 0.7877 / 0.0935\n",
            "[12/1][12/59] Loss_D: 1.4858 Loss_G: 1.1539 D(x): 0.3320 D(G(z)): 0.1687 / 0.3476\n",
            "[12/1][13/59] Loss_D: 1.0736 Loss_G: 1.5720 D(x): 0.6790 D(G(z)): 0.4563 / 0.2445\n",
            "[12/1][14/59] Loss_D: 0.9467 Loss_G: 1.6948 D(x): 0.6238 D(G(z)): 0.3314 / 0.2107\n",
            "[12/1][15/59] Loss_D: 0.8585 Loss_G: 1.4032 D(x): 0.6500 D(G(z)): 0.3183 / 0.2721\n",
            "[12/1][16/59] Loss_D: 0.8878 Loss_G: 1.6477 D(x): 0.7074 D(G(z)): 0.3893 / 0.2147\n",
            "[12/1][17/59] Loss_D: 0.9241 Loss_G: 1.2785 D(x): 0.6186 D(G(z)): 0.3247 / 0.3078\n",
            "[12/1][18/59] Loss_D: 0.8064 Loss_G: 2.0966 D(x): 0.7601 D(G(z)): 0.3855 / 0.1407\n",
            "[12/1][19/59] Loss_D: 0.7932 Loss_G: 1.2704 D(x): 0.6215 D(G(z)): 0.2419 / 0.3132\n",
            "[12/1][20/59] Loss_D: 0.8788 Loss_G: 2.1832 D(x): 0.7634 D(G(z)): 0.4223 / 0.1365\n",
            "[12/1][21/59] Loss_D: 0.8915 Loss_G: 1.1107 D(x): 0.5362 D(G(z)): 0.1849 / 0.3575\n",
            "[12/1][22/59] Loss_D: 0.7939 Loss_G: 3.1646 D(x): 0.8453 D(G(z)): 0.4405 / 0.0546\n",
            "[12/1][23/59] Loss_D: 1.0217 Loss_G: 0.6740 D(x): 0.4523 D(G(z)): 0.1264 / 0.5340\n",
            "[12/1][24/59] Loss_D: 1.1632 Loss_G: 2.4760 D(x): 0.8752 D(G(z)): 0.6111 / 0.1054\n",
            "[12/1][25/59] Loss_D: 1.2509 Loss_G: 0.9219 D(x): 0.3720 D(G(z)): 0.1504 / 0.4297\n",
            "[12/1][26/59] Loss_D: 0.9767 Loss_G: 2.0061 D(x): 0.8341 D(G(z)): 0.5167 / 0.1579\n",
            "[12/1][27/59] Loss_D: 0.9597 Loss_G: 1.5094 D(x): 0.5871 D(G(z)): 0.3048 / 0.2604\n",
            "[12/1][28/59] Loss_D: 0.9489 Loss_G: 1.5615 D(x): 0.6288 D(G(z)): 0.3401 / 0.2321\n",
            "[12/1][29/59] Loss_D: 0.6888 Loss_G: 2.3454 D(x): 0.7460 D(G(z)): 0.3032 / 0.1123\n",
            "[12/1][30/59] Loss_D: 0.6836 Loss_G: 1.5955 D(x): 0.6793 D(G(z)): 0.2289 / 0.2324\n",
            "[12/1][31/59] Loss_D: 0.8334 Loss_G: 2.6297 D(x): 0.7653 D(G(z)): 0.3996 / 0.0890\n",
            "[12/1][32/59] Loss_D: 1.0299 Loss_G: 0.7483 D(x): 0.4594 D(G(z)): 0.1579 / 0.5040\n",
            "[12/1][33/59] Loss_D: 0.9258 Loss_G: 2.8771 D(x): 0.9069 D(G(z)): 0.5288 / 0.0703\n",
            "[12/1][34/59] Loss_D: 1.0745 Loss_G: 0.7589 D(x): 0.4406 D(G(z)): 0.1625 / 0.4926\n",
            "[12/1][35/59] Loss_D: 1.3199 Loss_G: 2.8619 D(x): 0.8282 D(G(z)): 0.6453 / 0.0757\n",
            "[12/1][36/59] Loss_D: 1.4072 Loss_G: 0.6163 D(x): 0.3193 D(G(z)): 0.1243 / 0.5607\n",
            "[12/1][37/59] Loss_D: 1.2975 Loss_G: 3.7269 D(x): 0.8902 D(G(z)): 0.6632 / 0.0325\n",
            "[12/1][38/59] Loss_D: 1.4182 Loss_G: 0.7756 D(x): 0.3051 D(G(z)): 0.0755 / 0.4926\n",
            "[12/1][39/59] Loss_D: 1.3290 Loss_G: 2.5360 D(x): 0.8766 D(G(z)): 0.6623 / 0.1034\n",
            "[12/1][40/59] Loss_D: 1.2811 Loss_G: 0.8797 D(x): 0.4081 D(G(z)): 0.2280 / 0.4409\n",
            "[12/1][41/59] Loss_D: 1.1007 Loss_G: 2.2559 D(x): 0.7703 D(G(z)): 0.5324 / 0.1241\n",
            "[12/1][42/59] Loss_D: 0.8978 Loss_G: 1.4039 D(x): 0.5342 D(G(z)): 0.1926 / 0.2753\n",
            "[12/1][43/59] Loss_D: 0.8124 Loss_G: 2.0599 D(x): 0.8011 D(G(z)): 0.4160 / 0.1506\n",
            "[12/1][44/59] Loss_D: 0.8442 Loss_G: 1.4427 D(x): 0.6370 D(G(z)): 0.2904 / 0.2633\n",
            "[12/1][45/59] Loss_D: 0.8820 Loss_G: 1.8137 D(x): 0.7039 D(G(z)): 0.3799 / 0.1867\n",
            "[12/1][46/59] Loss_D: 0.8402 Loss_G: 1.3899 D(x): 0.6188 D(G(z)): 0.2692 / 0.2738\n",
            "[12/1][47/59] Loss_D: 0.7299 Loss_G: 2.8456 D(x): 0.7851 D(G(z)): 0.3622 / 0.0700\n",
            "[12/1][48/59] Loss_D: 0.8166 Loss_G: 0.7280 D(x): 0.5467 D(G(z)): 0.1428 / 0.5115\n",
            "[12/1][49/59] Loss_D: 1.1886 Loss_G: 3.2690 D(x): 0.9098 D(G(z)): 0.6338 / 0.0453\n",
            "[12/1][50/59] Loss_D: 1.4154 Loss_G: 0.7183 D(x): 0.2944 D(G(z)): 0.0819 / 0.5136\n",
            "[12/1][51/59] Loss_D: 1.1557 Loss_G: 2.5469 D(x): 0.8635 D(G(z)): 0.5975 / 0.0997\n",
            "[12/1][52/59] Loss_D: 1.0185 Loss_G: 0.9355 D(x): 0.4577 D(G(z)): 0.1526 / 0.4210\n",
            "[12/1][53/59] Loss_D: 0.9205 Loss_G: 2.9448 D(x): 0.8676 D(G(z)): 0.5134 / 0.0651\n",
            "[12/1][54/59] Loss_D: 0.9073 Loss_G: 0.8136 D(x): 0.5041 D(G(z)): 0.1395 / 0.4725\n",
            "[12/1][55/59] Loss_D: 1.1114 Loss_G: 3.3430 D(x): 0.8869 D(G(z)): 0.5984 / 0.0459\n",
            "[12/1][56/59] Loss_D: 1.1806 Loss_G: 0.9035 D(x): 0.3771 D(G(z)): 0.0907 / 0.4383\n",
            "[12/1][57/59] Loss_D: 1.0354 Loss_G: 4.1849 D(x): 0.8906 D(G(z)): 0.5625 / 0.0213\n",
            "[12/1][58/59] Loss_D: 1.6634 Loss_G: 0.3820 D(x): 0.2452 D(G(z)): 0.0597 / 0.6996\n",
            "[13/1][0/59] Loss_D: 1.5903 Loss_G: 2.8869 D(x): 0.9447 D(G(z)): 0.7476 / 0.0791\n",
            "[13/1][1/59] Loss_D: 1.4626 Loss_G: 0.8094 D(x): 0.3242 D(G(z)): 0.1322 / 0.4764\n",
            "[13/1][2/59] Loss_D: 1.2413 Loss_G: 1.6412 D(x): 0.7720 D(G(z)): 0.5845 / 0.2217\n",
            "[13/1][3/59] Loss_D: 1.1118 Loss_G: 1.2394 D(x): 0.5106 D(G(z)): 0.3072 / 0.3160\n",
            "[13/1][4/59] Loss_D: 1.0513 Loss_G: 1.4650 D(x): 0.6259 D(G(z)): 0.4062 / 0.2581\n",
            "[13/1][5/59] Loss_D: 0.9021 Loss_G: 1.6300 D(x): 0.6513 D(G(z)): 0.3428 / 0.2281\n",
            "[13/1][6/59] Loss_D: 1.0002 Loss_G: 1.3938 D(x): 0.6440 D(G(z)): 0.3915 / 0.2714\n",
            "[13/1][7/59] Loss_D: 1.0516 Loss_G: 1.4629 D(x): 0.6259 D(G(z)): 0.4072 / 0.2578\n",
            "[13/1][8/59] Loss_D: 0.9564 Loss_G: 1.5083 D(x): 0.6206 D(G(z)): 0.3484 / 0.2442\n",
            "[13/1][9/59] Loss_D: 0.8538 Loss_G: 1.9293 D(x): 0.6912 D(G(z)): 0.3554 / 0.1662\n",
            "[13/1][10/59] Loss_D: 0.8009 Loss_G: 1.7017 D(x): 0.6793 D(G(z)): 0.3088 / 0.2072\n",
            "[13/1][11/59] Loss_D: 0.9622 Loss_G: 2.2544 D(x): 0.7081 D(G(z)): 0.4286 / 0.1236\n",
            "[13/1][12/59] Loss_D: 1.0699 Loss_G: 0.9754 D(x): 0.4923 D(G(z)): 0.2394 / 0.4065\n",
            "[13/1][13/59] Loss_D: 0.9976 Loss_G: 3.8733 D(x): 0.8513 D(G(z)): 0.5356 / 0.0261\n",
            "[13/1][14/59] Loss_D: 1.3494 Loss_G: 0.3711 D(x): 0.3109 D(G(z)): 0.0689 / 0.7071\n",
            "[13/1][15/59] Loss_D: 1.8070 Loss_G: 3.6772 D(x): 0.9588 D(G(z)): 0.8013 / 0.0396\n",
            "[13/1][16/59] Loss_D: 1.4986 Loss_G: 0.6292 D(x): 0.3020 D(G(z)): 0.1088 / 0.5621\n",
            "[13/1][17/59] Loss_D: 1.3523 Loss_G: 2.6714 D(x): 0.8451 D(G(z)): 0.6588 / 0.0845\n",
            "[13/1][18/59] Loss_D: 1.2299 Loss_G: 0.9572 D(x): 0.3772 D(G(z)): 0.1348 / 0.4108\n",
            "[13/1][19/59] Loss_D: 1.0489 Loss_G: 2.1916 D(x): 0.8128 D(G(z)): 0.5404 / 0.1303\n",
            "[13/1][20/59] Loss_D: 0.9161 Loss_G: 1.5638 D(x): 0.5601 D(G(z)): 0.2424 / 0.2367\n",
            "[13/1][21/59] Loss_D: 0.9112 Loss_G: 1.8242 D(x): 0.6763 D(G(z)): 0.3707 / 0.1846\n",
            "[13/1][22/59] Loss_D: 0.8053 Loss_G: 1.8103 D(x): 0.6635 D(G(z)): 0.2942 / 0.1863\n",
            "[13/1][23/59] Loss_D: 0.7751 Loss_G: 1.9923 D(x): 0.7144 D(G(z)): 0.3257 / 0.1601\n",
            "[13/1][24/59] Loss_D: 0.7907 Loss_G: 1.4715 D(x): 0.6482 D(G(z)): 0.2663 / 0.2612\n",
            "[13/1][25/59] Loss_D: 0.7261 Loss_G: 2.9370 D(x): 0.8083 D(G(z)): 0.3756 / 0.0658\n",
            "[13/1][26/59] Loss_D: 0.9042 Loss_G: 0.6191 D(x): 0.5203 D(G(z)): 0.1678 / 0.5688\n",
            "[13/1][27/59] Loss_D: 1.2431 Loss_G: 3.8780 D(x): 0.9264 D(G(z)): 0.6529 / 0.0267\n",
            "[13/1][28/59] Loss_D: 1.4219 Loss_G: 0.9192 D(x): 0.2893 D(G(z)): 0.0507 / 0.4347\n",
            "[13/1][29/59] Loss_D: 1.1126 Loss_G: 2.4401 D(x): 0.8782 D(G(z)): 0.5868 / 0.1108\n",
            "[13/1][30/59] Loss_D: 1.0368 Loss_G: 0.9627 D(x): 0.4744 D(G(z)): 0.1945 / 0.4077\n",
            "[13/1][31/59] Loss_D: 0.9251 Loss_G: 2.5533 D(x): 0.8251 D(G(z)): 0.4925 / 0.0949\n",
            "[13/1][32/59] Loss_D: 0.8861 Loss_G: 1.1610 D(x): 0.5370 D(G(z)): 0.1760 / 0.3550\n",
            "[13/1][33/59] Loss_D: 0.9411 Loss_G: 2.8439 D(x): 0.8489 D(G(z)): 0.5043 / 0.0711\n",
            "[13/1][34/59] Loss_D: 0.9798 Loss_G: 0.8678 D(x): 0.4512 D(G(z)): 0.1140 / 0.4532\n",
            "[13/1][35/59] Loss_D: 1.0260 Loss_G: 3.3730 D(x): 0.9073 D(G(z)): 0.5667 / 0.0453\n",
            "[13/1][36/59] Loss_D: 1.0657 Loss_G: 0.7726 D(x): 0.4306 D(G(z)): 0.1277 / 0.4914\n",
            "[13/1][37/59] Loss_D: 1.1520 Loss_G: 3.5946 D(x): 0.8804 D(G(z)): 0.6065 / 0.0353\n",
            "[13/1][38/59] Loss_D: 1.3235 Loss_G: 1.1175 D(x): 0.3262 D(G(z)): 0.0629 / 0.3669\n",
            "[13/1][39/59] Loss_D: 0.8524 Loss_G: 3.2323 D(x): 0.8875 D(G(z)): 0.4837 / 0.0500\n",
            "[13/1][40/59] Loss_D: 1.1735 Loss_G: 0.3632 D(x): 0.3981 D(G(z)): 0.1314 / 0.7170\n",
            "[13/1][41/59] Loss_D: 1.6321 Loss_G: 3.1756 D(x): 0.9539 D(G(z)): 0.7482 / 0.0544\n",
            "[13/1][42/59] Loss_D: 1.3170 Loss_G: 0.7734 D(x): 0.3372 D(G(z)): 0.1067 / 0.4897\n",
            "[13/1][43/59] Loss_D: 1.0696 Loss_G: 2.1245 D(x): 0.8440 D(G(z)): 0.5564 / 0.1398\n",
            "[13/1][44/59] Loss_D: 0.8500 Loss_G: 1.6166 D(x): 0.5864 D(G(z)): 0.2327 / 0.2248\n",
            "[13/1][45/59] Loss_D: 0.8973 Loss_G: 1.3966 D(x): 0.6709 D(G(z)): 0.3560 / 0.2725\n",
            "[13/1][46/59] Loss_D: 0.9108 Loss_G: 2.0311 D(x): 0.6888 D(G(z)): 0.3871 / 0.1520\n",
            "[13/1][47/59] Loss_D: 0.8371 Loss_G: 1.2264 D(x): 0.5677 D(G(z)): 0.2016 / 0.3200\n",
            "[13/1][48/59] Loss_D: 0.8314 Loss_G: 2.8344 D(x): 0.8490 D(G(z)): 0.4616 / 0.0711\n",
            "[13/1][49/59] Loss_D: 0.8953 Loss_G: 1.0922 D(x): 0.5148 D(G(z)): 0.1577 / 0.3665\n",
            "[13/1][50/59] Loss_D: 0.9233 Loss_G: 3.0692 D(x): 0.8449 D(G(z)): 0.5014 / 0.0588\n",
            "[13/1][51/59] Loss_D: 0.9614 Loss_G: 0.8202 D(x): 0.4720 D(G(z)): 0.1294 / 0.4757\n",
            "[13/1][52/59] Loss_D: 1.1118 Loss_G: 3.5599 D(x): 0.9138 D(G(z)): 0.6046 / 0.0355\n",
            "[13/1][53/59] Loss_D: 1.1740 Loss_G: 0.8645 D(x): 0.3668 D(G(z)): 0.0731 / 0.4527\n",
            "[13/1][54/59] Loss_D: 1.0810 Loss_G: 2.7305 D(x): 0.8895 D(G(z)): 0.5817 / 0.0795\n",
            "[13/1][55/59] Loss_D: 0.9372 Loss_G: 1.1292 D(x): 0.4872 D(G(z)): 0.1413 / 0.3550\n",
            "[13/1][56/59] Loss_D: 0.8240 Loss_G: 2.9399 D(x): 0.8728 D(G(z)): 0.4701 / 0.0638\n",
            "[13/1][57/59] Loss_D: 0.8892 Loss_G: 1.0272 D(x): 0.5138 D(G(z)): 0.1483 / 0.3927\n",
            "[13/1][58/59] Loss_D: 0.9503 Loss_G: 3.0406 D(x): 0.8525 D(G(z)): 0.5147 / 0.0569\n",
            "[14/1][0/59] Loss_D: 0.9521 Loss_G: 1.1025 D(x): 0.4723 D(G(z)): 0.1269 / 0.3646\n",
            "[14/1][1/59] Loss_D: 0.7930 Loss_G: 3.7623 D(x): 0.8884 D(G(z)): 0.4616 / 0.0305\n",
            "[14/1][2/59] Loss_D: 1.1951 Loss_G: 0.5261 D(x): 0.3675 D(G(z)): 0.0859 / 0.6141\n",
            "[14/1][3/59] Loss_D: 1.4006 Loss_G: 3.7305 D(x): 0.9375 D(G(z)): 0.7013 / 0.0304\n",
            "[14/1][4/59] Loss_D: 2.1323 Loss_G: 0.5418 D(x): 0.1476 D(G(z)): 0.0582 / 0.6009\n",
            "[14/1][5/59] Loss_D: 1.4173 Loss_G: 3.1975 D(x): 0.8983 D(G(z)): 0.7027 / 0.0564\n",
            "[14/1][6/59] Loss_D: 1.4399 Loss_G: 0.6753 D(x): 0.3214 D(G(z)): 0.1340 / 0.5387\n",
            "[14/1][7/59] Loss_D: 1.3787 Loss_G: 2.3785 D(x): 0.8654 D(G(z)): 0.6707 / 0.1076\n",
            "[14/1][8/59] Loss_D: 1.2007 Loss_G: 0.9573 D(x): 0.4099 D(G(z)): 0.2005 / 0.4181\n",
            "[14/1][9/59] Loss_D: 1.2064 Loss_G: 2.1508 D(x): 0.7749 D(G(z)): 0.5765 / 0.1328\n",
            "[14/1][10/59] Loss_D: 1.0263 Loss_G: 1.2633 D(x): 0.4961 D(G(z)): 0.2320 / 0.3140\n",
            "[14/1][11/59] Loss_D: 0.8880 Loss_G: 2.1243 D(x): 0.7761 D(G(z)): 0.4378 / 0.1412\n",
            "[14/1][12/59] Loss_D: 0.8313 Loss_G: 1.7160 D(x): 0.6528 D(G(z)): 0.3000 / 0.2034\n",
            "[14/1][13/59] Loss_D: 0.9601 Loss_G: 1.8453 D(x): 0.6690 D(G(z)): 0.3969 / 0.1782\n",
            "[14/1][14/59] Loss_D: 0.8885 Loss_G: 1.4154 D(x): 0.5865 D(G(z)): 0.2631 / 0.2685\n",
            "[14/1][15/59] Loss_D: 0.7650 Loss_G: 3.0570 D(x): 0.8091 D(G(z)): 0.4003 / 0.0559\n",
            "[14/1][16/59] Loss_D: 0.8562 Loss_G: 1.0112 D(x): 0.5361 D(G(z)): 0.1580 / 0.4009\n",
            "[14/1][17/59] Loss_D: 1.0433 Loss_G: 4.0779 D(x): 0.8786 D(G(z)): 0.5607 / 0.0231\n",
            "[14/1][18/59] Loss_D: 1.4293 Loss_G: 0.5249 D(x): 0.3108 D(G(z)): 0.0820 / 0.6162\n",
            "[14/1][19/59] Loss_D: 1.4097 Loss_G: 2.6294 D(x): 0.9162 D(G(z)): 0.6947 / 0.1024\n",
            "[14/1][20/59] Loss_D: 1.1589 Loss_G: 1.3833 D(x): 0.4409 D(G(z)): 0.1774 / 0.2977\n",
            "[14/1][21/59] Loss_D: 0.9178 Loss_G: 1.8378 D(x): 0.7502 D(G(z)): 0.4205 / 0.1806\n",
            "[14/1][22/59] Loss_D: 0.7924 Loss_G: 1.9517 D(x): 0.6811 D(G(z)): 0.3072 / 0.1742\n",
            "[14/1][23/59] Loss_D: 0.8779 Loss_G: 1.6558 D(x): 0.6492 D(G(z)): 0.3156 / 0.2188\n",
            "[14/1][24/59] Loss_D: 0.8583 Loss_G: 2.6374 D(x): 0.7260 D(G(z)): 0.3832 / 0.0861\n",
            "[14/1][25/59] Loss_D: 0.8116 Loss_G: 1.1098 D(x): 0.5701 D(G(z)): 0.1741 / 0.3622\n",
            "[14/1][26/59] Loss_D: 0.8570 Loss_G: 3.3423 D(x): 0.8827 D(G(z)): 0.4872 / 0.0446\n",
            "[14/1][27/59] Loss_D: 0.9357 Loss_G: 0.9486 D(x): 0.4676 D(G(z)): 0.0977 / 0.4206\n",
            "[14/1][28/59] Loss_D: 1.0529 Loss_G: 3.5157 D(x): 0.9149 D(G(z)): 0.5857 / 0.0368\n",
            "[14/1][29/59] Loss_D: 1.2454 Loss_G: 0.6506 D(x): 0.3538 D(G(z)): 0.1008 / 0.5505\n",
            "[14/1][30/59] Loss_D: 1.1705 Loss_G: 3.6429 D(x): 0.9097 D(G(z)): 0.6240 / 0.0352\n",
            "[14/1][31/59] Loss_D: 1.0273 Loss_G: 1.1991 D(x): 0.4312 D(G(z)): 0.0750 / 0.3330\n",
            "[14/1][32/59] Loss_D: 0.9404 Loss_G: 2.5130 D(x): 0.8834 D(G(z)): 0.5256 / 0.1010\n",
            "[14/1][33/59] Loss_D: 0.9217 Loss_G: 1.0442 D(x): 0.5111 D(G(z)): 0.1689 / 0.3860\n",
            "[14/1][34/59] Loss_D: 0.9081 Loss_G: 3.8669 D(x): 0.8851 D(G(z)): 0.5147 / 0.0261\n",
            "[14/1][35/59] Loss_D: 1.0918 Loss_G: 0.9012 D(x): 0.3934 D(G(z)): 0.0571 / 0.4358\n",
            "[14/1][36/59] Loss_D: 1.1391 Loss_G: 2.9337 D(x): 0.8828 D(G(z)): 0.5999 / 0.0656\n",
            "[14/1][37/59] Loss_D: 1.0318 Loss_G: 1.1078 D(x): 0.4401 D(G(z)): 0.1251 / 0.3605\n",
            "[14/1][38/59] Loss_D: 0.8822 Loss_G: 1.9076 D(x): 0.8320 D(G(z)): 0.4744 / 0.1704\n",
            "[14/1][39/59] Loss_D: 0.8993 Loss_G: 1.8890 D(x): 0.6518 D(G(z)): 0.3418 / 0.1726\n",
            "[14/1][40/59] Loss_D: 0.8727 Loss_G: 1.7796 D(x): 0.6180 D(G(z)): 0.2894 / 0.1908\n",
            "[14/1][41/59] Loss_D: 0.6924 Loss_G: 2.4705 D(x): 0.7703 D(G(z)): 0.3261 / 0.1010\n",
            "[14/1][42/59] Loss_D: 0.7532 Loss_G: 1.5935 D(x): 0.6704 D(G(z)): 0.2668 / 0.2284\n",
            "[14/1][43/59] Loss_D: 0.8650 Loss_G: 3.8509 D(x): 0.7727 D(G(z)): 0.4247 / 0.0277\n",
            "[14/1][44/59] Loss_D: 1.0342 Loss_G: 0.8649 D(x): 0.4089 D(G(z)): 0.0533 / 0.4587\n",
            "[14/1][45/59] Loss_D: 1.2134 Loss_G: 3.9868 D(x): 0.9589 D(G(z)): 0.6519 / 0.0243\n",
            "[14/1][46/59] Loss_D: 1.3708 Loss_G: 0.7131 D(x): 0.3202 D(G(z)): 0.0837 / 0.5219\n",
            "[14/1][47/59] Loss_D: 1.4787 Loss_G: 3.6120 D(x): 0.9145 D(G(z)): 0.7118 / 0.0377\n",
            "[14/1][48/59] Loss_D: 1.2384 Loss_G: 0.4634 D(x): 0.3625 D(G(z)): 0.0882 / 0.6560\n",
            "[14/1][49/59] Loss_D: 1.8054 Loss_G: 4.4359 D(x): 0.9207 D(G(z)): 0.7831 / 0.0177\n",
            "[14/1][50/59] Loss_D: 2.0253 Loss_G: 0.7573 D(x): 0.1809 D(G(z)): 0.0788 / 0.5033\n",
            "[14/1][51/59] Loss_D: 1.2623 Loss_G: 2.2164 D(x): 0.8054 D(G(z)): 0.6046 / 0.1325\n",
            "[14/1][52/59] Loss_D: 1.0187 Loss_G: 1.4563 D(x): 0.5155 D(G(z)): 0.2366 / 0.2610\n",
            "[14/1][53/59] Loss_D: 0.9437 Loss_G: 1.8016 D(x): 0.7060 D(G(z)): 0.4174 / 0.1862\n",
            "[14/1][54/59] Loss_D: 0.9886 Loss_G: 1.5998 D(x): 0.6167 D(G(z)): 0.3630 / 0.2221\n",
            "[14/1][55/59] Loss_D: 0.9168 Loss_G: 1.7423 D(x): 0.6487 D(G(z)): 0.3537 / 0.1955\n",
            "[14/1][56/59] Loss_D: 0.7390 Loss_G: 1.8320 D(x): 0.6852 D(G(z)): 0.2772 / 0.1817\n",
            "[14/1][57/59] Loss_D: 0.6994 Loss_G: 2.0337 D(x): 0.7422 D(G(z)): 0.3055 / 0.1513\n",
            "[14/1][58/59] Loss_D: 0.6514 Loss_G: 2.1449 D(x): 0.7598 D(G(z)): 0.2918 / 0.1350\n",
            "[15/1][0/59] Loss_D: 0.8093 Loss_G: 1.5357 D(x): 0.6572 D(G(z)): 0.2862 / 0.2423\n",
            "[15/1][1/59] Loss_D: 0.8098 Loss_G: 3.1194 D(x): 0.7929 D(G(z)): 0.4106 / 0.0550\n",
            "[15/1][2/59] Loss_D: 0.9363 Loss_G: 1.0766 D(x): 0.4717 D(G(z)): 0.1104 / 0.3732\n",
            "[15/1][3/59] Loss_D: 0.8583 Loss_G: 3.2036 D(x): 0.9011 D(G(z)): 0.5031 / 0.0516\n",
            "[15/1][4/59] Loss_D: 0.9176 Loss_G: 1.0707 D(x): 0.4968 D(G(z)): 0.1273 / 0.3747\n",
            "[15/1][5/59] Loss_D: 1.1436 Loss_G: 2.6288 D(x): 0.8431 D(G(z)): 0.5858 / 0.0878\n",
            "[15/1][6/59] Loss_D: 1.1615 Loss_G: 0.9623 D(x): 0.4083 D(G(z)): 0.1556 / 0.4215\n",
            "[15/1][7/59] Loss_D: 0.9186 Loss_G: 2.9150 D(x): 0.8680 D(G(z)): 0.5064 / 0.0656\n",
            "[15/1][8/59] Loss_D: 0.6582 Loss_G: 1.9435 D(x): 0.6001 D(G(z)): 0.1061 / 0.1649\n",
            "[15/1][9/59] Loss_D: 0.6513 Loss_G: 2.2479 D(x): 0.8154 D(G(z)): 0.3375 / 0.1253\n",
            "[15/1][10/59] Loss_D: 0.7365 Loss_G: 1.4139 D(x): 0.6642 D(G(z)): 0.2482 / 0.2703\n",
            "[15/1][11/59] Loss_D: 0.7917 Loss_G: 3.6627 D(x): 0.8435 D(G(z)): 0.4351 / 0.0366\n",
            "[15/1][12/59] Loss_D: 1.0690 Loss_G: 0.9649 D(x): 0.4071 D(G(z)): 0.0680 / 0.4283\n",
            "[15/1][13/59] Loss_D: 1.0047 Loss_G: 4.5050 D(x): 0.9438 D(G(z)): 0.5706 / 0.0146\n",
            "[15/1][14/59] Loss_D: 1.7670 Loss_G: 0.3448 D(x): 0.2125 D(G(z)): 0.0485 / 0.7357\n",
            "[15/1][15/59] Loss_D: 1.7674 Loss_G: 3.3821 D(x): 0.9612 D(G(z)): 0.7738 / 0.0463\n",
            "[15/1][16/59] Loss_D: 1.5158 Loss_G: 0.7623 D(x): 0.2861 D(G(z)): 0.0865 / 0.4974\n",
            "[15/1][17/59] Loss_D: 1.1656 Loss_G: 1.8290 D(x): 0.8457 D(G(z)): 0.5903 / 0.1837\n",
            "[15/1][18/59] Loss_D: 0.9752 Loss_G: 1.7597 D(x): 0.6067 D(G(z)): 0.3354 / 0.1936\n",
            "[15/1][19/59] Loss_D: 1.0859 Loss_G: 1.1261 D(x): 0.5360 D(G(z)): 0.3257 / 0.3505\n",
            "[15/1][20/59] Loss_D: 1.0857 Loss_G: 2.6064 D(x): 0.7339 D(G(z)): 0.5051 / 0.0908\n",
            "[15/1][21/59] Loss_D: 1.0563 Loss_G: 1.0199 D(x): 0.4613 D(G(z)): 0.1887 / 0.3853\n",
            "[15/1][22/59] Loss_D: 1.0738 Loss_G: 2.5275 D(x): 0.8065 D(G(z)): 0.5487 / 0.0938\n",
            "[15/1][23/59] Loss_D: 0.9554 Loss_G: 1.3368 D(x): 0.5210 D(G(z)): 0.2170 / 0.2951\n",
            "[15/1][24/59] Loss_D: 0.9071 Loss_G: 2.2425 D(x): 0.7713 D(G(z)): 0.4470 / 0.1261\n",
            "[15/1][25/59] Loss_D: 0.7693 Loss_G: 1.7699 D(x): 0.6239 D(G(z)): 0.2188 / 0.1968\n",
            "[15/1][26/59] Loss_D: 0.7369 Loss_G: 2.4875 D(x): 0.7860 D(G(z)): 0.3638 / 0.1014\n",
            "[15/1][27/59] Loss_D: 0.7575 Loss_G: 1.5072 D(x): 0.6417 D(G(z)): 0.2320 / 0.2519\n",
            "[15/1][28/59] Loss_D: 0.8588 Loss_G: 3.0758 D(x): 0.8020 D(G(z)): 0.4372 / 0.0593\n",
            "[15/1][29/59] Loss_D: 0.9717 Loss_G: 0.7547 D(x): 0.4595 D(G(z)): 0.1161 / 0.4987\n",
            "[15/1][30/59] Loss_D: 1.1341 Loss_G: 4.1192 D(x): 0.9378 D(G(z)): 0.6201 / 0.0200\n",
            "[15/1][31/59] Loss_D: 1.1802 Loss_G: 1.1636 D(x): 0.3528 D(G(z)): 0.0545 / 0.3473\n",
            "[15/1][32/59] Loss_D: 1.0748 Loss_G: 2.8318 D(x): 0.9022 D(G(z)): 0.5905 / 0.0738\n",
            "[15/1][33/59] Loss_D: 1.0921 Loss_G: 1.0597 D(x): 0.4511 D(G(z)): 0.1926 / 0.3764\n",
            "[15/1][34/59] Loss_D: 0.9387 Loss_G: 3.5579 D(x): 0.8325 D(G(z)): 0.4999 / 0.0359\n",
            "[15/1][35/59] Loss_D: 1.0021 Loss_G: 1.0111 D(x): 0.4345 D(G(z)): 0.0872 / 0.3952\n",
            "[15/1][36/59] Loss_D: 1.0144 Loss_G: 3.3578 D(x): 0.9250 D(G(z)): 0.5759 / 0.0472\n",
            "[15/1][37/59] Loss_D: 0.9659 Loss_G: 1.2061 D(x): 0.4775 D(G(z)): 0.1220 / 0.3301\n",
            "[15/1][38/59] Loss_D: 1.0002 Loss_G: 3.4678 D(x): 0.8816 D(G(z)): 0.5446 / 0.0409\n",
            "[15/1][39/59] Loss_D: 1.0809 Loss_G: 0.9608 D(x): 0.4111 D(G(z)): 0.0951 / 0.4189\n",
            "[15/1][40/59] Loss_D: 1.0427 Loss_G: 3.3284 D(x): 0.8939 D(G(z)): 0.5659 / 0.0464\n",
            "[15/1][41/59] Loss_D: 1.0644 Loss_G: 1.0359 D(x): 0.4116 D(G(z)): 0.0800 / 0.3841\n",
            "[15/1][42/59] Loss_D: 0.9258 Loss_G: 2.7149 D(x): 0.9008 D(G(z)): 0.5320 / 0.0839\n",
            "[15/1][43/59] Loss_D: 0.7216 Loss_G: 1.8686 D(x): 0.6177 D(G(z)): 0.1737 / 0.1801\n",
            "[15/1][44/59] Loss_D: 0.6261 Loss_G: 1.9940 D(x): 0.7832 D(G(z)): 0.2911 / 0.1602\n",
            "[15/1][45/59] Loss_D: 0.6679 Loss_G: 2.2133 D(x): 0.7487 D(G(z)): 0.2891 / 0.1303\n",
            "[15/1][46/59] Loss_D: 0.6039 Loss_G: 2.0948 D(x): 0.7365 D(G(z)): 0.2308 / 0.1463\n",
            "[15/1][47/59] Loss_D: 0.5088 Loss_G: 2.7704 D(x): 0.8251 D(G(z)): 0.2526 / 0.0782\n",
            "[15/1][48/59] Loss_D: 0.4374 Loss_G: 2.1883 D(x): 0.7647 D(G(z)): 0.1343 / 0.1331\n",
            "[15/1][49/59] Loss_D: 0.3921 Loss_G: 3.4473 D(x): 0.8800 D(G(z)): 0.2193 / 0.0405\n",
            "[15/1][50/59] Loss_D: 0.4401 Loss_G: 1.7904 D(x): 0.7489 D(G(z)): 0.1207 / 0.1975\n",
            "[15/1][51/59] Loss_D: 0.5887 Loss_G: 4.9783 D(x): 0.9119 D(G(z)): 0.3671 / 0.0096\n",
            "[15/1][52/59] Loss_D: 1.3801 Loss_G: 0.8741 D(x): 0.3012 D(G(z)): 0.0183 / 0.4494\n",
            "[15/1][53/59] Loss_D: 0.8269 Loss_G: 3.1557 D(x): 0.9264 D(G(z)): 0.4992 / 0.0525\n",
            "[15/1][54/59] Loss_D: 1.1642 Loss_G: 0.4923 D(x): 0.3904 D(G(z)): 0.1177 / 0.6310\n",
            "[15/1][55/59] Loss_D: 1.7508 Loss_G: 4.0230 D(x): 0.9329 D(G(z)): 0.7864 / 0.0251\n",
            "[15/1][56/59] Loss_D: 2.0794 Loss_G: 0.4835 D(x): 0.1643 D(G(z)): 0.0556 / 0.6358\n",
            "[15/1][57/59] Loss_D: 1.6105 Loss_G: 2.6178 D(x): 0.9062 D(G(z)): 0.7502 / 0.0864\n",
            "[15/1][58/59] Loss_D: 1.2427 Loss_G: 1.1650 D(x): 0.3933 D(G(z)): 0.1983 / 0.3370\n",
            "[16/1][0/59] Loss_D: 1.1984 Loss_G: 1.8566 D(x): 0.7226 D(G(z)): 0.5532 / 0.1739\n",
            "[16/1][1/59] Loss_D: 1.1353 Loss_G: 1.3614 D(x): 0.5091 D(G(z)): 0.3231 / 0.2848\n",
            "[16/1][2/59] Loss_D: 1.2043 Loss_G: 2.3855 D(x): 0.6706 D(G(z)): 0.5122 / 0.1167\n",
            "[16/1][3/59] Loss_D: 1.0644 Loss_G: 1.2496 D(x): 0.4922 D(G(z)): 0.2393 / 0.3160\n",
            "[16/1][4/59] Loss_D: 1.0040 Loss_G: 2.7987 D(x): 0.8060 D(G(z)): 0.5138 / 0.0779\n",
            "[16/1][5/59] Loss_D: 0.9393 Loss_G: 1.2002 D(x): 0.5171 D(G(z)): 0.1917 / 0.3326\n",
            "[16/1][6/59] Loss_D: 1.2345 Loss_G: 3.4778 D(x): 0.8427 D(G(z)): 0.6174 / 0.0381\n",
            "[16/1][7/59] Loss_D: 1.5939 Loss_G: 0.4719 D(x): 0.2633 D(G(z)): 0.1206 / 0.6464\n",
            "[16/1][8/59] Loss_D: 1.7563 Loss_G: 2.6962 D(x): 0.8719 D(G(z)): 0.7640 / 0.0841\n",
            "[16/1][9/59] Loss_D: 1.2794 Loss_G: 1.3310 D(x): 0.3796 D(G(z)): 0.1654 / 0.2881\n",
            "[16/1][10/59] Loss_D: 0.9439 Loss_G: 1.7473 D(x): 0.7192 D(G(z)): 0.4293 / 0.1981\n",
            "[16/1][11/59] Loss_D: 0.8133 Loss_G: 2.2683 D(x): 0.7218 D(G(z)): 0.3555 / 0.1202\n",
            "[16/1][12/59] Loss_D: 0.8736 Loss_G: 1.5268 D(x): 0.5913 D(G(z)): 0.2592 / 0.2421\n",
            "[16/1][13/59] Loss_D: 0.9087 Loss_G: 2.6173 D(x): 0.7802 D(G(z)): 0.4528 / 0.0872\n",
            "[16/1][14/59] Loss_D: 0.9239 Loss_G: 1.1461 D(x): 0.5299 D(G(z)): 0.2039 / 0.3476\n",
            "[16/1][15/59] Loss_D: 1.0131 Loss_G: 3.7257 D(x): 0.8531 D(G(z)): 0.5430 / 0.0310\n",
            "[16/1][16/59] Loss_D: 1.2702 Loss_G: 0.7062 D(x): 0.3492 D(G(z)): 0.0960 / 0.5246\n",
            "[16/1][17/59] Loss_D: 1.2521 Loss_G: 2.9234 D(x): 0.9236 D(G(z)): 0.6580 / 0.0663\n",
            "[16/1][18/59] Loss_D: 0.9114 Loss_G: 1.6928 D(x): 0.5013 D(G(z)): 0.1415 / 0.2161\n",
            "[16/1][19/59] Loss_D: 0.6909 Loss_G: 1.9979 D(x): 0.7981 D(G(z)): 0.3464 / 0.1576\n",
            "[16/1][20/59] Loss_D: 0.6123 Loss_G: 2.2207 D(x): 0.7547 D(G(z)): 0.2583 / 0.1323\n",
            "[16/1][21/59] Loss_D: 0.6391 Loss_G: 1.7608 D(x): 0.7106 D(G(z)): 0.2287 / 0.1994\n",
            "[16/1][22/59] Loss_D: 0.7080 Loss_G: 2.9636 D(x): 0.8007 D(G(z)): 0.3597 / 0.0631\n",
            "[16/1][23/59] Loss_D: 0.8240 Loss_G: 0.7577 D(x): 0.5302 D(G(z)): 0.1190 / 0.4973\n",
            "[16/1][24/59] Loss_D: 1.3294 Loss_G: 4.9291 D(x): 0.9460 D(G(z)): 0.6852 / 0.0100\n",
            "[16/1][25/59] Loss_D: 1.8689 Loss_G: 1.1599 D(x): 0.1908 D(G(z)): 0.0261 / 0.3545\n",
            "[16/1][26/59] Loss_D: 0.8719 Loss_G: 2.0859 D(x): 0.8587 D(G(z)): 0.4749 / 0.1526\n",
            "[16/1][27/59] Loss_D: 0.8203 Loss_G: 1.6194 D(x): 0.6470 D(G(z)): 0.2804 / 0.2271\n",
            "[16/1][28/59] Loss_D: 0.9064 Loss_G: 1.7791 D(x): 0.7057 D(G(z)): 0.3943 / 0.1928\n",
            "[16/1][29/59] Loss_D: 0.9092 Loss_G: 1.6992 D(x): 0.6377 D(G(z)): 0.3357 / 0.2130\n",
            "[16/1][30/59] Loss_D: 0.7689 Loss_G: 2.2140 D(x): 0.7211 D(G(z)): 0.3222 / 0.1285\n",
            "[16/1][31/59] Loss_D: 0.6631 Loss_G: 1.9718 D(x): 0.7110 D(G(z)): 0.2460 / 0.1641\n",
            "[16/1][32/59] Loss_D: 0.6553 Loss_G: 3.0304 D(x): 0.7903 D(G(z)): 0.3134 / 0.0630\n",
            "[16/1][33/59] Loss_D: 0.7351 Loss_G: 0.7141 D(x): 0.5815 D(G(z)): 0.1322 / 0.5204\n",
            "[16/1][34/59] Loss_D: 1.4788 Loss_G: 7.0599 D(x): 0.9606 D(G(z)): 0.7258 / 0.0013\n",
            "[16/1][35/59] Loss_D: 2.6587 Loss_G: 0.5670 D(x): 0.0929 D(G(z)): 0.0097 / 0.6042\n",
            "[16/1][36/59] Loss_D: 1.6931 Loss_G: 4.1755 D(x): 0.9335 D(G(z)): 0.7585 / 0.0224\n",
            "[16/1][37/59] Loss_D: 2.4762 Loss_G: 0.2574 D(x): 0.1159 D(G(z)): 0.0557 / 0.7824\n",
            "[16/1][38/59] Loss_D: 2.1343 Loss_G: 2.4174 D(x): 0.9328 D(G(z)): 0.8484 / 0.1068\n",
            "[16/1][39/59] Loss_D: 1.1908 Loss_G: 1.3657 D(x): 0.4131 D(G(z)): 0.1951 / 0.2802\n",
            "[16/1][40/59] Loss_D: 1.0627 Loss_G: 1.3839 D(x): 0.6526 D(G(z)): 0.4367 / 0.2724\n",
            "[16/1][41/59] Loss_D: 0.9850 Loss_G: 1.9099 D(x): 0.6609 D(G(z)): 0.4070 / 0.1652\n",
            "[16/1][42/59] Loss_D: 0.9502 Loss_G: 1.3162 D(x): 0.5507 D(G(z)): 0.2597 / 0.2947\n",
            "[16/1][43/59] Loss_D: 0.9749 Loss_G: 2.0459 D(x): 0.7541 D(G(z)): 0.4691 / 0.1482\n",
            "[16/1][44/59] Loss_D: 0.9753 Loss_G: 1.7454 D(x): 0.6067 D(G(z)): 0.3397 / 0.2017\n",
            "[16/1][45/59] Loss_D: 1.0280 Loss_G: 1.8575 D(x): 0.6294 D(G(z)): 0.3908 / 0.1791\n",
            "[16/1][46/59] Loss_D: 0.8016 Loss_G: 1.9348 D(x): 0.6656 D(G(z)): 0.2941 / 0.1687\n",
            "[16/1][47/59] Loss_D: 0.7416 Loss_G: 2.2073 D(x): 0.7556 D(G(z)): 0.3442 / 0.1272\n",
            "[16/1][48/59] Loss_D: 0.8865 Loss_G: 1.8371 D(x): 0.6601 D(G(z)): 0.3422 / 0.1811\n",
            "[16/1][49/59] Loss_D: 0.9586 Loss_G: 1.9152 D(x): 0.6318 D(G(z)): 0.3576 / 0.1675\n",
            "[16/1][50/59] Loss_D: 0.8109 Loss_G: 2.3643 D(x): 0.6973 D(G(z)): 0.3314 / 0.1104\n",
            "[16/1][51/59] Loss_D: 0.7243 Loss_G: 1.7591 D(x): 0.6592 D(G(z)): 0.2326 / 0.1939\n",
            "[16/1][52/59] Loss_D: 0.7202 Loss_G: 2.9777 D(x): 0.8074 D(G(z)): 0.3746 / 0.0623\n",
            "[16/1][53/59] Loss_D: 0.8465 Loss_G: 1.1035 D(x): 0.5440 D(G(z)): 0.1645 / 0.3648\n",
            "[16/1][54/59] Loss_D: 1.0337 Loss_G: 4.3703 D(x): 0.8951 D(G(z)): 0.5721 / 0.0172\n",
            "[16/1][55/59] Loss_D: 1.3743 Loss_G: 0.7246 D(x): 0.3081 D(G(z)): 0.0534 / 0.5130\n",
            "[16/1][56/59] Loss_D: 1.2157 Loss_G: 3.3488 D(x): 0.9288 D(G(z)): 0.6484 / 0.0441\n",
            "[16/1][57/59] Loss_D: 1.3171 Loss_G: 1.0282 D(x): 0.3261 D(G(z)): 0.0802 / 0.3957\n",
            "[16/1][58/59] Loss_D: 1.1100 Loss_G: 2.9726 D(x): 0.8922 D(G(z)): 0.5867 / 0.0618\n",
            "[17/1][0/59] Loss_D: 1.0185 Loss_G: 1.0759 D(x): 0.4532 D(G(z)): 0.1399 / 0.3693\n",
            "[17/1][1/59] Loss_D: 1.0487 Loss_G: 2.6074 D(x): 0.8575 D(G(z)): 0.5632 / 0.0878\n",
            "[17/1][2/59] Loss_D: 0.8947 Loss_G: 1.3313 D(x): 0.5189 D(G(z)): 0.1689 / 0.2910\n",
            "[17/1][3/59] Loss_D: 0.7538 Loss_G: 2.8163 D(x): 0.8530 D(G(z)): 0.4228 / 0.0723\n",
            "[17/1][4/59] Loss_D: 0.7050 Loss_G: 1.4536 D(x): 0.6032 D(G(z)): 0.1455 / 0.2644\n",
            "[17/1][5/59] Loss_D: 0.7558 Loss_G: 2.9105 D(x): 0.8807 D(G(z)): 0.4415 / 0.0670\n",
            "[17/1][6/59] Loss_D: 0.6892 Loss_G: 1.5581 D(x): 0.6008 D(G(z)): 0.1241 / 0.2411\n",
            "[17/1][7/59] Loss_D: 0.7943 Loss_G: 3.2132 D(x): 0.8787 D(G(z)): 0.4556 / 0.0514\n",
            "[17/1][8/59] Loss_D: 0.8746 Loss_G: 1.5610 D(x): 0.4829 D(G(z)): 0.0720 / 0.2396\n",
            "[17/1][9/59] Loss_D: 0.5262 Loss_G: 3.5572 D(x): 0.8924 D(G(z)): 0.3200 / 0.0347\n",
            "[17/1][10/59] Loss_D: 0.8404 Loss_G: 0.5801 D(x): 0.4996 D(G(z)): 0.0817 / 0.5907\n",
            "[17/1][11/59] Loss_D: 1.6453 Loss_G: 4.9480 D(x): 0.9863 D(G(z)): 0.7640 / 0.0101\n",
            "[17/1][12/59] Loss_D: 2.3617 Loss_G: 0.5517 D(x): 0.1231 D(G(z)): 0.0289 / 0.6043\n",
            "[17/1][13/59] Loss_D: 1.3608 Loss_G: 1.8736 D(x): 0.8644 D(G(z)): 0.6603 / 0.1762\n",
            "[17/1][14/59] Loss_D: 1.0630 Loss_G: 1.2868 D(x): 0.5078 D(G(z)): 0.2679 / 0.3005\n",
            "[17/1][15/59] Loss_D: 1.0421 Loss_G: 1.1639 D(x): 0.6219 D(G(z)): 0.4023 / 0.3364\n",
            "[17/1][16/59] Loss_D: 1.0124 Loss_G: 1.6404 D(x): 0.6763 D(G(z)): 0.4365 / 0.2134\n",
            "[17/1][17/59] Loss_D: 0.9625 Loss_G: 1.2536 D(x): 0.5770 D(G(z)): 0.3039 / 0.3061\n",
            "[17/1][18/59] Loss_D: 0.9675 Loss_G: 1.7419 D(x): 0.7112 D(G(z)): 0.4402 / 0.1938\n",
            "[17/1][19/59] Loss_D: 0.9621 Loss_G: 1.3799 D(x): 0.5788 D(G(z)): 0.3075 / 0.2766\n",
            "[17/1][20/59] Loss_D: 0.9510 Loss_G: 1.6851 D(x): 0.6652 D(G(z)): 0.3884 / 0.2097\n",
            "[17/1][21/59] Loss_D: 0.8305 Loss_G: 1.8166 D(x): 0.6659 D(G(z)): 0.3144 / 0.1874\n",
            "[17/1][22/59] Loss_D: 0.7055 Loss_G: 2.0659 D(x): 0.7428 D(G(z)): 0.3100 / 0.1503\n",
            "[17/1][23/59] Loss_D: 0.7933 Loss_G: 1.8342 D(x): 0.7005 D(G(z)): 0.3202 / 0.1855\n",
            "[17/1][24/59] Loss_D: 0.7697 Loss_G: 2.7211 D(x): 0.7283 D(G(z)): 0.3350 / 0.0803\n",
            "[17/1][25/59] Loss_D: 0.6732 Loss_G: 1.1929 D(x): 0.6119 D(G(z)): 0.1321 / 0.3342\n",
            "[17/1][26/59] Loss_D: 0.8441 Loss_G: 5.3251 D(x): 0.9313 D(G(z)): 0.5102 / 0.0066\n",
            "[17/1][27/59] Loss_D: 1.6238 Loss_G: 0.6633 D(x): 0.2468 D(G(z)): 0.0344 / 0.5435\n",
            "[17/1][28/59] Loss_D: 1.4058 Loss_G: 4.4076 D(x): 0.9689 D(G(z)): 0.7143 / 0.0166\n",
            "[17/1][29/59] Loss_D: 1.2157 Loss_G: 0.8571 D(x): 0.3622 D(G(z)): 0.0911 / 0.4568\n",
            "[17/1][30/59] Loss_D: 1.5525 Loss_G: 2.7956 D(x): 0.8727 D(G(z)): 0.7248 / 0.0799\n",
            "[17/1][31/59] Loss_D: 1.3495 Loss_G: 1.2702 D(x): 0.3540 D(G(z)): 0.1797 / 0.3109\n",
            "[17/1][32/59] Loss_D: 1.0163 Loss_G: 2.6896 D(x): 0.7648 D(G(z)): 0.4968 / 0.0840\n",
            "[17/1][33/59] Loss_D: 0.8123 Loss_G: 1.7204 D(x): 0.5800 D(G(z)): 0.1863 / 0.2028\n",
            "[17/1][34/59] Loss_D: 0.8010 Loss_G: 2.3784 D(x): 0.7821 D(G(z)): 0.3930 / 0.1107\n",
            "[17/1][35/59] Loss_D: 0.7980 Loss_G: 1.9419 D(x): 0.6616 D(G(z)): 0.2866 / 0.1655\n",
            "[17/1][36/59] Loss_D: 0.8251 Loss_G: 2.0623 D(x): 0.6793 D(G(z)): 0.3232 / 0.1534\n",
            "[17/1][37/59] Loss_D: 0.7146 Loss_G: 2.3762 D(x): 0.7247 D(G(z)): 0.2952 / 0.1106\n",
            "[17/1][38/59] Loss_D: 0.5841 Loss_G: 1.8658 D(x): 0.7072 D(G(z)): 0.1885 / 0.1795\n",
            "[17/1][39/59] Loss_D: 0.5456 Loss_G: 2.9246 D(x): 0.8509 D(G(z)): 0.2987 / 0.0670\n",
            "[17/1][40/59] Loss_D: 0.5000 Loss_G: 1.5976 D(x): 0.7006 D(G(z)): 0.1118 / 0.2310\n",
            "[17/1][41/59] Loss_D: 0.6480 Loss_G: 3.4091 D(x): 0.9143 D(G(z)): 0.4027 / 0.0412\n",
            "[17/1][42/59] Loss_D: 0.5744 Loss_G: 1.7887 D(x): 0.6363 D(G(z)): 0.0848 / 0.1935\n",
            "[17/1][43/59] Loss_D: 0.5899 Loss_G: 3.2172 D(x): 0.8898 D(G(z)): 0.3538 / 0.0496\n",
            "[17/1][44/59] Loss_D: 0.4315 Loss_G: 2.2435 D(x): 0.7114 D(G(z)): 0.0688 / 0.1265\n",
            "[17/1][45/59] Loss_D: 0.4959 Loss_G: 3.3991 D(x): 0.9096 D(G(z)): 0.3132 / 0.0421\n",
            "[17/1][46/59] Loss_D: 0.4961 Loss_G: 1.6706 D(x): 0.6950 D(G(z)): 0.1006 / 0.2146\n",
            "[17/1][47/59] Loss_D: 0.5320 Loss_G: 3.9423 D(x): 0.9075 D(G(z)): 0.3324 / 0.0257\n",
            "[17/1][48/59] Loss_D: 0.6316 Loss_G: 1.4446 D(x): 0.5737 D(G(z)): 0.0293 / 0.2623\n",
            "[17/1][49/59] Loss_D: 0.7423 Loss_G: 3.9992 D(x): 0.9576 D(G(z)): 0.4781 / 0.0226\n",
            "[17/1][50/59] Loss_D: 0.6568 Loss_G: 2.0957 D(x): 0.5610 D(G(z)): 0.0328 / 0.1458\n",
            "[17/1][51/59] Loss_D: 0.6200 Loss_G: 3.9156 D(x): 0.9342 D(G(z)): 0.4004 / 0.0271\n",
            "[17/1][52/59] Loss_D: 0.5890 Loss_G: 1.6364 D(x): 0.6399 D(G(z)): 0.0855 / 0.2281\n",
            "[17/1][53/59] Loss_D: 0.7950 Loss_G: 5.6736 D(x): 0.9320 D(G(z)): 0.4818 / 0.0053\n",
            "[17/1][54/59] Loss_D: 1.5137 Loss_G: 0.9509 D(x): 0.2622 D(G(z)): 0.0103 / 0.4262\n",
            "[17/1][55/59] Loss_D: 1.3760 Loss_G: 4.8788 D(x): 0.9747 D(G(z)): 0.7046 / 0.0099\n",
            "[17/1][56/59] Loss_D: 2.4411 Loss_G: 0.2148 D(x): 0.1097 D(G(z)): 0.0277 / 0.8159\n",
            "[17/1][57/59] Loss_D: 1.9124 Loss_G: 4.4258 D(x): 0.9755 D(G(z)): 0.8160 / 0.0186\n",
            "[17/1][58/59] Loss_D: 1.9409 Loss_G: 0.4971 D(x): 0.1986 D(G(z)): 0.0658 / 0.6343\n",
            "[18/1][0/59] Loss_D: 1.5811 Loss_G: 2.1258 D(x): 0.9204 D(G(z)): 0.7339 / 0.1446\n",
            "[18/1][1/59] Loss_D: 0.8758 Loss_G: 2.0335 D(x): 0.5827 D(G(z)): 0.2384 / 0.1554\n",
            "[18/1][2/59] Loss_D: 0.8918 Loss_G: 1.0544 D(x): 0.5953 D(G(z)): 0.2729 / 0.3807\n",
            "[18/1][3/59] Loss_D: 1.1559 Loss_G: 2.1650 D(x): 0.8032 D(G(z)): 0.5768 / 0.1346\n",
            "[18/1][4/59] Loss_D: 1.1848 Loss_G: 1.0732 D(x): 0.4302 D(G(z)): 0.2213 / 0.3707\n",
            "[18/1][5/59] Loss_D: 0.8799 Loss_G: 2.2435 D(x): 0.7787 D(G(z)): 0.4405 / 0.1229\n",
            "[18/1][6/59] Loss_D: 0.7246 Loss_G: 1.6447 D(x): 0.6410 D(G(z)): 0.2140 / 0.2160\n",
            "[18/1][7/59] Loss_D: 0.8159 Loss_G: 2.0323 D(x): 0.7554 D(G(z)): 0.3894 / 0.1484\n",
            "[18/1][8/59] Loss_D: 0.8446 Loss_G: 1.9906 D(x): 0.6496 D(G(z)): 0.3087 / 0.1590\n",
            "[18/1][9/59] Loss_D: 0.7537 Loss_G: 1.8661 D(x): 0.6745 D(G(z)): 0.2734 / 0.1795\n",
            "[18/1][10/59] Loss_D: 0.6850 Loss_G: 2.5916 D(x): 0.7864 D(G(z)): 0.3358 / 0.0908\n",
            "[18/1][11/59] Loss_D: 0.6848 Loss_G: 1.5736 D(x): 0.6586 D(G(z)): 0.2061 / 0.2376\n",
            "[18/1][12/59] Loss_D: 0.8387 Loss_G: 3.8057 D(x): 0.8249 D(G(z)): 0.4447 / 0.0288\n",
            "[18/1][13/59] Loss_D: 1.2520 Loss_G: 0.6024 D(x): 0.3423 D(G(z)): 0.0779 / 0.5746\n",
            "[18/1][14/59] Loss_D: 1.4137 Loss_G: 4.7200 D(x): 0.9622 D(G(z)): 0.7117 / 0.0123\n",
            "[18/1][15/59] Loss_D: 1.4747 Loss_G: 0.9047 D(x): 0.2792 D(G(z)): 0.0454 / 0.4445\n",
            "[18/1][16/59] Loss_D: 1.3242 Loss_G: 2.4754 D(x): 0.8778 D(G(z)): 0.6518 / 0.1086\n",
            "[18/1][17/59] Loss_D: 1.0225 Loss_G: 1.8748 D(x): 0.5216 D(G(z)): 0.2415 / 0.1863\n",
            "[18/1][18/59] Loss_D: 0.7919 Loss_G: 1.9702 D(x): 0.6851 D(G(z)): 0.3034 / 0.1637\n",
            "[18/1][19/59] Loss_D: 0.7065 Loss_G: 2.4189 D(x): 0.7537 D(G(z)): 0.3181 / 0.1101\n",
            "[18/1][20/59] Loss_D: 0.7094 Loss_G: 1.4397 D(x): 0.6628 D(G(z)): 0.2245 / 0.2642\n",
            "[18/1][21/59] Loss_D: 0.8192 Loss_G: 3.1804 D(x): 0.8149 D(G(z)): 0.4295 / 0.0518\n",
            "[18/1][22/59] Loss_D: 0.9269 Loss_G: 0.8281 D(x): 0.4756 D(G(z)): 0.0957 / 0.4676\n",
            "[18/1][23/59] Loss_D: 1.1256 Loss_G: 4.4112 D(x): 0.9444 D(G(z)): 0.6228 / 0.0165\n",
            "[18/1][24/59] Loss_D: 1.2854 Loss_G: 1.1919 D(x): 0.3312 D(G(z)): 0.0392 / 0.3585\n",
            "[18/1][25/59] Loss_D: 1.1446 Loss_G: 3.0316 D(x): 0.9125 D(G(z)): 0.5901 / 0.0649\n",
            "[18/1][26/59] Loss_D: 1.1586 Loss_G: 0.9309 D(x): 0.4348 D(G(z)): 0.1880 / 0.4440\n",
            "[18/1][27/59] Loss_D: 1.1648 Loss_G: 4.1940 D(x): 0.8985 D(G(z)): 0.6024 / 0.0205\n",
            "[18/1][28/59] Loss_D: 1.2134 Loss_G: 1.0780 D(x): 0.3509 D(G(z)): 0.0495 / 0.3776\n",
            "[18/1][29/59] Loss_D: 0.9651 Loss_G: 2.5054 D(x): 0.9017 D(G(z)): 0.5398 / 0.1001\n",
            "[18/1][30/59] Loss_D: 0.7032 Loss_G: 2.1329 D(x): 0.6586 D(G(z)): 0.2109 / 0.1410\n",
            "[18/1][31/59] Loss_D: 0.7575 Loss_G: 1.7968 D(x): 0.6812 D(G(z)): 0.2789 / 0.1918\n",
            "[18/1][32/59] Loss_D: 0.6109 Loss_G: 2.9517 D(x): 0.7850 D(G(z)): 0.2863 / 0.0646\n",
            "[18/1][33/59] Loss_D: 0.4734 Loss_G: 1.8446 D(x): 0.7354 D(G(z)): 0.1303 / 0.1831\n",
            "[18/1][34/59] Loss_D: 0.5641 Loss_G: 3.6679 D(x): 0.9096 D(G(z)): 0.3531 / 0.0315\n",
            "[18/1][35/59] Loss_D: 0.6776 Loss_G: 1.2847 D(x): 0.5744 D(G(z)): 0.0792 / 0.3075\n",
            "[18/1][36/59] Loss_D: 0.9406 Loss_G: 4.0203 D(x): 0.9392 D(G(z)): 0.5512 / 0.0231\n",
            "[18/1][37/59] Loss_D: 1.0964 Loss_G: 1.1477 D(x): 0.3821 D(G(z)): 0.0484 / 0.3539\n",
            "[18/1][38/59] Loss_D: 0.7991 Loss_G: 2.9633 D(x): 0.9158 D(G(z)): 0.4798 / 0.0652\n",
            "[18/1][39/59] Loss_D: 0.6168 Loss_G: 1.8768 D(x): 0.6655 D(G(z)): 0.1521 / 0.1801\n",
            "[18/1][40/59] Loss_D: 0.5551 Loss_G: 2.7254 D(x): 0.8196 D(G(z)): 0.2783 / 0.0802\n",
            "[18/1][41/59] Loss_D: 0.4197 Loss_G: 1.9453 D(x): 0.7169 D(G(z)): 0.0601 / 0.1750\n",
            "[18/1][42/59] Loss_D: 0.5691 Loss_G: 4.6772 D(x): 0.9543 D(G(z)): 0.3781 / 0.0123\n",
            "[18/1][43/59] Loss_D: 0.6822 Loss_G: 1.2087 D(x): 0.5679 D(G(z)): 0.0653 / 0.3327\n",
            "[18/1][44/59] Loss_D: 1.0131 Loss_G: 4.9606 D(x): 0.9531 D(G(z)): 0.5836 / 0.0092\n",
            "[18/1][45/59] Loss_D: 1.2536 Loss_G: 1.6698 D(x): 0.3262 D(G(z)): 0.0189 / 0.2236\n",
            "[18/1][46/59] Loss_D: 0.7581 Loss_G: 3.6190 D(x): 0.9256 D(G(z)): 0.4627 / 0.0344\n",
            "[18/1][47/59] Loss_D: 1.2482 Loss_G: 0.3305 D(x): 0.3572 D(G(z)): 0.0942 / 0.7343\n",
            "[18/1][48/59] Loss_D: 1.9201 Loss_G: 4.4734 D(x): 0.9853 D(G(z)): 0.8178 / 0.0163\n",
            "[18/1][49/59] Loss_D: 1.2410 Loss_G: 1.3350 D(x): 0.3712 D(G(z)): 0.0905 / 0.3004\n",
            "[18/1][50/59] Loss_D: 0.9483 Loss_G: 2.2242 D(x): 0.8285 D(G(z)): 0.4976 / 0.1301\n",
            "[18/1][51/59] Loss_D: 0.8892 Loss_G: 2.4537 D(x): 0.6400 D(G(z)): 0.3147 / 0.1028\n",
            "[18/1][52/59] Loss_D: 0.7685 Loss_G: 1.8214 D(x): 0.6213 D(G(z)): 0.2178 / 0.1876\n",
            "[18/1][53/59] Loss_D: 0.6285 Loss_G: 3.6456 D(x): 0.8578 D(G(z)): 0.3552 / 0.0325\n",
            "[18/1][54/59] Loss_D: 0.6673 Loss_G: 1.2883 D(x): 0.6209 D(G(z)): 0.1362 / 0.3155\n",
            "[18/1][55/59] Loss_D: 1.0883 Loss_G: 4.8176 D(x): 0.9156 D(G(z)): 0.5917 / 0.0116\n",
            "[18/1][56/59] Loss_D: 1.5095 Loss_G: 0.8389 D(x): 0.2713 D(G(z)): 0.0377 / 0.4632\n",
            "[18/1][57/59] Loss_D: 1.1299 Loss_G: 4.3425 D(x): 0.9316 D(G(z)): 0.6160 / 0.0181\n",
            "[18/1][58/59] Loss_D: 1.2355 Loss_G: 1.1833 D(x): 0.3509 D(G(z)): 0.0525 / 0.3663\n",
            "[19/1][0/59] Loss_D: 1.1116 Loss_G: 3.5008 D(x): 0.9285 D(G(z)): 0.5890 / 0.0405\n",
            "[19/1][1/59] Loss_D: 1.1309 Loss_G: 0.9235 D(x): 0.4061 D(G(z)): 0.1140 / 0.4314\n",
            "[19/1][2/59] Loss_D: 1.1414 Loss_G: 3.0700 D(x): 0.9047 D(G(z)): 0.6113 / 0.0582\n",
            "[19/1][3/59] Loss_D: 0.9037 Loss_G: 1.3705 D(x): 0.5094 D(G(z)): 0.1497 / 0.2864\n",
            "[19/1][4/59] Loss_D: 0.8437 Loss_G: 2.4225 D(x): 0.8169 D(G(z)): 0.4419 / 0.1093\n",
            "[19/1][5/59] Loss_D: 0.7503 Loss_G: 1.7287 D(x): 0.6226 D(G(z)): 0.2022 / 0.2057\n",
            "[19/1][6/59] Loss_D: 0.6880 Loss_G: 2.6331 D(x): 0.8019 D(G(z)): 0.3482 / 0.0884\n",
            "[19/1][7/59] Loss_D: 0.7244 Loss_G: 1.3729 D(x): 0.6293 D(G(z)): 0.1902 / 0.2857\n",
            "[19/1][8/59] Loss_D: 0.7758 Loss_G: 3.3333 D(x): 0.8646 D(G(z)): 0.4344 / 0.0446\n",
            "[19/1][9/59] Loss_D: 0.7696 Loss_G: 1.3174 D(x): 0.5437 D(G(z)): 0.1020 / 0.2995\n",
            "[19/1][10/59] Loss_D: 0.6340 Loss_G: 3.4290 D(x): 0.9116 D(G(z)): 0.3955 / 0.0412\n",
            "[19/1][11/59] Loss_D: 0.5148 Loss_G: 2.1386 D(x): 0.6533 D(G(z)): 0.0516 / 0.1426\n",
            "[19/1][12/59] Loss_D: 0.4454 Loss_G: 3.3231 D(x): 0.9306 D(G(z)): 0.2958 / 0.0452\n",
            "[19/1][13/59] Loss_D: 0.4446 Loss_G: 2.6023 D(x): 0.7869 D(G(z)): 0.1684 / 0.0912\n",
            "[19/1][14/59] Loss_D: 0.3689 Loss_G: 2.3152 D(x): 0.8076 D(G(z)): 0.1303 / 0.1172\n",
            "[19/1][15/59] Loss_D: 0.1870 Loss_G: 2.8396 D(x): 0.8951 D(G(z)): 0.0689 / 0.0737\n",
            "[19/1][16/59] Loss_D: 0.3878 Loss_G: 4.1620 D(x): 0.9447 D(G(z)): 0.2678 / 0.0197\n",
            "[19/1][17/59] Loss_D: 0.3949 Loss_G: 2.4689 D(x): 0.7359 D(G(z)): 0.0636 / 0.1025\n",
            "[19/1][18/59] Loss_D: 0.2863 Loss_G: 3.3201 D(x): 0.9231 D(G(z)): 0.1773 / 0.0473\n",
            "[19/1][19/59] Loss_D: 0.1861 Loss_G: 2.9997 D(x): 0.8712 D(G(z)): 0.0409 / 0.0633\n",
            "[19/1][20/59] Loss_D: 0.2063 Loss_G: 3.6245 D(x): 0.9427 D(G(z)): 0.1311 / 0.0346\n",
            "[19/1][21/59] Loss_D: 0.2196 Loss_G: 4.1091 D(x): 0.9149 D(G(z)): 0.1151 / 0.0220\n",
            "[19/1][22/59] Loss_D: 0.3180 Loss_G: 2.7132 D(x): 0.8310 D(G(z)): 0.1109 / 0.0835\n",
            "[19/1][23/59] Loss_D: 0.2071 Loss_G: 3.3510 D(x): 0.9025 D(G(z)): 0.0921 / 0.0453\n",
            "[19/1][24/59] Loss_D: 0.1038 Loss_G: 3.7045 D(x): 0.9272 D(G(z)): 0.0252 / 0.0324\n",
            "[19/1][25/59] Loss_D: 0.1818 Loss_G: 4.4415 D(x): 0.9701 D(G(z)): 0.1327 / 0.0169\n",
            "[19/1][26/59] Loss_D: 0.2025 Loss_G: 3.5757 D(x): 0.8914 D(G(z)): 0.0778 / 0.0375\n",
            "[19/1][27/59] Loss_D: 0.2285 Loss_G: 4.5184 D(x): 0.9211 D(G(z)): 0.1271 / 0.0154\n",
            "[19/1][28/59] Loss_D: 0.2192 Loss_G: 2.5556 D(x): 0.8270 D(G(z)): 0.0182 / 0.1137\n",
            "[19/1][29/59] Loss_D: 0.7844 Loss_G: 10.9024 D(x): 0.9836 D(G(z)): 0.4825 / 0.0000\n",
            "[19/1][30/59] Loss_D: 3.2227 Loss_G: 1.2799 D(x): 0.0620 D(G(z)): 0.0005 / 0.3625\n",
            "[19/1][31/59] Loss_D: 1.1646 Loss_G: 3.0823 D(x): 0.8650 D(G(z)): 0.5640 / 0.0705\n",
            "[19/1][32/59] Loss_D: 1.7359 Loss_G: 0.2365 D(x): 0.2649 D(G(z)): 0.1480 / 0.7995\n",
            "[19/1][33/59] Loss_D: 2.2067 Loss_G: 3.4916 D(x): 0.9388 D(G(z)): 0.8571 / 0.0370\n",
            "[19/1][34/59] Loss_D: 1.5651 Loss_G: 1.0752 D(x): 0.2592 D(G(z)): 0.0633 / 0.3744\n",
            "[19/1][35/59] Loss_D: 0.9925 Loss_G: 1.8001 D(x): 0.8475 D(G(z)): 0.5291 / 0.1925\n",
            "[19/1][36/59] Loss_D: 0.9990 Loss_G: 1.8990 D(x): 0.6442 D(G(z)): 0.3875 / 0.1723\n",
            "[19/1][37/59] Loss_D: 1.1500 Loss_G: 1.0906 D(x): 0.5085 D(G(z)): 0.3246 / 0.3654\n",
            "[19/1][38/59] Loss_D: 1.1139 Loss_G: 2.2760 D(x): 0.7144 D(G(z)): 0.5045 / 0.1256\n",
            "[19/1][39/59] Loss_D: 1.0242 Loss_G: 1.1827 D(x): 0.4953 D(G(z)): 0.2119 / 0.3379\n",
            "[19/1][40/59] Loss_D: 0.9136 Loss_G: 2.5778 D(x): 0.8340 D(G(z)): 0.4873 / 0.0913\n",
            "[19/1][41/59] Loss_D: 0.8423 Loss_G: 1.4554 D(x): 0.5976 D(G(z)): 0.2422 / 0.2582\n",
            "[19/1][42/59] Loss_D: 1.0589 Loss_G: 2.2591 D(x): 0.7222 D(G(z)): 0.4864 / 0.1233\n",
            "[19/1][43/59] Loss_D: 1.1657 Loss_G: 0.8557 D(x): 0.4614 D(G(z)): 0.2660 / 0.4539\n",
            "[19/1][44/59] Loss_D: 1.2362 Loss_G: 3.4085 D(x): 0.8252 D(G(z)): 0.6105 / 0.0422\n",
            "[19/1][45/59] Loss_D: 1.4151 Loss_G: 0.8283 D(x): 0.3083 D(G(z)): 0.1037 / 0.4754\n",
            "[19/1][46/59] Loss_D: 1.2925 Loss_G: 3.4543 D(x): 0.9195 D(G(z)): 0.6633 / 0.0408\n",
            "[19/1][47/59] Loss_D: 1.0219 Loss_G: 1.3890 D(x): 0.4457 D(G(z)): 0.1117 / 0.2814\n",
            "[19/1][48/59] Loss_D: 1.0444 Loss_G: 2.6565 D(x): 0.8388 D(G(z)): 0.5459 / 0.0883\n",
            "[19/1][49/59] Loss_D: 1.0480 Loss_G: 1.2667 D(x): 0.4955 D(G(z)): 0.2322 / 0.3142\n",
            "[19/1][50/59] Loss_D: 1.0894 Loss_G: 3.1731 D(x): 0.7660 D(G(z)): 0.5216 / 0.0530\n",
            "[19/1][51/59] Loss_D: 1.1626 Loss_G: 0.8746 D(x): 0.3957 D(G(z)): 0.1327 / 0.4507\n",
            "[19/1][52/59] Loss_D: 1.1621 Loss_G: 3.0660 D(x): 0.8924 D(G(z)): 0.6148 / 0.0569\n",
            "[19/1][53/59] Loss_D: 1.0080 Loss_G: 1.4170 D(x): 0.4678 D(G(z)): 0.1586 / 0.2718\n",
            "[19/1][54/59] Loss_D: 0.8874 Loss_G: 2.3906 D(x): 0.7984 D(G(z)): 0.4548 / 0.1084\n",
            "[19/1][55/59] Loss_D: 0.7855 Loss_G: 1.6834 D(x): 0.6018 D(G(z)): 0.2066 / 0.2139\n",
            "[19/1][56/59] Loss_D: 0.6868 Loss_G: 2.2994 D(x): 0.8021 D(G(z)): 0.3476 / 0.1191\n",
            "[19/1][57/59] Loss_D: 0.7378 Loss_G: 2.1782 D(x): 0.7077 D(G(z)): 0.2930 / 0.1349\n",
            "[19/1][58/59] Loss_D: 0.7965 Loss_G: 1.8795 D(x): 0.6691 D(G(z)): 0.2926 / 0.1790\n"
          ]
        }
      ],
      "source": [
        "# This is the engine of the code base - explicitly taking the objects created above\n",
        "# (The generator, discrimator and the dataset) and connecting them together to learn.\n",
        "\n",
        "for epoch in range(nepocs):\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        # train with real\n",
        "\n",
        "        # Set the descrimator to forget any gradients.\n",
        "        netD.zero_grad()\n",
        "        # Get a sample of real handwritten digits and label them as 1 - all real\n",
        "        real_cpu = data[0].to(device)\n",
        "        batch_size = real_cpu.size(0)\n",
        "        label = torch.full((batch_size,), real_label, dtype=real_cpu.dtype, device=device)\n",
        "        # Pass the sample through the discrimator\n",
        "        output = netD(real_cpu)\n",
        "        # measure the error\n",
        "        errD_real = criterion(output, label)\n",
        "        # Calculate the gradients of each layer of the network\n",
        "        errD_real.backward()\n",
        "        # Get the average of the output across the batch\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        # train with fake\n",
        "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "        # pass the noise through the generator layers\n",
        "        fake = netG(noise)\n",
        "        # set the labels to all 0 - fake\n",
        "        label.fill_(fake_label)\n",
        "        # ask the discrimator to judge the fake images\n",
        "        output = netD(fake.detach())\n",
        "        # measure the error\n",
        "        errD_fake = criterion(output, label)\n",
        "        # Calculate the gradients\n",
        "        errD_fake.backward()\n",
        "        # Get the average output across the batch again\n",
        "        D_G_z1 = output.mean().item()\n",
        "        # Get the error\n",
        "        errD = errD_real + errD_fake\n",
        "        # Run the optimizer to update the weights\n",
        "        optimizerD.step()\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        # Set the gradients of the generator to zero\n",
        "        netG.zero_grad()\n",
        "        label.fill_(real_label)  # fake labels are real for generator cost\n",
        "        # get the judgements from the discrimator of the generator output is fake\n",
        "        output = netD(fake)\n",
        "        # calculate the error\n",
        "        errG = criterion(output, label)\n",
        "        # update the gradients\n",
        "        errG.backward()\n",
        "        # Get the average of the output across the batch\n",
        "        D_G_z2 = output.mean().item()\n",
        "        # update the weights\n",
        "        optimizerG.step()\n",
        "\n",
        "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
        "              % (epoch, 1, i, len(dataloader), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "        # every 100 steps save a real sample and a fake sample for comparison\n",
        "        if i % 100 == 0:\n",
        "            vutils.save_image(real_cpu,'real_samples.png',normalize=True)\n",
        "            fake = netG(fixed_noise)\n",
        "            vutils.save_image(fake.detach(),'fake_samples_epoch_%03d.png' % epoch, normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jRMFcgknyO0p"
      },
      "execution_count": 12,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}