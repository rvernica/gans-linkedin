{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intall torchvision\n",
    "%pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "# For this example we will use pytorch to manage the construction of the neural networks and the training\n",
    "# torchvision is a module that is part of pytorch that supports vision datasets and it will be where we will source the mnist - handwritten digits - data\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f43eef95a30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting a seed will determine which data elements are selected. To replicate results keep the same seed.\n",
    "manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a check if there is a gpu available for training. At the moment we are assuming that it is not available.\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the GPU is not available means we will set the device to cpu and set up some parameters\n",
    "cudnn.benchmark = True\n",
    "device = torch.device(\"cpu\")\n",
    "ngpu = 0\n",
    "#This is the width of the latent space matrix\n",
    "nz = 100\n",
    "# This is the generator matrix shape\n",
    "ngf = 64\n",
    "# This is the descrimator matrix shape\n",
    "ndf = 64\n",
    "# This is the number of color channels - other datasets may have 3 if they are color\n",
    "nc = 1\n",
    "# The number of sample to process per pass\n",
    "batch_size = 64\n",
    "# the number of CPU workers to work on the dataset\n",
    "workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dset.MNIST(root='data', download=True,\n",
    "                      transform=transforms.Compose([\n",
    "                          transforms.Resize(64),\n",
    "                          transforms.ToTensor(),\n",
    "                          transforms.Normalize((0.5,), (0.5,)),\n",
    "                      ]))\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=int(workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "# The weights will need to be initialised based on the layer type to some value before training. These could be imported from past training steps.\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        torch.nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        torch.nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# This is the bulk of the neural network definition for the Generator.\n",
    "# The init sets up the layers and connecting activation functions.\n",
    "# The forward function processes the data through the layers\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(ngf,      nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(\n",
    "                self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "netG = Generator(ngpu).to(device)\n",
    "netG.apply(weights_init)\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# This is the bulk of the neural network definition for the Discrimator.\n",
    "# The init sets up the layers and connecting activation functions.\n",
    "# The forward function processes the data through the layers\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(\n",
    "                self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "    \n",
    "netD = Discriminator(ngpu).to(device)\n",
    "netD.apply(weights_init)\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the loss function from pytorches established modules\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Set up the initial noise of the latent space to sample from.\n",
    "# Set the label of a real and fake sample to 0,1\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# Create the optimiser which will dynamically change the parameters of the learning function over time to imporve the training process\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=0.0005, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=0.0005, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][0/938] Loss_D: 1.5835 Loss_G: 7.6377 D(x): 0.4592 D(G(z)): 0.4420 / 0.0008\n",
      "[0/1][1/938] Loss_D: 6.3707 Loss_G: 12.9269 D(x): 0.9999 D(G(z)): 0.9966 / 0.0000\n",
      "[0/1][2/938] Loss_D: 0.6768 Loss_G: 16.8669 D(x): 0.9834 D(G(z)): 0.4200 / 0.0000\n",
      "[0/1][3/938] Loss_D: 0.6830 Loss_G: 6.9758 D(x): 0.7238 D(G(z)): 0.0103 / 0.0019\n",
      "[0/1][4/938] Loss_D: 5.0312 Loss_G: 22.7755 D(x): 0.9792 D(G(z)): 0.9849 / 0.0000\n",
      "[0/1][5/938] Loss_D: 0.8028 Loss_G: 20.7876 D(x): 0.7737 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][6/938] Loss_D: 0.1013 Loss_G: 6.8544 D(x): 0.9420 D(G(z)): 0.0100 / 0.0024\n",
      "[0/1][7/938] Loss_D: 4.5736 Loss_G: 27.5072 D(x): 0.9956 D(G(z)): 0.9841 / 0.0000\n",
      "[0/1][8/938] Loss_D: 0.2590 Loss_G: 30.5974 D(x): 0.8658 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][9/938] Loss_D: 0.5414 Loss_G: 27.9687 D(x): 0.7860 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][10/938] Loss_D: 0.0722 Loss_G: 12.9074 D(x): 0.9690 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][11/938] Loss_D: 3.3673 Loss_G: 34.2867 D(x): 0.9949 D(G(z)): 0.9281 / 0.0000\n",
      "[0/1][12/938] Loss_D: 0.4987 Loss_G: 39.9006 D(x): 0.8213 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][13/938] Loss_D: 0.3143 Loss_G: 41.1334 D(x): 0.8446 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][14/938] Loss_D: 0.0682 Loss_G: 41.4898 D(x): 0.9650 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][15/938] Loss_D: 0.0004 Loss_G: 41.6900 D(x): 0.9996 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][16/938] Loss_D: 0.0001 Loss_G: 41.7032 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][17/938] Loss_D: 0.0002 Loss_G: 41.6041 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][18/938] Loss_D: 0.0096 Loss_G: 41.7073 D(x): 0.9928 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][19/938] Loss_D: 0.0000 Loss_G: 41.6242 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][20/938] Loss_D: 0.0012 Loss_G: 41.6705 D(x): 0.9988 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][21/938] Loss_D: 0.0003 Loss_G: 41.6945 D(x): 0.9997 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][22/938] Loss_D: 0.0000 Loss_G: 41.6677 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][23/938] Loss_D: 0.0003 Loss_G: 41.7906 D(x): 0.9997 D(G(z)): 0.0000 / 0.0000\n"
     ]
    }
   ],
   "source": [
    "# This is the engine of the code base - explicitly taking the objects created above \n",
    "# (The generator, discrimator and the dataset) and connecting them together to learn.\n",
    "\n",
    "for epoch in range(1):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # train with real\n",
    "        \n",
    "        # Set the descrimator to forget any gradients.\n",
    "        netD.zero_grad()\n",
    "        # Get a sample of real handwritten digits and label them as 1 - all real\n",
    "        real_cpu = data[0].to(device)\n",
    "        batch_size = real_cpu.size(0)\n",
    "        label = torch.full((batch_size,), real_label, dtype=real_cpu.dtype, device=device)\n",
    "        # Pass the sample through the discrimator\n",
    "        output = netD(real_cpu)\n",
    "        # measure the error\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate the gradients of each layer of the network\n",
    "        errD_real.backward()\n",
    "        # Get the average of the output across the batch\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # train with fake\n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "        # pass the noise through the generator layers\n",
    "        fake = netG(noise)\n",
    "        # set the labels to all 0 - fake\n",
    "        label.fill_(fake_label)\n",
    "        # ask the discrimator to judge the fake images\n",
    "        output = netD(fake.detach())\n",
    "        # measure the error\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients \n",
    "        errD_fake.backward()\n",
    "        # Get the average output across the batch again\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Get the error\n",
    "        errD = errD_real + errD_fake\n",
    "        # Run the optimizer to update the weights\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        # Set the gradients of the generator to zero\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # get the judgements from the discrimator of the generator output is fake\n",
    "        output = netD(fake)\n",
    "        # calculate the error\n",
    "        errG = criterion(output, label)\n",
    "        # update the gradients\n",
    "        errG.backward()\n",
    "        # Get the average of the output across the batch\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # update the weights\n",
    "        optimizerG.step()\n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "              % (epoch, 1, i, len(dataloader), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "        # every 100 steps save a real sample and a fake sample for comparison\n",
    "        if i % 100 == 0:\n",
    "            vutils.save_image(real_cpu,'real_samples.png',normalize=True)\n",
    "            fake = netG(fixed_noise)\n",
    "            vutils.save_image(fake.detach(),'fake_samples_epoch_%03d.png' % epoch, normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
